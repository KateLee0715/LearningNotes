{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932, 429)\n",
      "Size of testing data: (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "data_root = 'timit_11'\n",
    "train = np.load(data_root + '/train_11.npy')\n",
    "train_label = np.load(data_root + '/train_label_11.npy')\n",
    "test = np.load(data_root + '/test_11.npy')\n",
    "\n",
    "print(\"Size of training data: {}\".format(train.shape))\n",
    "print(\"Size of testing data: {}\".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.random.get_state()\n",
    "np.random.shuffle(train)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, X, y = None):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (1217632, 429)\n",
      "Size of validation set: (12300, 429)\n"
     ]
    }
   ],
   "source": [
    "VAL_RATIO = 0.01\n",
    "\n",
    "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "\n",
    "print(\"Size of training set: {}\".format(train_x.shape))\n",
    "print(\"Size of validation set: {}\".format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "train_set = TIMITDataset(train_x, train_y)\n",
    "val_set = TIMITDataset(val_x, val_y)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del train, train_label, train_x, train_y, val_x, val_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer0_bn = nn.BatchNorm1d(429)\n",
    "        \n",
    "        self.layer1 = nn.Linear(429, 2048)\n",
    "        self.layer1_bn = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        self.layer2 = nn.Linear(2048, 2048)\n",
    "        self.layer2_bn = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        self.layer3 = nn.Linear(2048, 2048)\n",
    "        self.layer3_bn = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        self.layer4 = nn.Linear(2048, 2048)\n",
    "        self.layer4_bn = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        self.layer5 = nn.Linear(2048, 2048)\n",
    "        self.layer5_bn = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        self.out = nn.Linear(2048, 39)\n",
    "        \n",
    "        self.dropoutInput = nn.Dropout(p = 0.2)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        \n",
    "        self.act_fn = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer0_bn(x)\n",
    "        x = self.dropoutInput(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer1_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.layer2_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.layer3_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "        x = self.layer4_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = self.layer5_bn(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() == True else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "same_seeds(0)\n",
    "\n",
    "device = get_device()\n",
    "print(\"DEVICE: {}\".format(device))\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "early_stop = 20\n",
    "\n",
    "model_path = './model.ckpt'\n",
    "model = Classifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/100] Train Acc: 0.568132 Loss: 1.392063 | Val Acc: 0.670976 loss: 1.028025\n",
      "saving model with acc 0.671\n",
      "[002/100] Train Acc: 0.628680 Loss: 1.167372 | Val Acc: 0.698618 loss: 0.926482\n",
      "saving model with acc 0.699\n",
      "[003/100] Train Acc: 0.650225 Loss: 1.091552 | Val Acc: 0.712927 loss: 0.868010\n",
      "saving model with acc 0.713\n",
      "[004/100] Train Acc: 0.663834 Loss: 1.042040 | Val Acc: 0.723415 loss: 0.821924\n",
      "saving model with acc 0.723\n",
      "[005/100] Train Acc: 0.674101 Loss: 1.005106 | Val Acc: 0.731951 loss: 0.789527\n",
      "saving model with acc 0.732\n",
      "[006/100] Train Acc: 0.681501 Loss: 0.977676 | Val Acc: 0.739512 loss: 0.766864\n",
      "saving model with acc 0.740\n",
      "[007/100] Train Acc: 0.687464 Loss: 0.955807 | Val Acc: 0.748049 loss: 0.745484\n",
      "saving model with acc 0.748\n",
      "[008/100] Train Acc: 0.692833 Loss: 0.937016 | Val Acc: 0.754715 loss: 0.727212\n",
      "saving model with acc 0.755\n",
      "[009/100] Train Acc: 0.696895 Loss: 0.922548 | Val Acc: 0.757724 loss: 0.720874\n",
      "saving model with acc 0.758\n",
      "[010/100] Train Acc: 0.700789 Loss: 0.908043 | Val Acc: 0.762764 loss: 0.702051\n",
      "saving model with acc 0.763\n",
      "[011/100] Train Acc: 0.703340 Loss: 0.897701 | Val Acc: 0.766748 loss: 0.692197\n",
      "saving model with acc 0.767\n",
      "[012/100] Train Acc: 0.706320 Loss: 0.887685 | Val Acc: 0.770163 loss: 0.681520\n",
      "saving model with acc 0.770\n",
      "[013/100] Train Acc: 0.709415 Loss: 0.877437 | Val Acc: 0.773252 loss: 0.669343\n",
      "saving model with acc 0.773\n",
      "[014/100] Train Acc: 0.711460 Loss: 0.869529 | Val Acc: 0.776667 loss: 0.660331\n",
      "saving model with acc 0.777\n",
      "[015/100] Train Acc: 0.713298 Loss: 0.863241 | Val Acc: 0.776016 loss: 0.658139\n",
      "[016/100] Train Acc: 0.715375 Loss: 0.855436 | Val Acc: 0.776504 loss: 0.650904\n",
      "[017/100] Train Acc: 0.716905 Loss: 0.849655 | Val Acc: 0.778780 loss: 0.639031\n",
      "saving model with acc 0.779\n",
      "[018/100] Train Acc: 0.718383 Loss: 0.844044 | Val Acc: 0.779919 loss: 0.635342\n",
      "saving model with acc 0.780\n",
      "[019/100] Train Acc: 0.719998 Loss: 0.838187 | Val Acc: 0.786423 loss: 0.624549\n",
      "saving model with acc 0.786\n",
      "[020/100] Train Acc: 0.721282 Loss: 0.834427 | Val Acc: 0.786992 loss: 0.619284\n",
      "saving model with acc 0.787\n",
      "[021/100] Train Acc: 0.722166 Loss: 0.830703 | Val Acc: 0.785122 loss: 0.617464\n",
      "[022/100] Train Acc: 0.723499 Loss: 0.825600 | Val Acc: 0.787724 loss: 0.618151\n",
      "saving model with acc 0.788\n",
      "[023/100] Train Acc: 0.723869 Loss: 0.823107 | Val Acc: 0.787561 loss: 0.618892\n",
      "[024/100] Train Acc: 0.725673 Loss: 0.817940 | Val Acc: 0.787967 loss: 0.606571\n",
      "saving model with acc 0.788\n",
      "[025/100] Train Acc: 0.726546 Loss: 0.815386 | Val Acc: 0.788943 loss: 0.604329\n",
      "saving model with acc 0.789\n",
      "[026/100] Train Acc: 0.727248 Loss: 0.812287 | Val Acc: 0.792276 loss: 0.600464\n",
      "saving model with acc 0.792\n",
      "[027/100] Train Acc: 0.727671 Loss: 0.808235 | Val Acc: 0.792846 loss: 0.598878\n",
      "saving model with acc 0.793\n",
      "[028/100] Train Acc: 0.729044 Loss: 0.805246 | Val Acc: 0.792602 loss: 0.589899\n",
      "[029/100] Train Acc: 0.729598 Loss: 0.803968 | Val Acc: 0.794228 loss: 0.595220\n",
      "saving model with acc 0.794\n",
      "[030/100] Train Acc: 0.730393 Loss: 0.800314 | Val Acc: 0.794634 loss: 0.591448\n",
      "saving model with acc 0.795\n",
      "[031/100] Train Acc: 0.730628 Loss: 0.798231 | Val Acc: 0.794553 loss: 0.589711\n",
      "[032/100] Train Acc: 0.731577 Loss: 0.795660 | Val Acc: 0.796341 loss: 0.584273\n",
      "saving model with acc 0.796\n",
      "[033/100] Train Acc: 0.732550 Loss: 0.793556 | Val Acc: 0.799187 loss: 0.580555\n",
      "saving model with acc 0.799\n",
      "[034/100] Train Acc: 0.733010 Loss: 0.791243 | Val Acc: 0.797317 loss: 0.579030\n",
      "[035/100] Train Acc: 0.733570 Loss: 0.788793 | Val Acc: 0.800488 loss: 0.577868\n",
      "saving model with acc 0.800\n",
      "[036/100] Train Acc: 0.733848 Loss: 0.788269 | Val Acc: 0.797398 loss: 0.573836\n",
      "[037/100] Train Acc: 0.735108 Loss: 0.784824 | Val Acc: 0.798293 loss: 0.573117\n",
      "[038/100] Train Acc: 0.734861 Loss: 0.784183 | Val Acc: 0.799106 loss: 0.576454\n",
      "[039/100] Train Acc: 0.735805 Loss: 0.781295 | Val Acc: 0.802439 loss: 0.570220\n",
      "saving model with acc 0.802\n",
      "[040/100] Train Acc: 0.735307 Loss: 0.781496 | Val Acc: 0.800407 loss: 0.570095\n",
      "[041/100] Train Acc: 0.736375 Loss: 0.778797 | Val Acc: 0.800081 loss: 0.568018\n",
      "[042/100] Train Acc: 0.736947 Loss: 0.777895 | Val Acc: 0.801463 loss: 0.564773\n",
      "[043/100] Train Acc: 0.737167 Loss: 0.776535 | Val Acc: 0.802602 loss: 0.561845\n",
      "saving model with acc 0.803\n",
      "[044/100] Train Acc: 0.737647 Loss: 0.775375 | Val Acc: 0.803089 loss: 0.562343\n",
      "saving model with acc 0.803\n",
      "[045/100] Train Acc: 0.737959 Loss: 0.773551 | Val Acc: 0.804228 loss: 0.556364\n",
      "saving model with acc 0.804\n",
      "[046/100] Train Acc: 0.738166 Loss: 0.772065 | Val Acc: 0.803089 loss: 0.557898\n",
      "[047/100] Train Acc: 0.738230 Loss: 0.771606 | Val Acc: 0.807073 loss: 0.556464\n",
      "saving model with acc 0.807\n",
      "[048/100] Train Acc: 0.738083 Loss: 0.772663 | Val Acc: 0.808862 loss: 0.549808\n",
      "saving model with acc 0.809\n",
      "[049/100] Train Acc: 0.738630 Loss: 0.769790 | Val Acc: 0.803252 loss: 0.556788\n",
      "[050/100] Train Acc: 0.739468 Loss: 0.769132 | Val Acc: 0.805772 loss: 0.549075\n",
      "[051/100] Train Acc: 0.739261 Loss: 0.766732 | Val Acc: 0.807073 loss: 0.544561\n",
      "[052/100] Train Acc: 0.739915 Loss: 0.766568 | Val Acc: 0.806260 loss: 0.546930\n",
      "[053/100] Train Acc: 0.739945 Loss: 0.766048 | Val Acc: 0.805935 loss: 0.547348\n",
      "[054/100] Train Acc: 0.740059 Loss: 0.765125 | Val Acc: 0.807073 loss: 0.544279\n",
      "[055/100] Train Acc: 0.740593 Loss: 0.762992 | Val Acc: 0.807724 loss: 0.545649\n",
      "[056/100] Train Acc: 0.740468 Loss: 0.763356 | Val Acc: 0.806992 loss: 0.544453\n",
      "[057/100] Train Acc: 0.740820 Loss: 0.762430 | Val Acc: 0.808780 loss: 0.540420\n",
      "[058/100] Train Acc: 0.741165 Loss: 0.761604 | Val Acc: 0.807967 loss: 0.540520\n",
      "[059/100] Train Acc: 0.741482 Loss: 0.759795 | Val Acc: 0.808537 loss: 0.541227\n",
      "[060/100] Train Acc: 0.741505 Loss: 0.760002 | Val Acc: 0.806016 loss: 0.541973\n",
      "[061/100] Train Acc: 0.741466 Loss: 0.760134 | Val Acc: 0.807236 loss: 0.539232\n",
      "[062/100] Train Acc: 0.742147 Loss: 0.758874 | Val Acc: 0.805447 loss: 0.540586\n",
      "[063/100] Train Acc: 0.742433 Loss: 0.757344 | Val Acc: 0.808293 loss: 0.541037\n",
      "[064/100] Train Acc: 0.742508 Loss: 0.757555 | Val Acc: 0.806423 loss: 0.543014\n",
      "[065/100] Train Acc: 0.742146 Loss: 0.757054 | Val Acc: 0.806748 loss: 0.545373\n",
      "[066/100] Train Acc: 0.742467 Loss: 0.755906 | Val Acc: 0.809187 loss: 0.536944\n",
      "saving model with acc 0.809\n",
      "[067/100] Train Acc: 0.742697 Loss: 0.755508 | Val Acc: 0.808374 loss: 0.539220\n",
      "[068/100] Train Acc: 0.743402 Loss: 0.755029 | Val Acc: 0.810650 loss: 0.531325\n",
      "saving model with acc 0.811\n",
      "[069/100] Train Acc: 0.743313 Loss: 0.754776 | Val Acc: 0.809268 loss: 0.533572\n",
      "[070/100] Train Acc: 0.742930 Loss: 0.754221 | Val Acc: 0.812033 loss: 0.532273\n",
      "saving model with acc 0.812\n",
      "[071/100] Train Acc: 0.743508 Loss: 0.753396 | Val Acc: 0.809675 loss: 0.537663\n",
      "[072/100] Train Acc: 0.743152 Loss: 0.753370 | Val Acc: 0.808537 loss: 0.535479\n",
      "[073/100] Train Acc: 0.743205 Loss: 0.753722 | Val Acc: 0.809268 loss: 0.536857\n",
      "[074/100] Train Acc: 0.743984 Loss: 0.752357 | Val Acc: 0.809593 loss: 0.537914\n",
      "[075/100] Train Acc: 0.744322 Loss: 0.751362 | Val Acc: 0.811951 loss: 0.530353\n",
      "[076/100] Train Acc: 0.743969 Loss: 0.750329 | Val Acc: 0.812114 loss: 0.527662\n",
      "saving model with acc 0.812\n",
      "[077/100] Train Acc: 0.744340 Loss: 0.750726 | Val Acc: 0.811870 loss: 0.532739\n",
      "[078/100] Train Acc: 0.744139 Loss: 0.750784 | Val Acc: 0.814309 loss: 0.530875\n",
      "saving model with acc 0.814\n",
      "[079/100] Train Acc: 0.744541 Loss: 0.749926 | Val Acc: 0.813415 loss: 0.527190\n",
      "[080/100] Train Acc: 0.745052 Loss: 0.748350 | Val Acc: 0.811301 loss: 0.528676\n",
      "[081/100] Train Acc: 0.744753 Loss: 0.749317 | Val Acc: 0.811057 loss: 0.531036\n",
      "[082/100] Train Acc: 0.744603 Loss: 0.749741 | Val Acc: 0.810650 loss: 0.529792\n",
      "[083/100] Train Acc: 0.744995 Loss: 0.747859 | Val Acc: 0.813252 loss: 0.528097\n",
      "[084/100] Train Acc: 0.745233 Loss: 0.748097 | Val Acc: 0.813902 loss: 0.528194\n",
      "[085/100] Train Acc: 0.745127 Loss: 0.747692 | Val Acc: 0.811626 loss: 0.526299\n",
      "[086/100] Train Acc: 0.745504 Loss: 0.746571 | Val Acc: 0.815447 loss: 0.525121\n",
      "saving model with acc 0.815\n",
      "[087/100] Train Acc: 0.745351 Loss: 0.747150 | Val Acc: 0.810813 loss: 0.528156\n",
      "[088/100] Train Acc: 0.745208 Loss: 0.746523 | Val Acc: 0.815691 loss: 0.525380\n",
      "saving model with acc 0.816\n",
      "[089/100] Train Acc: 0.745620 Loss: 0.745945 | Val Acc: 0.810325 loss: 0.529223\n",
      "[090/100] Train Acc: 0.745229 Loss: 0.746992 | Val Acc: 0.815854 loss: 0.529505\n",
      "saving model with acc 0.816\n",
      "[091/100] Train Acc: 0.745219 Loss: 0.745480 | Val Acc: 0.810732 loss: 0.528949\n",
      "[092/100] Train Acc: 0.745948 Loss: 0.745056 | Val Acc: 0.813008 loss: 0.525165\n",
      "[093/100] Train Acc: 0.745720 Loss: 0.744127 | Val Acc: 0.814959 loss: 0.520369\n",
      "[094/100] Train Acc: 0.746221 Loss: 0.744286 | Val Acc: 0.814959 loss: 0.518961\n",
      "[095/100] Train Acc: 0.746167 Loss: 0.743260 | Val Acc: 0.814553 loss: 0.521409\n",
      "[096/100] Train Acc: 0.746391 Loss: 0.743644 | Val Acc: 0.814715 loss: 0.523332\n",
      "[097/100] Train Acc: 0.746609 Loss: 0.742689 | Val Acc: 0.814065 loss: 0.517429\n",
      "[098/100] Train Acc: 0.745923 Loss: 0.743117 | Val Acc: 0.814228 loss: 0.521236\n",
      "[099/100] Train Acc: 0.746197 Loss: 0.743081 | Val Acc: 0.817886 loss: 0.525376\n",
      "saving model with acc 0.818\n",
      "[100/100] Train Acc: 0.746334 Loss: 0.743161 | Val Acc: 0.812033 loss: 0.523740\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "early_stop_cnt = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         reg_loss = 0\n",
    "#         for param in model.parameters():\n",
    "#             reg_loss += torch.sum(param ** 2)\n",
    "#         loss += 0.00075 * reg_loss\n",
    "        _, train_preds = torch.max(outputs, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += (train_preds.cpu() == labels.cpu()).sum().item()\n",
    "    \n",
    "    if len(val_set) > 0:\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "#                 reg_loss = 0\n",
    "#                 for param in model.parameters():\n",
    "#                     reg_loss += torch.sum(param ** 2)\n",
    "#                 loss += 0.00075 * reg_loss\n",
    "                _, val_preds = torch.max(outputs, 1)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_acc += (val_preds.cpu() == labels.cpu()).sum().item()\n",
    "            \n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                 epoch + 1, num_epochs, train_acc / len(train_set), train_loss / len(train_loader), val_acc / len(val_set), val_loss / len(val_loader)\n",
    "             ))\n",
    "        \n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                early_stop_cnt = 0\n",
    "                print('saving model with acc {:.3f}'.format(best_acc / len(val_set)))\n",
    "            else:\n",
    "                early_stop_cnt += 1\n",
    "                if(early_stop_cnt >= early_stop):\n",
    "                    break\n",
    "                \n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epochs, train_acc / len(train_set), train_loss / len(train_loader)\n",
    "        ))\n",
    "        \n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = TIMITDataset(test)\n",
    "test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "model = Classifier().to(device)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predicts = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs = data\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, test_preds = torch.max(outputs, 1)\n",
    "        \n",
    "        for y in test_preds.cpu().numpy():\n",
    "            predicts.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, p in enumerate(predicts):\n",
    "        f.write('{},{}\\n'.format(i, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451552"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
