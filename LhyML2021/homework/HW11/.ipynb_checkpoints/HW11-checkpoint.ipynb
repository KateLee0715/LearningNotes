{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def no_axis_show(img, title='', cmap=None):\n",
    "    fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    plt.title(title)\n",
    "\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    img = plt.imread(f'./real_or_drawing/train_data/{i}/{500*i}.bmp')\n",
    "    no_axis_show(img, titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/4AAABnCAYAAAC5HZnbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnkUlEQVR4nO3deaBV8/rH8e+hDFdoppSSKUQSiSbKmFJCylAUMpRkLJI0KHMTIpk1mm5JhjJUhlI0km6haCBSiOqm8/vn5+m99l2r9j5777XXWefz+uuj9rA6a+81OM/zfPPy8/OdiIiIiIiIiMTTLrneABERERERERHJHt34i4iIiIiIiMSYbvxFREREREREYkw3/iIiIiIiIiIxpht/ERERERERkRjTjb+IiIiIiIhIjBVL5cF5eXla+y8L8vPz89J5vvZLdmi/RJP2SzRpv0ST9ktk/Zyfn18unRfQvskOfWeiSfslmrRfoilov+g3/iIiIiLhWp7rDRARkaIlpd/4FyZ5edv/R0eHDh0sv/zyy5Y3bNgQ6jaJiEju1K1b13LDhg0t83wRZPPmzZY/+eQTy7Nnz7b8999/p7uJIiJZVbFiRcuNGjWyPHr06FxsjoiESL/xFxEREREREYkx3fiLiIiIiIiIxFhsS/1r165t+amnnrK8atUqy5MnTw51m3Jpl122/z+eatWqWV66dGkuNkdEJCuaNWtmuU+fPp6/q1WrVsbfjy1jH3zwgeWXXnrJ8iuvvGJ527ZtGd8GEZFkXX311ZZvu+02yyr1D1/Lli0t9+jRw3Lp0qUtjxw50vLAgQND2S7xd99991n+4osvLI8ZMyYXm1Mg+o2/iIiIiIiISIzpxl9EREREREQkxmJb6t+gQQPL+fnbl4icOXNmLjYn584880zLb7zxhuWaNWtaXrhwYajbJNlTo0YNyywlq1OnjuXDDz/cMsuV161b55u///57y9OmTbO8Zs0ay8uWLbO8fv36Amy5pOqQQw6xfN5551lmew9bnLjvnHNu+fLCv6rYlVdeaXn48OGWFyxY4Hlc+/btLY8bN87ypk2bdvoe++67r2VOwm7SpIllHmf5+vPmzbN85513Wp40adJO31dEJJOOOuooy2r3DB9//myvWLx4sW8eMGCAZZ6rBg0alKUtFCpZsqTlm266yTLP6yr1FxEREREREZFI0I2/iIiIiIiISIzFttS/VKlSllkaw9LlomTOnDmWWQJ88sknW1apf+FTr149y/fcc49llh9v3brVMvfxrFmzLO+1116WOU2WLQPNmze3zEnAxPd68803PX83dOhQy1OmTPF9viTn2GOPtcyfZZkyZVJ+rRUrVlhmGwAz24NWr16d8ntkw80332z5gQcesPz6669bbtu2rec5mzdvLvD7sR1mwoQJvjkvL89yixYtLPft29cyf5Ys4WQLgHPeFjWRwqhq1aqW2XZTokQJ38fzOzZ37lzLs2fPtjxs2DDL3333XfobWQRVr17d8pdffrnTxx922GGWWaa+ZMkSy4sWLcrQ1sXfxRdfbJnH+caNG1v+9ddfLXNVmN69e1seMmSIZa0Wkz2nn3665WLFtt82H3fccZYrVqxoma2VUaTf+IuIiIiIiIjEmG78RURERERERGIstqX+xYsXt8zy46KKJXTEEm+Jvmuvvdbz34MHD7bMqftdu3a1/Oyzz1ree++9LbNcv2PHjpa3bNni+95sAbjuuuss33DDDZbLlStn+eyzz/Y8n60CDz74oOU77rjDsr6ryenWrZtl/swOPPBAy3/88YdllnOOGDHC81ps97n00kt9M1do4OR6touE4aqrrrLMz9Bzzz1nmRP+w/48sWyTLQdsB3jooYcs9+jRwzJLBZ3z/lv/+9//ZnIzc+6DDz7w/DcnInNVBsk9tg/xM8nvFo8PLFHeuHGjZZYrH3TQQZbZrsaVM/i+nTp18s0smXbOuYkTJ+7gX1K07brrrpYPPfRQy6+++qrlyy+/3DKPTSz1D8KWs/79+3v+LvH7XtTVqlXL8vz58y3zu0OTJ0+23KpVK8vly5e3zNWVJLN4zfPnn39a/te//mWZ7QC8Hoki/cZfREREREREJMZ04y8iIiIiIiISYzkp9edE6pYtW3r+7sgjj/R9DstWWQoYVGrKyYtxK5MsCE6zZomeSv2jia0qnNx6zTXXeB738ssvW2aZHkssid83lujff//9lleuXOn7XK6I0a9fP8tsJZgxY4blxYsXe57PFQVuueUWy/vtt5/l9u3b+763eHGa7PTp0y2z3YOrd3DFB05lds65Bg0a+L4Hjxmcnr3bbrulvsFp4KR8fm7ef/99y1dccYXlTE7D52ezQoUKljlxPBmcuMw2DX7X+B1MfO8LL7zQMs+FhRXbS5zzXhOwJHzt2rVhbZL8v8Rrsscff9zy/vvvn/H3++WXXyxzxYsbb7zR8h577GF5/Pjxll977TXPa916662WH3nkkUxuZqHH9ordd9/dMluj2G7Ec/nAgQMtf/bZZ5YbNWpkma2DPDY759yoUaMst2vXzvLff/+d/D+gkOD5tXv37pb5eS5ZsqTlZI5xQddkBxxwgGWV+mcWrzvYtsprIR4P2b6hUn8RERERERERyRnd+IuIiIiIiIjEWFZL/UuVKmWZE5hZlpk4dXnRokWWWQZUuXJl3+dziiunUGuqf7CgqZSSW5y4zyngLKfr3bu35zl9+vSxnEyJM6fuE8stU/XDDz9YrlOnjuXEknA+bvXq1ZZ5bJgzZ45ltjiIF1dYCCrxY3k52y5Yuuucc08++aRllscuXbrUci6Pofz8c7r0nXfeaTmT5f3E0mO2oWWqRYqffU4xd85bJvr5559bbtq0qWXuo6jj8YDnZ+e809x5jLv++uuzvl2ZUqNGDcvcL5s2bcrF5qSEP/O7777b83effvqp5VNPPdXysmXLLPNaj2XM/HNmtqt88sknlpMp/T7ttNMsJ65Q8vDDD1uuXr265c6dO1uOe+tniRIlLPP6ju1hxHM/y/45ST4I2/d4Hrnppps8j2OrAPc9V2iIOpZ+s1XpmGOO8TyOf9emTRvLbdu2tcw2PJ67Vq1aZXn58uWWec1ObAHj9SNX8fnpp598nys7dvTRR1tmCwyPH2zBSGyhjDL9xl9EREREREQkxnTjLyIiIiIiIhJjuvEXERERERERibG8VHoj8/Lydvpg9mOOGTPGMvu7BgwYYDlxyZXffvvN93X33HNPy1zSjK81bdo0y+w/O/fcc30ze57Yz8GlTnbkm2++sfzEE09YXr9+fVLP/0d+fn7ezh8VLJn9QuyvHjdunOWuXbumsxmRV7VqVcuDBg2yPHjwYMtchibs/fLCCy9Ybt26tWX2iSUuX5SqoUOHWuZMDH4/w8YlCc8880zLZcuWtcyl5cLeL1H0xRdfWObshObNm/s+/vzzz7fMn7dz3l7Yr7/+usDblK398tJLL1lmfy/nvmzZsiWdtw702GOPWWY/ZdCsjHSwh9Q57z57+umnLfO8c9JJJ1n+66+/fF83Kt8X9vEnniPZ21q+fHnLNWvWtMy+1ajgkouctcHrgcQlWGFOfn7+8em8fzr7hr3g/PlzroVzzl122WWWo7b8WuJ3hnM/OP9m6tSplvk9DrpWi8p3Zkd43Oa1G5fLY48/r/X5c+Njgo4h6erVq5dlLi3LfZF4XvIT9n4566yzLPfv399y0LwE57wzE/j54lwYzjvhUpXZmLn17rvvWuYSzs7975LLBVUYvi+puv322y3fe++9lrmsL48xvPbiMou5FLRf9Bt/ERERERERkRjTjb+IiIiIiIhIjGVkOT8u8fLmm29aZtlokyZNLBekZI8lSGwPYNk6S0Lr1atnmaX7s2fP9n19thgkW862zz77WOZSHfy3prNMWrZs3LjRcqaWpYqqCy64wPJTTz1lmUty9O3bN9RtIn5uWHp/6623Wk63vJ9Lx7Rs2dJyMsv1hGH48OGWWd7MZXHYghFHLB1nefmoUaMs87g5a9Ysy1zelKXHP/74o+Wg5YCc8y6/FRW77rqrZZbQjRw50nK2yvuJx2+WZGZDYtsdS1+5HSxb7tixo+Vhw4ZlcevSt6NzDVv2evbsaZnLHXIZw2xjKXTi9+PXX3+1HLRNiUuZRhGPFWyl5JKZzjl34oknWv7444+zv2EpSPzO9OvXz/KSJUssP/vss5anTJlimdeJbCeLCh5zEq9TunXrZpnXx2wLmjt3rmUeR1u0aOH73Gzhfjn99NMtP/roo5YnTZoU6jYF4RKDXPqWS43zuo2l9M6lfs3P67MDDzzQcrFi22/PqlWrZpnnRt5jsfXt+OO3dxBxacXPPvvM8948f7DtV7xtp1xmmp9ltllyyT8ut7xu3bpsbWKB6Tf+IiIiIiIiIjGmG38RERERERGRGMtIqX+zZs0sb9261TIn/G/YsCETb/U/uHIAS7s4iZaleZ9//rlv5lTbZNWvX9/y22+/bfnhhx+23L59+5RfN9tYxtiwYUPLLIdj+ZFz3vIiljLNmzfPMtsoWBbFz0Q2sCTKOe/kek5U/vDDDy1fcskllleuXJnFrftf/FmybGjGjBmW+RlKF0vmK1WqZHn8+PEZe4908N/N0s06depYzmWpPye0slWCJYuJU95Z1rz33ntbZnsQH8NSW2KrCgVNaGYJIleuYGtAYnksPx8zZ870fb+wHXrooZb58wu71PiPP/6wzP3F73AYk875+eeKDq1atbIc9VL/HU2sZlsKp36z7YWllzzfZgPP2yxHds47TZ2l/mwfK168eBa3ruDKlCljuUePHpb5ea5du7bnOR999JFl7g9OOS/I9VO2sXSZ14DvvPOOZbZJseUsl1im/dxzz1nm584554YMGWKZ08X5byWWH4fZNuOcc9u2bbPMz9306dN9t+mVV14JZ8P+H9tZ+H3nNRKvGTN5TcufzXfffef7mKVLl+70dbjaC691n3/+ectsh3bO20rI41e67aWFFVc64XURr8ePOOIIy7x2JT6Gx8+o0G/8RURERERERGJMN/4iIiIiIiIiMZaRUn+WvbDsMVvl/UGCyvizJag0+4477rB82223WWY5Y9hYDsySS5b+sbx2R7hf9913X9/HcCorS1PZDsDMieVsJeB7sYSNrQicYuucdzo+y0Y5FTeM8twgLN/mtNbrrrvOMsu/0tW6dWvLLF1+6623MvYe6di0aZPvnye2cGQbS7n5uenatavvNs2fP9/y8uXLPa/1/fffW+ZnmD9/rq7BvH79esuc6nzxxRdbrlChgu/jE6f2/oPfqa+++srzdw0aNLDMFVNy6ZBDDvH9c7ZzhYGrvfAYys8KHxMGlmHefffdlnksj+JqMjsq9eeqE08++aRlHhP52TzmmGMsZ6OVjG2KidvNFVdOO+00yyzTrVu3bsa3qaB4zOKqS0cffbRlnhsXLlzoeT7bmzp06OCbuWIOWwDWrFlTwK3OLJbbstWJ35lcYvk7y/ZXrFhhmatnOefctGnTUnoP7qNctnSxXYtT0XldFEapP68hR4wYYfnbb7+1zFaQbLesZsvatWstc2UH57zHg9GjR1s+++yzLRemFZXYssSyfeeC70fZpskVELga3IQJEyzz/m7//ff3fc2jjjrKskr9RURERERERCRUuvEXERERERERibGMl/qz1KIomTx5suWePXtaZjldLkv9OaGS5SmJZX3/4CRQ55y7//77LbP0+b333rPMabQnnHCCb27Xrp3lG264Ialt/8fvv/9umWWV/Bk75y3XiUrpMrF8jC0OnDacLpaxnXfeeZbfeOMNy2zHSAbLWvkZYMtA4gTsZLCEmjLZ7pDMe7Mkq0aNGpY5LZ2T8oMm8GbL4MGDM/I6iZNoW7RokZHXzaSgVQ7YEhGGoPfjSgO5LPVniTanYr/wwgtZ3w6WQrLs+8gjj7RctWpVyztqJePPmW1YbJVjWerEiRMts+WGk8953mErwU8//WT5559/tswS8MaNG/v+uXPOXX/99ZZ5nOXPvEuXLparVKliObElKAwsI+dKKeeee65l/jwrVqzoef5JJ51kmdd6LKXt3LmzZf58eEzlRPG5c+da5nGUrQFsseDni+f7xPLdsWPHWr7ooosst2nTxjI/X2G0hAa55ZZbLN97772WR44caZnn2XSPfWyVTLVNIJN4XmdJf8eOHS3z+J/qdUqyeHzgZ4qfGx434mDz5s2e/2YbD1cC4GoSBx10kOWwW2R5P8kVbHgteuyxx1quWbOm5d12283zWmyJJJ7L+X5TpkyxzBZKtl/y8Wzj5D0gr6/4HeYqCnyuc85t2bLFMo9pQasIpEq/8RcRERERERGJMd34i4iIiIiIiMRYRkr9WbKQWF5RVCxatMgySwNZ9sjSkbCx1I0l84sXL7bMkrtnnnnG83yW+rPEk9PLmVmOSiytPvjggy2zDJQrBZQsWdIyJ32zTDGx/Ijl2FHBskVOjOa2JpaUpoNtF5UqVbI8btw438cXL17cMkvdbr75ZsssqeK04eHDh6e1rVzZgJ8PTvzNluOOO84yy8RYWpr4XSjsWOrsnHdCfVTwGEMsgQtD4s/qHzwWrVy5MqzNcc4Fn2sOPPDArL93+fLlLb/77ruWWSq7evVqyyxtnzdvnuXEqd2zZs3yfT+20D3wwAOWzznnHMunnHKK5aVLl1pm+TqPb2XLlrXMic7E12E7gHPe4ynPYVyNgKX+3D6W0GYTj9UsO2UZOX8+bG1KnOLNkv4XX3zRMstf+bm4/fbbLdevX983B+Hngm0JlStXtszrl8RzJldc4GoxbBPhCkxhT9xmiTKvqdhaedVVV1nO5DVBFHG/sK2hVq1alrkKQCax/ZXefvvtrLxfFPG7dNddd1lmS+jpp59uOYyVoNg+NX36dMtsOeI5hquG8RjM47xz3mtWWrduneVXX33VMs8BxJ8Tv6s8nnKqf+nSpS3vt99+lnndlXjfXKpUKctso+J+Yds023iSod/4i4iIiIiIiMSYbvxFREREREREYizjpf4sqStKOF32l19+scyJmGHjxMkLL7zQ8uuvv255yJAhvs9lOWOidNo5WLrGUpqgspogbCtILO1nWX3YU8CDnHjiiZY5FTSTk/yJJezEcluW1nElBJYMc/py+/btLY8ePdoyv/8FwZJOmjlzZlqvm4yGDRv6/jlLvsLG7y1bXYIyS805FTtIYml/FCcXB031z9aE5yBshaKpU6daZskzV0lhqwrzqlWrLLPEneeNqOIKIcccc4xllsfeeOONljmtmMfixGnsyazgwQn/LP/lKjU8FnG1BZZ98zPE7xGnO7PUOLE96vLLL7d82GGHWT788MMtP/7445b/85//+Pxrsourq/DzxuN8sWLbL//YgpD4HeP086CVTFg23adPH8sTJkywzJLcU0891TLb0s4//3zf1ydefySeP7mv/v3vf1tmSXMutW7d2vLatWstX3nllZbjXt5PYbduUdCUd7a2JB6n4oxl/DxmcBWqMEr92cbF8v5rrrnG8hNPPJH17QjClcJ4vOEKXeniMa5Tp06WBw4caJltUWeccYZlrrwSRL/xFxEREREREYkx3fiLiIiIiIiIxFhGSv05FZFTuVmCl0wJalywVI5l3WFr0qSJZU6T5CRVlppy3+1I2FOs/VSsWNEyy0mdi2bpcpUqVXz/fMmSJVl5P64mwdJBlluytJUlXCxlTZzwnA0tWrSwzCnqYZTIsi2EP6egEj9+Ry6++GLLtWvX9jyOZb/Tpk2zzDIxljnyddnikPi6O8MJ5Dzmbt682TIn0jvnnRIfFUFT/fnvCANLm7nvWJrOkneW3FWoUMFyUAscy5FbtmyZzqaGYtKkSZbHjBlj+YILLrD81Vdfpfy6bJeoV6+e5aAVg8aPH2+ZJeuZwmPBdddd5/k7tsfNmTPH8qhRoyw3a9bMcramkifiFOu6deta5pR9tj9cf/31ltka1rRpU8/rBpX3U//+/X0fzxVi2G7Ru3dvy7xG4ioQPFe1bdvWMs/vYR8P0sUVHj744APL6bbLFVZBx8UwWgAWLFhgmd93thSxHTLuuDoWV5no1q2b5TJlyljOVmta0HeaZf/Lli2zzDZhXrcltnJkqoWG78FjGn9OPE8W5Bqfn/+hQ4da5vUdzzdsj+A1RRD9xl9EREREREQkxnTjLyIiIiIiIhJjGamRmz59uu+fN2/e3PKIESMy8VaFAktVgkpWw9CmTRvLLBHhNMgjjjjCcuXKlS2XKlXK81osReaqALnCacqJJeFRnIpbrlw53z9naXsmscyYZYTc9w8//LBlTiMPA8ujueLE3XffbTmK+5FlgJywmlhW9u2331q+9957LdeoUcPyZZddZpmT/LkSCMvEevbsaZnlZnw8WzzYarXLLtv/Hy9XxHDOO4U9KqJS6k+ciJ6MoLa3Aw44wHIU2qZSwdUJWH7N7wVXMOHqDMyJ7Sa33nqr5WuvvdYyy+qrVq1qmeX9HTp0sMwSS07nZksPP1v8HnHyO489PCY55/2+8HH8jg0fPtxy9erVLWezpJuT8rktU6ZM8X08jz8zZsywPHny5KTeb5999rFcv359yyzjZylskLPOOsty+fLlLd93332Wf/3116S2KYp4Lj700EMts3WoqApaISqM1ofZs2db7tevn+W77rrLMtvgnnzyyaxvU1Q888wzlrt37265VatWlrN1T8eVOh566CHLnTt3tswVppLFlmC2PPG4zz/ntQbvk3i9+vXXX1seMGCA73aPHTvW9zHz588P3FYeB9myzfZSnnuSWRWH9Bt/ERERERERkRjTjb+IiIiIiIhIjGWk1J9lwh9++KHle+65x/LLL79suTCXbSWDExmDppZmC0tKWRbCCbkFmR7NshJOcA6ydetWy19++aVlli6nU8rNUlG+flQFla6xZDWT5W2dOnWyzKnUa9euzdh7pIplfYMHD7bM6bCDBg0Kc5MCS6RYes9ptyzVZDl/tWrVAt+DZdAsX3366actc/UETnFleT+/w5xkG0f8+VOqJW25xOPb6tWrfXNBsF2E7QSJLRxh4nsXZDtYDs9ScU4uDvqOcTo3W6cy1UbF6ffOeUt+27VrZ5mtddxWrszx6aefZmSb/LA0laX7LCmtVKmS5Tp16ljmROpkNWrUyDLPY1OnTk3pdbgCAr8bvJYszFjqTytWrAh5S6Jnr7328v3zsFu62M7DdrnHHnvMMluEeI6OI66uQWyfCsMtt9ximS0YPHZxhS+2H5UsWdLzWvw7Zq5uxT/nSlxczYBtu7xu47U1M1f64TmC94mJ98Qs9ec5nis18Zg9ceJElwr9xl9EREREREQkxnTjLyIiIiIiIhJjGSn1pxtuuMHyzJkzLX/88ceWW7RoYZml33HBUpB169aF+t5169a1zNIk/vm4ceNC3SZiScusWbMsf/bZZ5aXLVtmmVOkmVlyHYVVBnaG5ezEcqLFixdn7P3C/tz5YYmSc86NHDnScsOGDS1fdNFFljl5NQz8mXN7WZ7Lzymn97P8K7E0ne0BnN7MyfCcqs2SMU4yv+OOOyxzinrfvn39/jmxEVQuzu9L3NsdgrD0nO1Bb7zxRi42JyNYtshJ2r169bL81Vdf+T73m2++yd6G+Zg3b57lm2++2TJXJuAUaP57smnChAm+mYLa9F577bWU34+rCHAy9pw5c1J6Ha64wNWHClNbz44EtTeyvYWrURQlXNGB+z7sYzv3yyWXXGKZ0+15XuZ5iK18zsXjcxtU0h/29Rn99ddflsNoA2KrAK+n2VpQtmxZy2yj4kpOL774omUef4NaCZzz3gNxVRaeS9JpldZv/EVERERERERiTDf+IiIiIiIiIjGW8VJ/TpDl1FeWkrEFgJPnnXNu8uTJmd6kUJQuXdoyS/05+TsMQVNSGzRoYHnVqlVZ3w5OcK9Vq5blE0880TezTJItCskoDO0ib775pmWWLLFst0uXLqFuUzYccMABljn92jnnmjZtarlr166WueJH2DiBmqWGLNU688wzLX/00UeWWeZ79NFHe1537ty5llmOzVaXoMn1LCvjdGN+p+Iu6DvN8ti4l/pzUnr//v0tX3PNNZb5OY1Ce09BsbSRk7R5THznnXcsb9iwwXJU/t0s8Q2rvD9VQSsMFGTCfOPGjS2z9DaTq9PEAcuj2cLEY1lRwvMY237Hjx9vmatChY3nXJb985zEVQAOOuggz/MvvfRSy7zWK0w46Z5yWeofNrZ18j6O9zNB7blcgYltUGwdySX9xl9EREREREQkxnTjLyIiIiIiIhJjGS/1J07DPv744y2/+uqrlidOnOh5DqdY33///VncusxiySXLlFieGAZOyCVORA6aMJ8tLIcZPXq072M4Ub1ChQqWK1eubJltIVw9gisCRBV/5sOHD7d84403Wl6wYIHlxDL5qOHE044dO1ru2bOn5V128f5/xQ4dOliOSsnTb7/9Zpn74oUXXrD81FNPWea/j3h8c85b6k/Fixe3HFQSu+eee/rmjRs3+j4+jhYuXGiZpdw8BhTmKfZ+OC3YOefGjBljuX79+pYfeughy3fddVf2Nyxkffr0scyVL9gq9Pnnn4e6TXHB1onE43MyOH2a7U08XkqwL7/80nJie1hRwdY5tsiOHTs2F5uzQ5yc3rt3b8s//vij5aFDh3qew2t+tjJEpSUpGfvvv7/vn7P8vSjh+ea4446zzHsbruTElvco0m/8RURERERERGJMN/4iIiIiIiIiMZbVUn9avXq15VNOOcXy448/7nncfffdZ7lmzZqWr7zySstRmZTZuXNny506dbJ8zz33WGZpVxiCpvr/+eefoW5HqlhSxVUHmPfYYw/LLPUvW7Zslrcus7iCQZUqVSxzmjVLqsaNGxfKdvmpUaOGZX7eOe2Wn7lJkyZZ5ooFzhVscnSYXnrpJcssdXvwwQd3+lyW8Cf7uKBSf67Awcn/Qe0DccTJyvxedO/e3TLbwr777rtQtivTTjvtNMv8/DnnnX7dqlUry6+//nrWtyuXeOzr1auXZX4Owl4tJy7SLdVNnGD+j7feeiul1+Fxjav7BE3JjospU6ZY5nmlTZs2ltniE0dt27a1vGbNGsvTpk3LxeYUCO9bfvjhB8/fjRo1yvLHH39sma1K33zzTRa3Ln1cOYaT/GfMmJGLzck5lvqfd955ltmCx3bqqN9v6Tf+IiIiIiIiIjGmG38RERERERGRGNONv4iIiIiIiEiMhdbjT+zf5BJfzjk3b948y+yBOvzwwy2zx+L777/Pxiaavffe2zL7DZ1zrlu3bpbZl8Ue/7BxWbxNmzb55sJq+fLlvn/OJf8KAy77weXJuLQll0c66qijLHMpL+e8y9GlqlKlSpbZV3711Vdb5jyO33//3fLzzz9v+dFHH7Uc9kyLbOHPed9997UctHwaZ1TsCHv899lnH8ucU8E5J8uWLbP8/vvvJ/UecTNs2DDLnDPBZWEbNWpkmZ/TqNh9990tc04BP0889znn3IUXXmg56j2h2TJixAjLrVu3tsxeaQnP7NmzLZcvX95yqksEN2vWzHK1atUsd+3aNY2ti75BgwZZ5rJ2zz33nGXOP0ic+1FYsRf6oosussxlsHldVJgkLkl+6qmnWuaSs+z3b9mypeVPP/00exu3E+XKlbPM+5v27dtb5jKGUTy3hoE9/vx+nnXWWZafeOKJULcpHfqNv4iIiIiIiEiM6cZfREREREREJMbyki1Rdc65vLy85B+cAU2aNLE8duxYy1u3brV82WWXWX733XcL/F6lSpWy3KJFC8t9+/a1XKFCBc9zuLzQbbfdZjnVsvr8/Py8nT8qGPcLl1Y7+OCDLc+fPz+dt4gElklzSUeWIjnnXL9+/TLyfpncL8ngvmP5PD/jicuEsLT+66+/9n2tMmXKWD7ssMMsJ36e/7FkyRLf7Xj22Wctp9NikK6w9wsNGDDAMpfwZKuEc84tWrTI9/k8jrF0mbZt22b5nHPOsZzqkllhC2O/sPWEP4+ZM2da5jKSCxcuTGeTUlaxYkXLXBKJ7TP77bef5eHDh1tm65hzmWvPyuX3RXZoTn5+/vHpvEBh3TddunSx3K5dO8tc2o/HwbCF8Z3hOZpL9p599tmWhw4davmRRx7xPD+ZJUyrV69umUuHMvPcxXJ0nt+4rHIy2AbinHMfffSRZV6/16pVy3Iyx7vCdiyrWrWq5cmTJ1s+5JBDLPOagvcbQcv9BilWbHvn9vHHbz+ssPWA91TOOVevXj3f5z/wwAOWuWxukMK2X1LFVsy1a9f6PqZx48aWo9KWGbRf9Bt/ERERERERkRjTjb+IiIiIiIhIjEW61P/888+3zGmvLFFmyQwnL7L0kyWhtWvXtnzGGWdYPuGEEyxzaiNLn1hC6pxzc+fO3fk/IglxL5PJhvfee88yy/6d85ZHpyMq+6VGjRqWL730Us/fceI/y/g5ffXnn3+2zJUROKGZ35cFCxZYTuX4EJao7JcCvrdlfk6rVKlimeXpH374YTgblgFh7xdORn7mmWcscxWGSZMmeZ7z9NNPW2Y53vr16y3vsccelvfcc0/f9z7iiCMsc6WBCy64wPfxr732muXBgwdb5vklWwrz9yXmimypf9SF/Z1hmTVXs2K7EB/jnLd9k20RvD5m6xHxHMNzP1cS4ZT9gQMHWmZbAtsVuFJDjx49PO+32267WWbpOd87GYX5WFaiRAnLLKVnS8WPP/5omSuXsOyfr8NVx04++WTLXDGI+3HOnDmebeJ7jBw50nKqq8gU5v2SKq4kx9V6+F1jO0suqdRfREREREREpAjSjb+IiIiIiIhIjEW61P+KK66wzBLNdGzZssUyyyzfeecd38z2gWyVPRelMplM4SoMnPbvnHM//fRTRt5D+yWatF+iKZf7haWNnKDPFjHnnKtUqVJB3yIQp/yOGDHCMld9WblyZcbfN1n6vkSWSv0jKirfGU7Hv+qqqzx/V7NmTd/nbNiwwTJbxaZOnWp59erVvs9ly9mwYcMsN23a1PIuu+z894WvvPKK57+56lWqZeQUlf2SSY0aNbLcoUMHy3Xr1rXMNg/u382bN1tm2wRbYfkZ4HMzKY77JUj37t0tc5WtIUOG5GJzdkil/iIiIiIiIiJFkG78RURERERERGIs0qX+LG9p27at5dKlS1vmBGZOxFyzZo3lH374wfK3335reePGjZnb2DQUpTKZwkT7JZq0X6IpivuFK7Q451ytWrUs16tXzzLPI8TSSE7O/uWXXyxz5YBNmzYVfGOzJIr7RZxzKvWPLH1nvNgi1bx5c8tcMYil5itWrMjKdmi/RJP2SzSp1F9ERERERESkCNKNv4iIiIiIiEiMRbrUv6hQmUw0ab9Ek/ZLNGm/RJP2S2Sp1D+i9J2JJu2XaNJ+iSaV+ouIiIiIiIgUQbrxFxEREREREYmxYjt/iMfPzrnl2diQIqxKBl5D+yXztF+iSfslmrRfokn7Jbq0b6JJ+yWatF+iSfslmgL3S0o9/iIiIiIiIiJSuKjUX0RERERERCTGdOMvIiIiIiIiEmO68RcRERERERGJMd34i4iIiIiIiMSYbvxFREREREREYkw3/iIiIiIiIiIxpht/ERERERERkRjTjb+IiIiIiIhIjOnGX0RERERERCTG/g+Q4ShxiesRqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x1296 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    fig = no_axis_show(plt.imread(f'./real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./real_or_drawing/train_data/0/0.bmp')\n",
    "plt.figure(figsize = (18, 18))\n",
    "plt.subplot(1, 5, 1)\n",
    "no_axis_show(img, 'Original')\n",
    "\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, 'gray scale', cmap='gray')\n",
    "\n",
    "canny50100 = cv2.Canny(gray_img, 50, 100)\n",
    "plt.subplot(1, 5, 3)\n",
    "no_axis_show(canny50100, 'Canny(50, 100)', cmap='gray')\n",
    "\n",
    "canny150200 = cv2.Canny(gray_img, 150, 200)\n",
    "plt.subplot(1, 5, 4)\n",
    "no_axis_show(canny150200, 'Canny(150, 200)', cmap='gray')\n",
    "\n",
    "canny250300 = cv2.Canny(gray_img, 250, 300)\n",
    "plt.subplot(1, 5, 5)\n",
    "no_axis_show(canny250300, 'Canny(250, 300)', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "source_trans = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Lambda(lambda x : cv2.Canny(np.array(x), 170, 300)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "target_trans = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "source_set = ImageFolder('./real_or_drawing/train_data', transform=source_trans)\n",
    "target_set = ImageFolder('./real_or_drawing/test_data', transform=target_trans)\n",
    "\n",
    "source_loader = DataLoader(source_set, batch_size=32, shuffle=True)\n",
    "target_loader = DataLoader(target_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(target_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def same_seeds(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(1116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze()\n",
    "        return x\n",
    "\n",
    "class LabelPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LabelPredictor, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lambda(epoch, num_epoch):\n",
    "    p = epoch / num_epoch\n",
    "    return 2. / (1+np.exp(-10*p)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor().cuda()\n",
    "label_predictor = LabelPredictor().cuda()\n",
    "domain_classifier = DomainClassifier().cuda()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_L = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: train D loss: 0.1416, train F loss: 1.9411, acc: 0.2780\n",
      "epoch   1: train D loss: 0.1935, train F loss: 1.6095, acc: 0.4292\n",
      "epoch   2: train D loss: 0.2055, train F loss: 1.4934, acc: 0.4812\n",
      "epoch   3: train D loss: 0.1950, train F loss: 1.4099, acc: 0.5048\n",
      "epoch   4: train D loss: 0.2007, train F loss: 1.3454, acc: 0.5354\n",
      "epoch   5: train D loss: 0.2281, train F loss: 1.2964, acc: 0.5474\n",
      "epoch   6: train D loss: 0.2001, train F loss: 1.2531, acc: 0.5632\n",
      "epoch   7: train D loss: 0.1971, train F loss: 1.2214, acc: 0.5724\n",
      "epoch   8: train D loss: 0.2106, train F loss: 1.1686, acc: 0.5864\n",
      "epoch   9: train D loss: 0.2368, train F loss: 1.1666, acc: 0.5906\n",
      "epoch  10: train D loss: 0.2437, train F loss: 1.1057, acc: 0.6092\n",
      "epoch  11: train D loss: 0.2594, train F loss: 1.0746, acc: 0.6268\n",
      "epoch  12: train D loss: 0.2504, train F loss: 1.0497, acc: 0.6238\n",
      "epoch  13: train D loss: 0.2732, train F loss: 1.0000, acc: 0.6472\n",
      "epoch  14: train D loss: 0.2539, train F loss: 0.9911, acc: 0.6550\n",
      "epoch  15: train D loss: 0.2801, train F loss: 0.9427, acc: 0.6706\n",
      "epoch  16: train D loss: 0.2598, train F loss: 0.8860, acc: 0.6886\n",
      "epoch  17: train D loss: 0.2664, train F loss: 0.8630, acc: 0.6968\n",
      "epoch  18: train D loss: 0.2657, train F loss: 0.8221, acc: 0.7020\n",
      "epoch  19: train D loss: 0.2887, train F loss: 0.7918, acc: 0.7252\n",
      "epoch  20: train D loss: 0.2617, train F loss: 0.7724, acc: 0.7296\n",
      "epoch  21: train D loss: 0.2878, train F loss: 0.7298, acc: 0.7390\n",
      "epoch  22: train D loss: 0.2645, train F loss: 0.6885, acc: 0.7608\n",
      "epoch  23: train D loss: 0.2894, train F loss: 0.6660, acc: 0.7652\n",
      "epoch  24: train D loss: 0.2804, train F loss: 0.6356, acc: 0.7730\n",
      "epoch  25: train D loss: 0.2942, train F loss: 0.6216, acc: 0.7800\n",
      "epoch  26: train D loss: 0.2875, train F loss: 0.5705, acc: 0.7914\n",
      "epoch  27: train D loss: 0.2995, train F loss: 0.5584, acc: 0.7970\n",
      "epoch  28: train D loss: 0.2854, train F loss: 0.5183, acc: 0.8140\n",
      "epoch  29: train D loss: 0.2844, train F loss: 0.5043, acc: 0.8210\n",
      "epoch  30: train D loss: 0.2804, train F loss: 0.4561, acc: 0.8348\n",
      "epoch  31: train D loss: 0.2716, train F loss: 0.4383, acc: 0.8490\n",
      "epoch  32: train D loss: 0.2811, train F loss: 0.4307, acc: 0.8418\n",
      "epoch  33: train D loss: 0.2731, train F loss: 0.4078, acc: 0.8510\n",
      "epoch  34: train D loss: 0.2873, train F loss: 0.3756, acc: 0.8650\n",
      "epoch  35: train D loss: 0.2951, train F loss: 0.3486, acc: 0.8690\n",
      "epoch  36: train D loss: 0.2974, train F loss: 0.3421, acc: 0.8698\n",
      "epoch  37: train D loss: 0.2837, train F loss: 0.3229, acc: 0.8844\n",
      "epoch  38: train D loss: 0.2839, train F loss: 0.3256, acc: 0.8856\n",
      "epoch  39: train D loss: 0.2894, train F loss: 0.2891, acc: 0.8914\n",
      "epoch  40: train D loss: 0.2830, train F loss: 0.2799, acc: 0.8966\n",
      "epoch  41: train D loss: 0.3017, train F loss: 0.2725, acc: 0.8952\n",
      "epoch  42: train D loss: 0.2954, train F loss: 0.2478, acc: 0.9026\n",
      "epoch  43: train D loss: 0.3126, train F loss: 0.2231, acc: 0.9162\n",
      "epoch  44: train D loss: 0.3147, train F loss: 0.2218, acc: 0.9116\n",
      "epoch  45: train D loss: 0.3133, train F loss: 0.2275, acc: 0.9158\n",
      "epoch  46: train D loss: 0.3013, train F loss: 0.2243, acc: 0.9130\n",
      "epoch  47: train D loss: 0.3209, train F loss: 0.2076, acc: 0.9174\n",
      "epoch  48: train D loss: 0.3228, train F loss: 0.2152, acc: 0.9166\n",
      "epoch  49: train D loss: 0.3147, train F loss: 0.1866, acc: 0.9252\n",
      "epoch  50: train D loss: 0.3346, train F loss: 0.1748, acc: 0.9282\n",
      "epoch  51: train D loss: 0.3404, train F loss: 0.1560, acc: 0.9326\n",
      "epoch  52: train D loss: 0.3241, train F loss: 0.1784, acc: 0.9294\n",
      "epoch  53: train D loss: 0.3549, train F loss: 0.1748, acc: 0.9266\n",
      "epoch  54: train D loss: 0.3451, train F loss: 0.1400, acc: 0.9368\n",
      "epoch  55: train D loss: 0.3405, train F loss: 0.1301, acc: 0.9388\n",
      "epoch  56: train D loss: 0.3628, train F loss: 0.1592, acc: 0.9314\n",
      "epoch  57: train D loss: 0.3807, train F loss: 0.1231, acc: 0.9442\n",
      "epoch  58: train D loss: 0.3685, train F loss: 0.1199, acc: 0.9410\n",
      "epoch  59: train D loss: 0.3606, train F loss: 0.1257, acc: 0.9420\n",
      "epoch  60: train D loss: 0.3862, train F loss: 0.1267, acc: 0.9404\n",
      "epoch  61: train D loss: 0.3828, train F loss: 0.1035, acc: 0.9474\n",
      "epoch  62: train D loss: 0.3867, train F loss: 0.1267, acc: 0.9454\n",
      "epoch  63: train D loss: 0.3935, train F loss: 0.1127, acc: 0.9456\n",
      "epoch  64: train D loss: 0.4268, train F loss: 0.0935, acc: 0.9480\n",
      "epoch  65: train D loss: 0.4046, train F loss: 0.0954, acc: 0.9494\n",
      "epoch  66: train D loss: 0.4066, train F loss: 0.0993, acc: 0.9478\n",
      "epoch  67: train D loss: 0.3976, train F loss: 0.0720, acc: 0.9512\n",
      "epoch  68: train D loss: 0.4199, train F loss: 0.0873, acc: 0.9484\n",
      "epoch  69: train D loss: 0.4102, train F loss: 0.0734, acc: 0.9522\n",
      "epoch  70: train D loss: 0.4230, train F loss: 0.0836, acc: 0.9506\n",
      "epoch  71: train D loss: 0.4243, train F loss: 0.0713, acc: 0.9540\n",
      "epoch  72: train D loss: 0.4384, train F loss: 0.0597, acc: 0.9552\n",
      "epoch  73: train D loss: 0.4498, train F loss: 0.0538, acc: 0.9592\n",
      "epoch  74: train D loss: 0.4442, train F loss: 0.0727, acc: 0.9514\n",
      "epoch  75: train D loss: 0.4533, train F loss: 0.0620, acc: 0.9556\n",
      "epoch  76: train D loss: 0.4667, train F loss: 0.0675, acc: 0.9510\n",
      "epoch  77: train D loss: 0.4522, train F loss: 0.0504, acc: 0.9572\n",
      "epoch  78: train D loss: 0.4517, train F loss: 0.0323, acc: 0.9588\n",
      "epoch  79: train D loss: 0.4529, train F loss: 0.0492, acc: 0.9554\n",
      "epoch  80: train D loss: 0.4529, train F loss: 0.0417, acc: 0.9620\n",
      "epoch  81: train D loss: 0.4676, train F loss: 0.0431, acc: 0.9516\n",
      "epoch  82: train D loss: 0.4673, train F loss: 0.0244, acc: 0.9618\n",
      "epoch  83: train D loss: 0.4779, train F loss: 0.0233, acc: 0.9652\n",
      "epoch  84: train D loss: 0.4763, train F loss: 0.0320, acc: 0.9578\n",
      "epoch  85: train D loss: 0.4785, train F loss: 0.0094, acc: 0.9658\n",
      "epoch  86: train D loss: 0.4767, train F loss: 0.0122, acc: 0.9612\n",
      "epoch  87: train D loss: 0.4856, train F loss: 0.0098, acc: 0.9604\n",
      "epoch  88: train D loss: 0.4913, train F loss: 0.0367, acc: 0.9534\n",
      "epoch  89: train D loss: 0.4731, train F loss: 0.0230, acc: 0.9592\n",
      "epoch  90: train D loss: 0.4923, train F loss: 0.0016, acc: 0.9668\n",
      "epoch  91: train D loss: 0.5012, train F loss: 0.0079, acc: 0.9648\n",
      "epoch  92: train D loss: 0.4942, train F loss: 0.0093, acc: 0.9632\n",
      "epoch  93: train D loss: 0.4959, train F loss: 0.0047, acc: 0.9632\n",
      "epoch  94: train D loss: 0.4919, train F loss: 0.0072, acc: 0.9630\n",
      "epoch  95: train D loss: 0.4868, train F loss: 0.0035, acc: 0.9664\n",
      "epoch  96: train D loss: 0.4987, train F loss: 0.0011, acc: 0.9622\n",
      "epoch  97: train D loss: 0.5112, train F loss: -0.0139, acc: 0.9688\n",
      "epoch  98: train D loss: 0.5046, train F loss: -0.0154, acc: 0.9662\n",
      "epoch  99: train D loss: 0.5102, train F loss: -0.0034, acc: 0.9616\n",
      "epoch 100: train D loss: 0.5157, train F loss: -0.0090, acc: 0.9648\n",
      "epoch 101: train D loss: 0.5128, train F loss: -0.0082, acc: 0.9632\n",
      "epoch 102: train D loss: 0.5218, train F loss: -0.0198, acc: 0.9690\n",
      "epoch 103: train D loss: 0.5252, train F loss: -0.0208, acc: 0.9632\n",
      "epoch 104: train D loss: 0.5225, train F loss: -0.0142, acc: 0.9640\n",
      "epoch 105: train D loss: 0.5128, train F loss: -0.0261, acc: 0.9682\n",
      "epoch 106: train D loss: 0.5251, train F loss: -0.0372, acc: 0.9702\n",
      "epoch 107: train D loss: 0.5148, train F loss: -0.0304, acc: 0.9696\n",
      "epoch 108: train D loss: 0.5325, train F loss: -0.0255, acc: 0.9686\n",
      "epoch 109: train D loss: 0.5295, train F loss: -0.0393, acc: 0.9682\n",
      "epoch 110: train D loss: 0.5334, train F loss: -0.0374, acc: 0.9640\n",
      "epoch 111: train D loss: 0.5416, train F loss: -0.0493, acc: 0.9710\n",
      "epoch 112: train D loss: 0.5348, train F loss: -0.0341, acc: 0.9664\n",
      "epoch 113: train D loss: 0.5341, train F loss: -0.0403, acc: 0.9672\n",
      "epoch 114: train D loss: 0.5404, train F loss: -0.0564, acc: 0.9716\n",
      "epoch 115: train D loss: 0.5393, train F loss: -0.0475, acc: 0.9686\n",
      "epoch 116: train D loss: 0.5349, train F loss: -0.0636, acc: 0.9742\n",
      "epoch 117: train D loss: 0.5320, train F loss: -0.0421, acc: 0.9682\n",
      "epoch 118: train D loss: 0.5313, train F loss: -0.0591, acc: 0.9724\n",
      "epoch 119: train D loss: 0.5401, train F loss: -0.0627, acc: 0.9720\n",
      "epoch 120: train D loss: 0.5354, train F loss: -0.0494, acc: 0.9664\n",
      "epoch 121: train D loss: 0.5420, train F loss: -0.0748, acc: 0.9724\n",
      "epoch 122: train D loss: 0.5469, train F loss: -0.0629, acc: 0.9694\n",
      "epoch 123: train D loss: 0.5401, train F loss: -0.0454, acc: 0.9640\n",
      "epoch 124: train D loss: 0.5459, train F loss: -0.0812, acc: 0.9736\n",
      "epoch 125: train D loss: 0.5499, train F loss: -0.0642, acc: 0.9710\n",
      "epoch 126: train D loss: 0.5391, train F loss: -0.0747, acc: 0.9718\n",
      "epoch 127: train D loss: 0.5603, train F loss: -0.0823, acc: 0.9726\n",
      "epoch 128: train D loss: 0.5468, train F loss: -0.0647, acc: 0.9704\n",
      "epoch 129: train D loss: 0.5468, train F loss: -0.0734, acc: 0.9684\n",
      "epoch 130: train D loss: 0.5463, train F loss: -0.0748, acc: 0.9698\n",
      "epoch 131: train D loss: 0.5397, train F loss: -0.0682, acc: 0.9698\n",
      "epoch 132: train D loss: 0.5426, train F loss: -0.0884, acc: 0.9760\n",
      "epoch 133: train D loss: 0.5579, train F loss: -0.0858, acc: 0.9722\n",
      "epoch 134: train D loss: 0.5451, train F loss: -0.0666, acc: 0.9662\n",
      "epoch 135: train D loss: 0.5535, train F loss: -0.0916, acc: 0.9734\n",
      "epoch 136: train D loss: 0.5510, train F loss: -0.0828, acc: 0.9710\n",
      "epoch 137: train D loss: 0.5525, train F loss: -0.0975, acc: 0.9762\n",
      "epoch 138: train D loss: 0.5681, train F loss: -0.0948, acc: 0.9762\n",
      "epoch 139: train D loss: 0.5591, train F loss: -0.0822, acc: 0.9672\n",
      "epoch 140: train D loss: 0.5528, train F loss: -0.0956, acc: 0.9746\n",
      "epoch 141: train D loss: 0.5539, train F loss: -0.1075, acc: 0.9754\n",
      "epoch 142: train D loss: 0.5634, train F loss: -0.1089, acc: 0.9742\n",
      "epoch 143: train D loss: 0.5683, train F loss: -0.1032, acc: 0.9738\n",
      "epoch 144: train D loss: 0.5576, train F loss: -0.0936, acc: 0.9696\n",
      "epoch 145: train D loss: 0.5564, train F loss: -0.1147, acc: 0.9780\n",
      "epoch 146: train D loss: 0.5742, train F loss: -0.1193, acc: 0.9762\n",
      "epoch 147: train D loss: 0.5639, train F loss: -0.1109, acc: 0.9736\n",
      "epoch 148: train D loss: 0.5685, train F loss: -0.1156, acc: 0.9766\n",
      "epoch 149: train D loss: 0.5799, train F loss: -0.1321, acc: 0.9782\n",
      "epoch 150: train D loss: 0.5711, train F loss: -0.1280, acc: 0.9792\n",
      "epoch 151: train D loss: 0.5738, train F loss: -0.1168, acc: 0.9744\n",
      "epoch 152: train D loss: 0.5672, train F loss: -0.1200, acc: 0.9758\n",
      "epoch 153: train D loss: 0.5615, train F loss: -0.1261, acc: 0.9764\n",
      "epoch 154: train D loss: 0.5781, train F loss: -0.1180, acc: 0.9700\n",
      "epoch 155: train D loss: 0.5789, train F loss: -0.1296, acc: 0.9760\n",
      "epoch 156: train D loss: 0.5772, train F loss: -0.1368, acc: 0.9744\n",
      "epoch 157: train D loss: 0.5784, train F loss: -0.1407, acc: 0.9782\n",
      "epoch 158: train D loss: 0.5835, train F loss: -0.1371, acc: 0.9742\n",
      "epoch 159: train D loss: 0.5811, train F loss: -0.1214, acc: 0.9692\n",
      "epoch 160: train D loss: 0.5743, train F loss: -0.1305, acc: 0.9740\n",
      "epoch 161: train D loss: 0.5814, train F loss: -0.1329, acc: 0.9730\n",
      "epoch 162: train D loss: 0.5644, train F loss: -0.1271, acc: 0.9736\n",
      "epoch 163: train D loss: 0.5840, train F loss: -0.1453, acc: 0.9782\n",
      "epoch 164: train D loss: 0.5814, train F loss: -0.1453, acc: 0.9754\n",
      "epoch 165: train D loss: 0.5869, train F loss: -0.1509, acc: 0.9752\n",
      "epoch 166: train D loss: 0.5795, train F loss: -0.1498, acc: 0.9778\n",
      "epoch 167: train D loss: 0.5933, train F loss: -0.1473, acc: 0.9738\n",
      "epoch 168: train D loss: 0.5817, train F loss: -0.1248, acc: 0.9718\n",
      "epoch 169: train D loss: 0.5895, train F loss: -0.1581, acc: 0.9772\n",
      "epoch 170: train D loss: 0.5964, train F loss: -0.1579, acc: 0.9744\n",
      "epoch 171: train D loss: 0.5788, train F loss: -0.1448, acc: 0.9768\n",
      "epoch 172: train D loss: 0.5917, train F loss: -0.1525, acc: 0.9744\n",
      "epoch 173: train D loss: 0.5872, train F loss: -0.1518, acc: 0.9752\n",
      "epoch 174: train D loss: 0.5957, train F loss: -0.1683, acc: 0.9778\n",
      "epoch 175: train D loss: 0.5797, train F loss: -0.1613, acc: 0.9776\n",
      "epoch 176: train D loss: 0.5908, train F loss: -0.1559, acc: 0.9746\n",
      "epoch 177: train D loss: 0.5981, train F loss: -0.1635, acc: 0.9730\n",
      "epoch 178: train D loss: 0.5933, train F loss: -0.1705, acc: 0.9762\n",
      "epoch 179: train D loss: 0.5989, train F loss: -0.1670, acc: 0.9756\n",
      "epoch 180: train D loss: 0.6004, train F loss: -0.1626, acc: 0.9736\n",
      "epoch 181: train D loss: 0.6020, train F loss: -0.1732, acc: 0.9768\n",
      "epoch 182: train D loss: 0.5863, train F loss: -0.1706, acc: 0.9756\n",
      "epoch 183: train D loss: 0.5953, train F loss: -0.1762, acc: 0.9768\n",
      "epoch 184: train D loss: 0.5955, train F loss: -0.1871, acc: 0.9780\n",
      "epoch 185: train D loss: 0.5977, train F loss: -0.1765, acc: 0.9774\n",
      "epoch 186: train D loss: 0.6041, train F loss: -0.1841, acc: 0.9760\n",
      "epoch 187: train D loss: 0.5952, train F loss: -0.1858, acc: 0.9792\n",
      "epoch 188: train D loss: 0.5938, train F loss: -0.1401, acc: 0.9662\n",
      "epoch 189: train D loss: 0.5970, train F loss: -0.1860, acc: 0.9778\n",
      "epoch 190: train D loss: 0.6037, train F loss: -0.1990, acc: 0.9818\n",
      "epoch 191: train D loss: 0.6094, train F loss: -0.2079, acc: 0.9810\n",
      "epoch 192: train D loss: 0.6119, train F loss: -0.1845, acc: 0.9756\n",
      "epoch 193: train D loss: 0.6071, train F loss: -0.1950, acc: 0.9780\n",
      "epoch 194: train D loss: 0.5978, train F loss: -0.1928, acc: 0.9782\n",
      "epoch 195: train D loss: 0.6052, train F loss: -0.2020, acc: 0.9778\n",
      "epoch 196: train D loss: 0.6076, train F loss: -0.2079, acc: 0.9814\n",
      "epoch 197: train D loss: 0.6052, train F loss: -0.1945, acc: 0.9800\n",
      "epoch 198: train D loss: 0.6070, train F loss: -0.2001, acc: 0.9776\n",
      "epoch 199: train D loss: 0.6029, train F loss: -0.1961, acc: 0.9774\n",
      "epoch 200: train D loss: 0.6146, train F loss: -0.2059, acc: 0.9776\n",
      "epoch 201: train D loss: 0.6030, train F loss: -0.2068, acc: 0.9808\n",
      "epoch 202: train D loss: 0.6089, train F loss: -0.2043, acc: 0.9776\n",
      "epoch 203: train D loss: 0.6120, train F loss: -0.2127, acc: 0.9778\n",
      "epoch 204: train D loss: 0.6080, train F loss: -0.2140, acc: 0.9782\n",
      "epoch 205: train D loss: 0.6124, train F loss: -0.2147, acc: 0.9786\n",
      "epoch 206: train D loss: 0.6047, train F loss: -0.2127, acc: 0.9778\n",
      "epoch 207: train D loss: 0.6112, train F loss: -0.2094, acc: 0.9790\n",
      "epoch 208: train D loss: 0.6021, train F loss: -0.2095, acc: 0.9776\n",
      "epoch 209: train D loss: 0.6101, train F loss: -0.2162, acc: 0.9778\n",
      "epoch 210: train D loss: 0.6174, train F loss: -0.2297, acc: 0.9812\n",
      "epoch 211: train D loss: 0.6122, train F loss: -0.2163, acc: 0.9772\n",
      "epoch 212: train D loss: 0.6193, train F loss: -0.2295, acc: 0.9768\n",
      "epoch 213: train D loss: 0.6046, train F loss: -0.2146, acc: 0.9778\n",
      "epoch 214: train D loss: 0.6179, train F loss: -0.2157, acc: 0.9758\n",
      "epoch 215: train D loss: 0.6126, train F loss: -0.2179, acc: 0.9756\n",
      "epoch 216: train D loss: 0.6118, train F loss: -0.2221, acc: 0.9762\n",
      "epoch 217: train D loss: 0.6161, train F loss: -0.2366, acc: 0.9782\n",
      "epoch 218: train D loss: 0.6176, train F loss: -0.2406, acc: 0.9812\n",
      "epoch 219: train D loss: 0.6133, train F loss: -0.2284, acc: 0.9774\n",
      "epoch 220: train D loss: 0.6198, train F loss: -0.2380, acc: 0.9800\n",
      "epoch 221: train D loss: 0.6209, train F loss: -0.2386, acc: 0.9788\n",
      "epoch 222: train D loss: 0.6127, train F loss: -0.2376, acc: 0.9824\n",
      "epoch 223: train D loss: 0.6240, train F loss: -0.2365, acc: 0.9790\n",
      "epoch 224: train D loss: 0.6128, train F loss: -0.2305, acc: 0.9758\n",
      "epoch 225: train D loss: 0.6098, train F loss: -0.2210, acc: 0.9754\n",
      "epoch 226: train D loss: 0.6140, train F loss: -0.2386, acc: 0.9792\n",
      "epoch 227: train D loss: 0.6269, train F loss: -0.2486, acc: 0.9780\n",
      "epoch 228: train D loss: 0.6148, train F loss: -0.2560, acc: 0.9836\n",
      "epoch 229: train D loss: 0.6173, train F loss: -0.2436, acc: 0.9808\n",
      "epoch 230: train D loss: 0.6214, train F loss: -0.2544, acc: 0.9810\n",
      "epoch 231: train D loss: 0.6220, train F loss: -0.2489, acc: 0.9788\n",
      "epoch 232: train D loss: 0.6202, train F loss: -0.2487, acc: 0.9794\n",
      "epoch 233: train D loss: 0.6167, train F loss: -0.2435, acc: 0.9774\n",
      "epoch 234: train D loss: 0.6124, train F loss: -0.2576, acc: 0.9822\n",
      "epoch 235: train D loss: 0.6219, train F loss: -0.2453, acc: 0.9764\n",
      "epoch 236: train D loss: 0.6105, train F loss: -0.2550, acc: 0.9804\n",
      "epoch 237: train D loss: 0.6250, train F loss: -0.2523, acc: 0.9740\n",
      "epoch 238: train D loss: 0.6234, train F loss: -0.2599, acc: 0.9822\n",
      "epoch 239: train D loss: 0.6207, train F loss: -0.2583, acc: 0.9816\n",
      "epoch 240: train D loss: 0.6185, train F loss: -0.2620, acc: 0.9800\n",
      "epoch 241: train D loss: 0.6334, train F loss: -0.2803, acc: 0.9802\n",
      "epoch 242: train D loss: 0.6118, train F loss: -0.2653, acc: 0.9812\n",
      "epoch 243: train D loss: 0.6262, train F loss: -0.2625, acc: 0.9788\n",
      "epoch 244: train D loss: 0.6241, train F loss: -0.2632, acc: 0.9766\n",
      "epoch 245: train D loss: 0.6269, train F loss: -0.2663, acc: 0.9796\n",
      "epoch 246: train D loss: 0.6207, train F loss: -0.2689, acc: 0.9832\n",
      "epoch 247: train D loss: 0.6283, train F loss: -0.2797, acc: 0.9806\n",
      "epoch 248: train D loss: 0.6229, train F loss: -0.2587, acc: 0.9778\n",
      "epoch 249: train D loss: 0.6182, train F loss: -0.2705, acc: 0.9794\n",
      "epoch 250: train D loss: 0.6278, train F loss: -0.2877, acc: 0.9842\n",
      "epoch 251: train D loss: 0.6332, train F loss: -0.2841, acc: 0.9808\n",
      "epoch 252: train D loss: 0.6324, train F loss: -0.2848, acc: 0.9814\n",
      "epoch 253: train D loss: 0.6311, train F loss: -0.2788, acc: 0.9800\n",
      "epoch 254: train D loss: 0.6264, train F loss: -0.2793, acc: 0.9804\n",
      "epoch 255: train D loss: 0.6313, train F loss: -0.2970, acc: 0.9836\n",
      "epoch 256: train D loss: 0.6292, train F loss: -0.2883, acc: 0.9822\n",
      "epoch 257: train D loss: 0.6319, train F loss: -0.2843, acc: 0.9816\n",
      "epoch 258: train D loss: 0.6246, train F loss: -0.2806, acc: 0.9798\n",
      "epoch 259: train D loss: 0.6295, train F loss: -0.2876, acc: 0.9790\n",
      "epoch 260: train D loss: 0.6301, train F loss: -0.2858, acc: 0.9788\n",
      "epoch 261: train D loss: 0.6343, train F loss: -0.3005, acc: 0.9826\n",
      "epoch 262: train D loss: 0.6277, train F loss: -0.2759, acc: 0.9764\n",
      "epoch 263: train D loss: 0.6324, train F loss: -0.2813, acc: 0.9786\n",
      "epoch 264: train D loss: 0.6322, train F loss: -0.2931, acc: 0.9802\n",
      "epoch 265: train D loss: 0.6245, train F loss: -0.2886, acc: 0.9784\n",
      "epoch 266: train D loss: 0.6235, train F loss: -0.2983, acc: 0.9832\n",
      "epoch 267: train D loss: 0.6329, train F loss: -0.3018, acc: 0.9824\n",
      "epoch 268: train D loss: 0.6234, train F loss: -0.2980, acc: 0.9834\n",
      "epoch 269: train D loss: 0.6278, train F loss: -0.3002, acc: 0.9800\n",
      "epoch 270: train D loss: 0.6383, train F loss: -0.3094, acc: 0.9798\n",
      "epoch 271: train D loss: 0.6342, train F loss: -0.3006, acc: 0.9802\n",
      "epoch 272: train D loss: 0.6322, train F loss: -0.2975, acc: 0.9792\n",
      "epoch 273: train D loss: 0.6357, train F loss: -0.3089, acc: 0.9812\n",
      "epoch 274: train D loss: 0.6256, train F loss: -0.3103, acc: 0.9820\n",
      "epoch 275: train D loss: 0.6357, train F loss: -0.3167, acc: 0.9814\n",
      "epoch 276: train D loss: 0.6405, train F loss: -0.3144, acc: 0.9802\n",
      "epoch 277: train D loss: 0.6345, train F loss: -0.3170, acc: 0.9826\n",
      "epoch 278: train D loss: 0.6392, train F loss: -0.3096, acc: 0.9792\n",
      "epoch 279: train D loss: 0.6258, train F loss: -0.3051, acc: 0.9796\n",
      "epoch 280: train D loss: 0.6287, train F loss: -0.3162, acc: 0.9828\n",
      "epoch 281: train D loss: 0.6402, train F loss: -0.3327, acc: 0.9852\n",
      "epoch 282: train D loss: 0.6311, train F loss: -0.3194, acc: 0.9830\n",
      "epoch 283: train D loss: 0.6407, train F loss: -0.3197, acc: 0.9812\n",
      "epoch 284: train D loss: 0.6321, train F loss: -0.3265, acc: 0.9824\n",
      "epoch 285: train D loss: 0.6376, train F loss: -0.3176, acc: 0.9840\n",
      "epoch 286: train D loss: 0.6258, train F loss: -0.2916, acc: 0.9740\n",
      "epoch 287: train D loss: 0.6308, train F loss: -0.3062, acc: 0.9792\n",
      "epoch 288: train D loss: 0.6330, train F loss: -0.3271, acc: 0.9826\n",
      "epoch 289: train D loss: 0.6334, train F loss: -0.3248, acc: 0.9812\n",
      "epoch 290: train D loss: 0.6299, train F loss: -0.3261, acc: 0.9814\n",
      "epoch 291: train D loss: 0.6304, train F loss: -0.3204, acc: 0.9814\n",
      "epoch 292: train D loss: 0.6415, train F loss: -0.3396, acc: 0.9818\n",
      "epoch 293: train D loss: 0.6370, train F loss: -0.3330, acc: 0.9814\n",
      "epoch 294: train D loss: 0.6331, train F loss: -0.3295, acc: 0.9840\n",
      "epoch 295: train D loss: 0.6318, train F loss: -0.3415, acc: 0.9866\n",
      "epoch 296: train D loss: 0.6399, train F loss: -0.3383, acc: 0.9848\n",
      "epoch 297: train D loss: 0.6380, train F loss: -0.3251, acc: 0.9782\n",
      "epoch 298: train D loss: 0.6283, train F loss: -0.3326, acc: 0.9852\n",
      "epoch 299: train D loss: 0.6326, train F loss: -0.3377, acc: 0.9812\n",
      "epoch 300: train D loss: 0.6394, train F loss: -0.3507, acc: 0.9846\n",
      "epoch 301: train D loss: 0.6383, train F loss: -0.3293, acc: 0.9768\n",
      "epoch 302: train D loss: 0.6320, train F loss: -0.3356, acc: 0.9822\n",
      "epoch 303: train D loss: 0.6470, train F loss: -0.3473, acc: 0.9826\n",
      "epoch 304: train D loss: 0.6296, train F loss: -0.3407, acc: 0.9834\n",
      "epoch 305: train D loss: 0.6370, train F loss: -0.3507, acc: 0.9844\n",
      "epoch 306: train D loss: 0.6404, train F loss: -0.3473, acc: 0.9828\n",
      "epoch 307: train D loss: 0.6443, train F loss: -0.3546, acc: 0.9826\n",
      "epoch 308: train D loss: 0.6428, train F loss: -0.3489, acc: 0.9816\n",
      "epoch 309: train D loss: 0.6372, train F loss: -0.3450, acc: 0.9808\n",
      "epoch 310: train D loss: 0.6394, train F loss: -0.3412, acc: 0.9804\n",
      "epoch 311: train D loss: 0.6398, train F loss: -0.3393, acc: 0.9796\n",
      "epoch 312: train D loss: 0.6391, train F loss: -0.3568, acc: 0.9844\n",
      "epoch 313: train D loss: 0.6398, train F loss: -0.3608, acc: 0.9840\n",
      "epoch 314: train D loss: 0.6422, train F loss: -0.3590, acc: 0.9836\n",
      "epoch 315: train D loss: 0.6415, train F loss: -0.3610, acc: 0.9836\n",
      "epoch 316: train D loss: 0.6312, train F loss: -0.3556, acc: 0.9832\n",
      "epoch 317: train D loss: 0.6449, train F loss: -0.3554, acc: 0.9798\n",
      "epoch 318: train D loss: 0.6374, train F loss: -0.3598, acc: 0.9852\n",
      "epoch 319: train D loss: 0.6337, train F loss: -0.3472, acc: 0.9824\n",
      "epoch 320: train D loss: 0.6427, train F loss: -0.3619, acc: 0.9810\n",
      "epoch 321: train D loss: 0.6503, train F loss: -0.3741, acc: 0.9826\n",
      "epoch 322: train D loss: 0.6424, train F loss: -0.3625, acc: 0.9818\n",
      "epoch 323: train D loss: 0.6409, train F loss: -0.3690, acc: 0.9848\n",
      "epoch 324: train D loss: 0.6384, train F loss: -0.3520, acc: 0.9808\n",
      "epoch 325: train D loss: 0.6429, train F loss: -0.3654, acc: 0.9810\n",
      "epoch 326: train D loss: 0.6412, train F loss: -0.3622, acc: 0.9814\n",
      "epoch 327: train D loss: 0.6446, train F loss: -0.3619, acc: 0.9788\n",
      "epoch 328: train D loss: 0.6367, train F loss: -0.3583, acc: 0.9828\n",
      "epoch 329: train D loss: 0.6346, train F loss: -0.3553, acc: 0.9804\n",
      "epoch 330: train D loss: 0.6434, train F loss: -0.3766, acc: 0.9818\n",
      "epoch 331: train D loss: 0.6444, train F loss: -0.3739, acc: 0.9846\n",
      "epoch 332: train D loss: 0.6442, train F loss: -0.3787, acc: 0.9820\n",
      "epoch 333: train D loss: 0.6385, train F loss: -0.3558, acc: 0.9784\n",
      "epoch 334: train D loss: 0.6420, train F loss: -0.3758, acc: 0.9828\n",
      "epoch 335: train D loss: 0.6452, train F loss: -0.3857, acc: 0.9864\n",
      "epoch 336: train D loss: 0.6442, train F loss: -0.3768, acc: 0.9826\n",
      "epoch 337: train D loss: 0.6350, train F loss: -0.3806, acc: 0.9858\n",
      "epoch 338: train D loss: 0.6455, train F loss: -0.3913, acc: 0.9866\n",
      "epoch 339: train D loss: 0.6422, train F loss: -0.3812, acc: 0.9802\n",
      "epoch 340: train D loss: 0.6454, train F loss: -0.3957, acc: 0.9852\n",
      "epoch 341: train D loss: 0.6518, train F loss: -0.3720, acc: 0.9780\n",
      "epoch 342: train D loss: 0.6461, train F loss: -0.3877, acc: 0.9822\n",
      "epoch 343: train D loss: 0.6462, train F loss: -0.3820, acc: 0.9808\n",
      "epoch 344: train D loss: 0.6463, train F loss: -0.3954, acc: 0.9840\n",
      "epoch 345: train D loss: 0.6465, train F loss: -0.3985, acc: 0.9856\n",
      "epoch 346: train D loss: 0.6538, train F loss: -0.3902, acc: 0.9814\n",
      "epoch 347: train D loss: 0.6423, train F loss: -0.3966, acc: 0.9850\n",
      "epoch 348: train D loss: 0.6499, train F loss: -0.3956, acc: 0.9830\n",
      "epoch 349: train D loss: 0.6512, train F loss: -0.4052, acc: 0.9846\n",
      "epoch 350: train D loss: 0.6462, train F loss: -0.4006, acc: 0.9858\n",
      "epoch 351: train D loss: 0.6461, train F loss: -0.3938, acc: 0.9844\n",
      "epoch 352: train D loss: 0.6509, train F loss: -0.4006, acc: 0.9836\n",
      "epoch 353: train D loss: 0.6450, train F loss: -0.3894, acc: 0.9816\n",
      "epoch 354: train D loss: 0.6525, train F loss: -0.3901, acc: 0.9788\n",
      "epoch 355: train D loss: 0.6475, train F loss: -0.3830, acc: 0.9792\n",
      "epoch 356: train D loss: 0.6439, train F loss: -0.4092, acc: 0.9868\n",
      "epoch 357: train D loss: 0.6418, train F loss: -0.3969, acc: 0.9842\n",
      "epoch 358: train D loss: 0.6444, train F loss: -0.3967, acc: 0.9812\n",
      "epoch 359: train D loss: 0.6522, train F loss: -0.4014, acc: 0.9810\n",
      "epoch 360: train D loss: 0.6426, train F loss: -0.4013, acc: 0.9826\n",
      "epoch 361: train D loss: 0.6477, train F loss: -0.4097, acc: 0.9842\n",
      "epoch 362: train D loss: 0.6470, train F loss: -0.3915, acc: 0.9792\n",
      "epoch 363: train D loss: 0.6508, train F loss: -0.4157, acc: 0.9862\n",
      "epoch 364: train D loss: 0.6499, train F loss: -0.4089, acc: 0.9808\n",
      "epoch 365: train D loss: 0.6438, train F loss: -0.4070, acc: 0.9844\n",
      "epoch 366: train D loss: 0.6474, train F loss: -0.4109, acc: 0.9844\n",
      "epoch 367: train D loss: 0.6460, train F loss: -0.4024, acc: 0.9858\n",
      "epoch 368: train D loss: 0.6501, train F loss: -0.4036, acc: 0.9816\n",
      "epoch 369: train D loss: 0.6584, train F loss: -0.4213, acc: 0.9852\n",
      "epoch 370: train D loss: 0.6590, train F loss: -0.4193, acc: 0.9804\n",
      "epoch 371: train D loss: 0.6549, train F loss: -0.4188, acc: 0.9844\n",
      "epoch 372: train D loss: 0.6458, train F loss: -0.4168, acc: 0.9864\n",
      "epoch 373: train D loss: 0.6468, train F loss: -0.3999, acc: 0.9786\n",
      "epoch 374: train D loss: 0.6533, train F loss: -0.3991, acc: 0.9810\n",
      "epoch 375: train D loss: 0.6448, train F loss: -0.4175, acc: 0.9864\n",
      "epoch 376: train D loss: 0.6481, train F loss: -0.4171, acc: 0.9824\n",
      "epoch 377: train D loss: 0.6450, train F loss: -0.4206, acc: 0.9876\n",
      "epoch 378: train D loss: 0.6540, train F loss: -0.4222, acc: 0.9846\n",
      "epoch 379: train D loss: 0.6564, train F loss: -0.4317, acc: 0.9852\n",
      "epoch 380: train D loss: 0.6578, train F loss: -0.4373, acc: 0.9874\n",
      "epoch 381: train D loss: 0.6499, train F loss: -0.4152, acc: 0.9814\n",
      "epoch 382: train D loss: 0.6482, train F loss: -0.4274, acc: 0.9836\n",
      "epoch 383: train D loss: 0.6502, train F loss: -0.4243, acc: 0.9844\n",
      "epoch 384: train D loss: 0.6544, train F loss: -0.4307, acc: 0.9852\n",
      "epoch 385: train D loss: 0.6450, train F loss: -0.4236, acc: 0.9838\n",
      "epoch 386: train D loss: 0.6490, train F loss: -0.4368, acc: 0.9872\n",
      "epoch 387: train D loss: 0.6571, train F loss: -0.4358, acc: 0.9848\n",
      "epoch 388: train D loss: 0.6461, train F loss: -0.4179, acc: 0.9810\n",
      "epoch 389: train D loss: 0.6550, train F loss: -0.4324, acc: 0.9842\n",
      "epoch 390: train D loss: 0.6479, train F loss: -0.4182, acc: 0.9816\n",
      "epoch 391: train D loss: 0.6461, train F loss: -0.4189, acc: 0.9836\n",
      "epoch 392: train D loss: 0.6453, train F loss: -0.4273, acc: 0.9838\n",
      "epoch 393: train D loss: 0.6538, train F loss: -0.4168, acc: 0.9800\n",
      "epoch 394: train D loss: 0.6505, train F loss: -0.4376, acc: 0.9878\n",
      "epoch 395: train D loss: 0.6517, train F loss: -0.4270, acc: 0.9846\n",
      "epoch 396: train D loss: 0.6477, train F loss: -0.4368, acc: 0.9836\n",
      "epoch 397: train D loss: 0.6504, train F loss: -0.4271, acc: 0.9840\n",
      "epoch 398: train D loss: 0.6539, train F loss: -0.4315, acc: 0.9836\n",
      "epoch 399: train D loss: 0.6508, train F loss: -0.4372, acc: 0.9866\n",
      "epoch 400: train D loss: 0.6484, train F loss: -0.4247, acc: 0.9804\n",
      "epoch 401: train D loss: 0.6502, train F loss: -0.4412, acc: 0.9856\n",
      "epoch 402: train D loss: 0.6575, train F loss: -0.4460, acc: 0.9848\n",
      "epoch 403: train D loss: 0.6486, train F loss: -0.4331, acc: 0.9850\n",
      "epoch 404: train D loss: 0.6540, train F loss: -0.4509, acc: 0.9872\n",
      "epoch 405: train D loss: 0.6516, train F loss: -0.4483, acc: 0.9866\n",
      "epoch 406: train D loss: 0.6572, train F loss: -0.4473, acc: 0.9844\n",
      "epoch 407: train D loss: 0.6612, train F loss: -0.4491, acc: 0.9838\n",
      "epoch 408: train D loss: 0.6590, train F loss: -0.4619, acc: 0.9884\n",
      "epoch 409: train D loss: 0.6569, train F loss: -0.4503, acc: 0.9846\n",
      "epoch 410: train D loss: 0.6528, train F loss: -0.4406, acc: 0.9830\n",
      "epoch 411: train D loss: 0.6483, train F loss: -0.4406, acc: 0.9854\n",
      "epoch 412: train D loss: 0.6632, train F loss: -0.4601, acc: 0.9846\n",
      "epoch 413: train D loss: 0.6556, train F loss: -0.4563, acc: 0.9848\n",
      "epoch 414: train D loss: 0.6565, train F loss: -0.4421, acc: 0.9828\n",
      "epoch 415: train D loss: 0.6565, train F loss: -0.4559, acc: 0.9854\n",
      "epoch 416: train D loss: 0.6534, train F loss: -0.4582, acc: 0.9868\n",
      "epoch 417: train D loss: 0.6564, train F loss: -0.4690, acc: 0.9876\n",
      "epoch 418: train D loss: 0.6557, train F loss: -0.4543, acc: 0.9862\n",
      "epoch 419: train D loss: 0.6568, train F loss: -0.4580, acc: 0.9852\n",
      "epoch 420: train D loss: 0.6570, train F loss: -0.4581, acc: 0.9844\n",
      "epoch 421: train D loss: 0.6595, train F loss: -0.4615, acc: 0.9864\n",
      "epoch 422: train D loss: 0.6589, train F loss: -0.4613, acc: 0.9834\n",
      "epoch 423: train D loss: 0.6539, train F loss: -0.4493, acc: 0.9844\n",
      "epoch 424: train D loss: 0.6538, train F loss: -0.4467, acc: 0.9872\n",
      "epoch 425: train D loss: 0.6596, train F loss: -0.4557, acc: 0.9808\n",
      "epoch 426: train D loss: 0.6576, train F loss: -0.4627, acc: 0.9848\n",
      "epoch 427: train D loss: 0.6580, train F loss: -0.4701, acc: 0.9852\n",
      "epoch 428: train D loss: 0.6466, train F loss: -0.4596, acc: 0.9868\n",
      "epoch 429: train D loss: 0.6542, train F loss: -0.4525, acc: 0.9838\n",
      "epoch 430: train D loss: 0.6567, train F loss: -0.4622, acc: 0.9828\n",
      "epoch 431: train D loss: 0.6566, train F loss: -0.4670, acc: 0.9838\n",
      "epoch 432: train D loss: 0.6564, train F loss: -0.4696, acc: 0.9858\n",
      "epoch 433: train D loss: 0.6584, train F loss: -0.4615, acc: 0.9808\n",
      "epoch 434: train D loss: 0.6546, train F loss: -0.4612, acc: 0.9858\n",
      "epoch 435: train D loss: 0.6572, train F loss: -0.4688, acc: 0.9858\n",
      "epoch 436: train D loss: 0.6547, train F loss: -0.4607, acc: 0.9828\n",
      "epoch 437: train D loss: 0.6537, train F loss: -0.4667, acc: 0.9858\n",
      "epoch 438: train D loss: 0.6552, train F loss: -0.4723, acc: 0.9848\n",
      "epoch 439: train D loss: 0.6566, train F loss: -0.4675, acc: 0.9842\n",
      "epoch 440: train D loss: 0.6579, train F loss: -0.4742, acc: 0.9858\n",
      "epoch 441: train D loss: 0.6630, train F loss: -0.4689, acc: 0.9848\n",
      "epoch 442: train D loss: 0.6567, train F loss: -0.4730, acc: 0.9860\n",
      "epoch 443: train D loss: 0.6562, train F loss: -0.4733, acc: 0.9868\n",
      "epoch 444: train D loss: 0.6622, train F loss: -0.4851, acc: 0.9874\n",
      "epoch 445: train D loss: 0.6534, train F loss: -0.4764, acc: 0.9864\n",
      "epoch 446: train D loss: 0.6558, train F loss: -0.4727, acc: 0.9858\n",
      "epoch 447: train D loss: 0.6582, train F loss: -0.4756, acc: 0.9868\n",
      "epoch 448: train D loss: 0.6550, train F loss: -0.4725, acc: 0.9850\n",
      "epoch 449: train D loss: 0.6556, train F loss: -0.4741, acc: 0.9850\n",
      "epoch 450: train D loss: 0.6546, train F loss: -0.4721, acc: 0.9852\n",
      "epoch 451: train D loss: 0.6577, train F loss: -0.4802, acc: 0.9856\n",
      "epoch 452: train D loss: 0.6574, train F loss: -0.4692, acc: 0.9838\n",
      "epoch 453: train D loss: 0.6607, train F loss: -0.4733, acc: 0.9866\n",
      "epoch 454: train D loss: 0.6628, train F loss: -0.4869, acc: 0.9856\n",
      "epoch 455: train D loss: 0.6531, train F loss: -0.4800, acc: 0.9878\n",
      "epoch 456: train D loss: 0.6516, train F loss: -0.4727, acc: 0.9848\n",
      "epoch 457: train D loss: 0.6557, train F loss: -0.4832, acc: 0.9852\n",
      "epoch 458: train D loss: 0.6662, train F loss: -0.4869, acc: 0.9870\n",
      "epoch 459: train D loss: 0.6596, train F loss: -0.4935, acc: 0.9864\n",
      "epoch 460: train D loss: 0.6565, train F loss: -0.4857, acc: 0.9860\n",
      "epoch 461: train D loss: 0.6578, train F loss: -0.4893, acc: 0.9870\n",
      "epoch 462: train D loss: 0.6516, train F loss: -0.4769, acc: 0.9844\n",
      "epoch 463: train D loss: 0.6586, train F loss: -0.4892, acc: 0.9864\n",
      "epoch 464: train D loss: 0.6603, train F loss: -0.4906, acc: 0.9856\n",
      "epoch 465: train D loss: 0.6588, train F loss: -0.4778, acc: 0.9844\n",
      "epoch 466: train D loss: 0.6583, train F loss: -0.4899, acc: 0.9842\n",
      "epoch 467: train D loss: 0.6541, train F loss: -0.4782, acc: 0.9842\n",
      "epoch 468: train D loss: 0.6568, train F loss: -0.4859, acc: 0.9856\n",
      "epoch 469: train D loss: 0.6513, train F loss: -0.4904, acc: 0.9872\n",
      "epoch 470: train D loss: 0.6537, train F loss: -0.4814, acc: 0.9832\n",
      "epoch 471: train D loss: 0.6544, train F loss: -0.4861, acc: 0.9856\n",
      "epoch 472: train D loss: 0.6603, train F loss: -0.4808, acc: 0.9822\n",
      "epoch 473: train D loss: 0.6562, train F loss: -0.4955, acc: 0.9870\n",
      "epoch 474: train D loss: 0.6547, train F loss: -0.4874, acc: 0.9866\n",
      "epoch 475: train D loss: 0.6627, train F loss: -0.4941, acc: 0.9874\n",
      "epoch 476: train D loss: 0.6563, train F loss: -0.4931, acc: 0.9862\n",
      "epoch 477: train D loss: 0.6605, train F loss: -0.4922, acc: 0.9870\n",
      "epoch 478: train D loss: 0.6612, train F loss: -0.4920, acc: 0.9832\n",
      "epoch 479: train D loss: 0.6588, train F loss: -0.4925, acc: 0.9834\n",
      "epoch 480: train D loss: 0.6541, train F loss: -0.4873, acc: 0.9868\n",
      "epoch 481: train D loss: 0.6641, train F loss: -0.4981, acc: 0.9862\n",
      "epoch 482: train D loss: 0.6640, train F loss: -0.5003, acc: 0.9852\n",
      "epoch 483: train D loss: 0.6574, train F loss: -0.4995, acc: 0.9886\n",
      "epoch 484: train D loss: 0.6599, train F loss: -0.4907, acc: 0.9848\n",
      "epoch 485: train D loss: 0.6617, train F loss: -0.4904, acc: 0.9842\n",
      "epoch 486: train D loss: 0.6621, train F loss: -0.4953, acc: 0.9852\n",
      "epoch 487: train D loss: 0.6542, train F loss: -0.4989, acc: 0.9860\n",
      "epoch 488: train D loss: 0.6611, train F loss: -0.4986, acc: 0.9854\n",
      "epoch 489: train D loss: 0.6521, train F loss: -0.4940, acc: 0.9872\n",
      "epoch 490: train D loss: 0.6606, train F loss: -0.5043, acc: 0.9882\n",
      "epoch 491: train D loss: 0.6618, train F loss: -0.5107, acc: 0.9882\n",
      "epoch 492: train D loss: 0.6627, train F loss: -0.5079, acc: 0.9872\n",
      "epoch 493: train D loss: 0.6629, train F loss: -0.5111, acc: 0.9858\n",
      "epoch 494: train D loss: 0.6604, train F loss: -0.5096, acc: 0.9884\n",
      "epoch 495: train D loss: 0.6671, train F loss: -0.5085, acc: 0.9830\n",
      "epoch 496: train D loss: 0.6582, train F loss: -0.4939, acc: 0.9844\n",
      "epoch 497: train D loss: 0.6615, train F loss: -0.5010, acc: 0.9860\n",
      "epoch 498: train D loss: 0.6606, train F loss: -0.5130, acc: 0.9874\n",
      "epoch 499: train D loss: 0.6625, train F loss: -0.5099, acc: 0.9864\n",
      "epoch 500: train D loss: 0.6579, train F loss: -0.5013, acc: 0.9836\n",
      "epoch 501: train D loss: 0.6542, train F loss: -0.5053, acc: 0.9880\n",
      "epoch 502: train D loss: 0.6607, train F loss: -0.5029, acc: 0.9854\n",
      "epoch 503: train D loss: 0.6558, train F loss: -0.5077, acc: 0.9852\n",
      "epoch 504: train D loss: 0.6558, train F loss: -0.5018, acc: 0.9858\n",
      "epoch 505: train D loss: 0.6592, train F loss: -0.5048, acc: 0.9846\n",
      "epoch 506: train D loss: 0.6609, train F loss: -0.5105, acc: 0.9866\n",
      "epoch 507: train D loss: 0.6571, train F loss: -0.5045, acc: 0.9868\n",
      "epoch 508: train D loss: 0.6651, train F loss: -0.5223, acc: 0.9864\n",
      "epoch 509: train D loss: 0.6593, train F loss: -0.5077, acc: 0.9858\n",
      "epoch 510: train D loss: 0.6649, train F loss: -0.5138, acc: 0.9862\n",
      "epoch 511: train D loss: 0.6623, train F loss: -0.5124, acc: 0.9868\n",
      "epoch 512: train D loss: 0.6588, train F loss: -0.5057, acc: 0.9846\n",
      "epoch 513: train D loss: 0.6623, train F loss: -0.5205, acc: 0.9872\n",
      "epoch 514: train D loss: 0.6622, train F loss: -0.5124, acc: 0.9854\n",
      "epoch 515: train D loss: 0.6621, train F loss: -0.5245, acc: 0.9886\n",
      "epoch 516: train D loss: 0.6607, train F loss: -0.5213, acc: 0.9876\n",
      "epoch 517: train D loss: 0.6622, train F loss: -0.5061, acc: 0.9850\n",
      "epoch 518: train D loss: 0.6651, train F loss: -0.5231, acc: 0.9862\n",
      "epoch 519: train D loss: 0.6579, train F loss: -0.5037, acc: 0.9812\n",
      "epoch 520: train D loss: 0.6517, train F loss: -0.5083, acc: 0.9876\n",
      "epoch 521: train D loss: 0.6650, train F loss: -0.5197, acc: 0.9880\n",
      "epoch 522: train D loss: 0.6626, train F loss: -0.5114, acc: 0.9832\n",
      "epoch 523: train D loss: 0.6575, train F loss: -0.5071, acc: 0.9850\n",
      "epoch 524: train D loss: 0.6622, train F loss: -0.5110, acc: 0.9816\n",
      "epoch 525: train D loss: 0.6527, train F loss: -0.5029, acc: 0.9826\n",
      "epoch 526: train D loss: 0.6611, train F loss: -0.5253, acc: 0.9884\n",
      "epoch 527: train D loss: 0.6628, train F loss: -0.5280, acc: 0.9888\n",
      "epoch 528: train D loss: 0.6572, train F loss: -0.5186, acc: 0.9864\n",
      "epoch 529: train D loss: 0.6635, train F loss: -0.5287, acc: 0.9880\n",
      "epoch 530: train D loss: 0.6543, train F loss: -0.5143, acc: 0.9854\n",
      "epoch 531: train D loss: 0.6674, train F loss: -0.5246, acc: 0.9828\n",
      "epoch 532: train D loss: 0.6604, train F loss: -0.5192, acc: 0.9854\n",
      "epoch 533: train D loss: 0.6566, train F loss: -0.5118, acc: 0.9854\n",
      "epoch 534: train D loss: 0.6597, train F loss: -0.5161, acc: 0.9864\n",
      "epoch 535: train D loss: 0.6624, train F loss: -0.5173, acc: 0.9850\n",
      "epoch 536: train D loss: 0.6584, train F loss: -0.5286, acc: 0.9882\n",
      "epoch 537: train D loss: 0.6610, train F loss: -0.5248, acc: 0.9894\n",
      "epoch 538: train D loss: 0.6623, train F loss: -0.5248, acc: 0.9848\n",
      "epoch 539: train D loss: 0.6665, train F loss: -0.5247, acc: 0.9834\n",
      "epoch 540: train D loss: 0.6590, train F loss: -0.5217, acc: 0.9868\n",
      "epoch 541: train D loss: 0.6615, train F loss: -0.5307, acc: 0.9870\n",
      "epoch 542: train D loss: 0.6649, train F loss: -0.5308, acc: 0.9858\n",
      "epoch 543: train D loss: 0.6588, train F loss: -0.5284, acc: 0.9878\n",
      "epoch 544: train D loss: 0.6627, train F loss: -0.5305, acc: 0.9874\n",
      "epoch 545: train D loss: 0.6672, train F loss: -0.5345, acc: 0.9880\n",
      "epoch 546: train D loss: 0.6611, train F loss: -0.5319, acc: 0.9878\n",
      "epoch 547: train D loss: 0.6620, train F loss: -0.5322, acc: 0.9864\n",
      "epoch 548: train D loss: 0.6680, train F loss: -0.5462, acc: 0.9874\n",
      "epoch 549: train D loss: 0.6647, train F loss: -0.5334, acc: 0.9860\n",
      "epoch 550: train D loss: 0.6633, train F loss: -0.5275, acc: 0.9862\n",
      "epoch 551: train D loss: 0.6654, train F loss: -0.5444, acc: 0.9902\n",
      "epoch 552: train D loss: 0.6622, train F loss: -0.5371, acc: 0.9874\n",
      "epoch 553: train D loss: 0.6626, train F loss: -0.5428, acc: 0.9902\n",
      "epoch 554: train D loss: 0.6612, train F loss: -0.5347, acc: 0.9904\n",
      "epoch 555: train D loss: 0.6606, train F loss: -0.5175, acc: 0.9806\n",
      "epoch 556: train D loss: 0.6622, train F loss: -0.5265, acc: 0.9842\n",
      "epoch 557: train D loss: 0.6678, train F loss: -0.5428, acc: 0.9878\n",
      "epoch 558: train D loss: 0.6651, train F loss: -0.5389, acc: 0.9858\n",
      "epoch 559: train D loss: 0.6553, train F loss: -0.5333, acc: 0.9886\n",
      "epoch 560: train D loss: 0.6645, train F loss: -0.5409, acc: 0.9854\n",
      "epoch 561: train D loss: 0.6591, train F loss: -0.5353, acc: 0.9866\n",
      "epoch 562: train D loss: 0.6661, train F loss: -0.5402, acc: 0.9878\n",
      "epoch 563: train D loss: 0.6630, train F loss: -0.5473, acc: 0.9886\n",
      "epoch 564: train D loss: 0.6621, train F loss: -0.5377, acc: 0.9864\n",
      "epoch 565: train D loss: 0.6648, train F loss: -0.5352, acc: 0.9844\n",
      "epoch 566: train D loss: 0.6649, train F loss: -0.5244, acc: 0.9822\n",
      "epoch 567: train D loss: 0.6600, train F loss: -0.5400, acc: 0.9874\n",
      "epoch 568: train D loss: 0.6616, train F loss: -0.5502, acc: 0.9900\n",
      "epoch 569: train D loss: 0.6640, train F loss: -0.5414, acc: 0.9868\n",
      "epoch 570: train D loss: 0.6608, train F loss: -0.5374, acc: 0.9886\n",
      "epoch 571: train D loss: 0.6646, train F loss: -0.5488, acc: 0.9894\n",
      "epoch 572: train D loss: 0.6646, train F loss: -0.5455, acc: 0.9850\n",
      "epoch 573: train D loss: 0.6675, train F loss: -0.5482, acc: 0.9876\n",
      "epoch 574: train D loss: 0.6613, train F loss: -0.5353, acc: 0.9876\n",
      "epoch 575: train D loss: 0.6667, train F loss: -0.5364, acc: 0.9860\n",
      "epoch 576: train D loss: 0.6637, train F loss: -0.5504, acc: 0.9892\n",
      "epoch 577: train D loss: 0.6595, train F loss: -0.5437, acc: 0.9882\n",
      "epoch 578: train D loss: 0.6620, train F loss: -0.5359, acc: 0.9860\n",
      "epoch 579: train D loss: 0.6644, train F loss: -0.5458, acc: 0.9872\n",
      "epoch 580: train D loss: 0.6695, train F loss: -0.5447, acc: 0.9874\n",
      "epoch 581: train D loss: 0.6648, train F loss: -0.5484, acc: 0.9884\n",
      "epoch 582: train D loss: 0.6586, train F loss: -0.5097, acc: 0.9826\n",
      "epoch 583: train D loss: 0.6608, train F loss: -0.5410, acc: 0.9846\n",
      "epoch 584: train D loss: 0.6595, train F loss: -0.5428, acc: 0.9874\n",
      "epoch 585: train D loss: 0.6569, train F loss: -0.5378, acc: 0.9888\n",
      "epoch 586: train D loss: 0.6625, train F loss: -0.5427, acc: 0.9868\n",
      "epoch 587: train D loss: 0.6626, train F loss: -0.5536, acc: 0.9886\n",
      "epoch 588: train D loss: 0.6617, train F loss: -0.5511, acc: 0.9868\n",
      "epoch 589: train D loss: 0.6583, train F loss: -0.5414, acc: 0.9878\n",
      "epoch 590: train D loss: 0.6658, train F loss: -0.5438, acc: 0.9838\n",
      "epoch 591: train D loss: 0.6594, train F loss: -0.5529, acc: 0.9892\n",
      "epoch 592: train D loss: 0.6578, train F loss: -0.5463, acc: 0.9866\n",
      "epoch 593: train D loss: 0.6623, train F loss: -0.5402, acc: 0.9856\n",
      "epoch 594: train D loss: 0.6574, train F loss: -0.5394, acc: 0.9868\n",
      "epoch 595: train D loss: 0.6582, train F loss: -0.5421, acc: 0.9878\n",
      "epoch 596: train D loss: 0.6656, train F loss: -0.5538, acc: 0.9882\n",
      "epoch 597: train D loss: 0.6659, train F loss: -0.5616, acc: 0.9900\n",
      "epoch 598: train D loss: 0.6620, train F loss: -0.5290, acc: 0.9838\n",
      "epoch 599: train D loss: 0.6635, train F loss: -0.5542, acc: 0.9886\n",
      "epoch 600: train D loss: 0.6652, train F loss: -0.5554, acc: 0.9882\n",
      "epoch 601: train D loss: 0.6653, train F loss: -0.5588, acc: 0.9872\n",
      "epoch 602: train D loss: 0.6601, train F loss: -0.5321, acc: 0.9848\n",
      "epoch 603: train D loss: 0.6626, train F loss: -0.5544, acc: 0.9874\n",
      "epoch 604: train D loss: 0.6621, train F loss: -0.5554, acc: 0.9884\n",
      "epoch 605: train D loss: 0.6618, train F loss: -0.5440, acc: 0.9858\n",
      "epoch 606: train D loss: 0.6640, train F loss: -0.5465, acc: 0.9848\n",
      "epoch 607: train D loss: 0.6643, train F loss: -0.5614, acc: 0.9882\n",
      "epoch 608: train D loss: 0.6651, train F loss: -0.5562, acc: 0.9880\n",
      "epoch 609: train D loss: 0.6621, train F loss: -0.5504, acc: 0.9880\n",
      "epoch 610: train D loss: 0.6670, train F loss: -0.5565, acc: 0.9868\n",
      "epoch 611: train D loss: 0.6654, train F loss: -0.5695, acc: 0.9896\n",
      "epoch 612: train D loss: 0.6679, train F loss: -0.5621, acc: 0.9880\n",
      "epoch 613: train D loss: 0.6660, train F loss: -0.5466, acc: 0.9846\n",
      "epoch 614: train D loss: 0.6671, train F loss: -0.5601, acc: 0.9888\n",
      "epoch 615: train D loss: 0.6638, train F loss: -0.5488, acc: 0.9860\n",
      "epoch 616: train D loss: 0.6547, train F loss: -0.5567, acc: 0.9910\n",
      "epoch 617: train D loss: 0.6653, train F loss: -0.5442, acc: 0.9824\n",
      "epoch 618: train D loss: 0.6664, train F loss: -0.5596, acc: 0.9880\n",
      "epoch 619: train D loss: 0.6627, train F loss: -0.5530, acc: 0.9856\n",
      "epoch 620: train D loss: 0.6690, train F loss: -0.5534, acc: 0.9856\n",
      "epoch 621: train D loss: 0.6599, train F loss: -0.5501, acc: 0.9864\n",
      "epoch 622: train D loss: 0.6638, train F loss: -0.5620, acc: 0.9888\n",
      "epoch 623: train D loss: 0.6656, train F loss: -0.5629, acc: 0.9876\n",
      "epoch 624: train D loss: 0.6656, train F loss: -0.5658, acc: 0.9902\n",
      "epoch 625: train D loss: 0.6648, train F loss: -0.5449, acc: 0.9844\n",
      "epoch 626: train D loss: 0.6654, train F loss: -0.5705, acc: 0.9896\n",
      "epoch 627: train D loss: 0.6658, train F loss: -0.5614, acc: 0.9876\n",
      "epoch 628: train D loss: 0.6602, train F loss: -0.5585, acc: 0.9882\n",
      "epoch 629: train D loss: 0.6686, train F loss: -0.5629, acc: 0.9872\n",
      "epoch 630: train D loss: 0.6641, train F loss: -0.5552, acc: 0.9864\n",
      "epoch 631: train D loss: 0.6660, train F loss: -0.5625, acc: 0.9862\n",
      "epoch 632: train D loss: 0.6663, train F loss: -0.5732, acc: 0.9888\n",
      "epoch 633: train D loss: 0.6656, train F loss: -0.5604, acc: 0.9850\n",
      "epoch 634: train D loss: 0.6641, train F loss: -0.5589, acc: 0.9870\n",
      "epoch 635: train D loss: 0.6697, train F loss: -0.5715, acc: 0.9876\n",
      "epoch 636: train D loss: 0.6690, train F loss: -0.5712, acc: 0.9876\n",
      "epoch 637: train D loss: 0.6638, train F loss: -0.5542, acc: 0.9850\n",
      "epoch 638: train D loss: 0.6638, train F loss: -0.5632, acc: 0.9882\n",
      "epoch 639: train D loss: 0.6653, train F loss: -0.5671, acc: 0.9876\n",
      "epoch 640: train D loss: 0.6619, train F loss: -0.5620, acc: 0.9878\n",
      "epoch 641: train D loss: 0.6616, train F loss: -0.5695, acc: 0.9904\n",
      "epoch 642: train D loss: 0.6624, train F loss: -0.5681, acc: 0.9888\n",
      "epoch 643: train D loss: 0.6650, train F loss: -0.5682, acc: 0.9882\n",
      "epoch 644: train D loss: 0.6663, train F loss: -0.5751, acc: 0.9890\n",
      "epoch 645: train D loss: 0.6647, train F loss: -0.5563, acc: 0.9854\n",
      "epoch 646: train D loss: 0.6670, train F loss: -0.5675, acc: 0.9862\n",
      "epoch 647: train D loss: 0.6648, train F loss: -0.5714, acc: 0.9896\n",
      "epoch 648: train D loss: 0.6653, train F loss: -0.5624, acc: 0.9860\n",
      "epoch 649: train D loss: 0.6666, train F loss: -0.5670, acc: 0.9884\n",
      "epoch 650: train D loss: 0.6647, train F loss: -0.5669, acc: 0.9892\n",
      "epoch 651: train D loss: 0.6625, train F loss: -0.5512, acc: 0.9858\n",
      "epoch 652: train D loss: 0.6672, train F loss: -0.5741, acc: 0.9884\n",
      "epoch 653: train D loss: 0.6592, train F loss: -0.5524, acc: 0.9850\n",
      "epoch 654: train D loss: 0.6566, train F loss: -0.5523, acc: 0.9866\n",
      "epoch 655: train D loss: 0.6625, train F loss: -0.5798, acc: 0.9922\n",
      "epoch 656: train D loss: 0.6638, train F loss: -0.5677, acc: 0.9880\n",
      "epoch 657: train D loss: 0.6637, train F loss: -0.5680, acc: 0.9860\n",
      "epoch 658: train D loss: 0.6693, train F loss: -0.5719, acc: 0.9868\n",
      "epoch 659: train D loss: 0.6663, train F loss: -0.5716, acc: 0.9874\n",
      "epoch 660: train D loss: 0.6680, train F loss: -0.5783, acc: 0.9888\n",
      "epoch 661: train D loss: 0.6662, train F loss: -0.5731, acc: 0.9890\n",
      "epoch 662: train D loss: 0.6660, train F loss: -0.5760, acc: 0.9886\n",
      "epoch 663: train D loss: 0.6692, train F loss: -0.5739, acc: 0.9882\n",
      "epoch 664: train D loss: 0.6618, train F loss: -0.5756, acc: 0.9910\n",
      "epoch 665: train D loss: 0.6631, train F loss: -0.5689, acc: 0.9886\n",
      "epoch 666: train D loss: 0.6678, train F loss: -0.5796, acc: 0.9902\n",
      "epoch 667: train D loss: 0.6656, train F loss: -0.5673, acc: 0.9858\n",
      "epoch 668: train D loss: 0.6725, train F loss: -0.5761, acc: 0.9858\n",
      "epoch 669: train D loss: 0.6648, train F loss: -0.5533, acc: 0.9854\n",
      "epoch 670: train D loss: 0.6612, train F loss: -0.5701, acc: 0.9884\n",
      "epoch 671: train D loss: 0.6680, train F loss: -0.5782, acc: 0.9882\n",
      "epoch 672: train D loss: 0.6640, train F loss: -0.5466, acc: 0.9844\n",
      "epoch 673: train D loss: 0.6615, train F loss: -0.5674, acc: 0.9870\n",
      "epoch 674: train D loss: 0.6654, train F loss: -0.5781, acc: 0.9876\n",
      "epoch 675: train D loss: 0.6604, train F loss: -0.5760, acc: 0.9918\n",
      "epoch 676: train D loss: 0.6647, train F loss: -0.5693, acc: 0.9874\n",
      "epoch 677: train D loss: 0.6677, train F loss: -0.5773, acc: 0.9886\n",
      "epoch 678: train D loss: 0.6667, train F loss: -0.5678, acc: 0.9862\n",
      "epoch 679: train D loss: 0.6643, train F loss: -0.5811, acc: 0.9896\n",
      "epoch 680: train D loss: 0.6631, train F loss: -0.5684, acc: 0.9868\n",
      "epoch 681: train D loss: 0.6668, train F loss: -0.5787, acc: 0.9888\n",
      "epoch 682: train D loss: 0.6715, train F loss: -0.5808, acc: 0.9858\n",
      "epoch 683: train D loss: 0.6635, train F loss: -0.5790, acc: 0.9892\n",
      "epoch 684: train D loss: 0.6653, train F loss: -0.5743, acc: 0.9876\n",
      "epoch 685: train D loss: 0.6729, train F loss: -0.5663, acc: 0.9896\n",
      "epoch 686: train D loss: 0.6673, train F loss: -0.5739, acc: 0.9866\n",
      "epoch 687: train D loss: 0.6634, train F loss: -0.5723, acc: 0.9878\n",
      "epoch 688: train D loss: 0.6637, train F loss: -0.5733, acc: 0.9888\n",
      "epoch 689: train D loss: 0.6657, train F loss: -0.5690, acc: 0.9882\n",
      "epoch 690: train D loss: 0.6651, train F loss: -0.5740, acc: 0.9878\n",
      "epoch 691: train D loss: 0.6661, train F loss: -0.5831, acc: 0.9902\n",
      "epoch 692: train D loss: 0.6659, train F loss: -0.5791, acc: 0.9874\n",
      "epoch 693: train D loss: 0.6715, train F loss: -0.5843, acc: 0.9880\n",
      "epoch 694: train D loss: 0.6662, train F loss: -0.5753, acc: 0.9872\n",
      "epoch 695: train D loss: 0.6656, train F loss: -0.5847, acc: 0.9890\n",
      "epoch 696: train D loss: 0.6693, train F loss: -0.5741, acc: 0.9878\n",
      "epoch 697: train D loss: 0.6683, train F loss: -0.5770, acc: 0.9878\n",
      "epoch 698: train D loss: 0.6663, train F loss: -0.5741, acc: 0.9872\n",
      "epoch 699: train D loss: 0.6678, train F loss: -0.5845, acc: 0.9882\n",
      "epoch 700: train D loss: 0.6700, train F loss: -0.5628, acc: 0.9860\n",
      "epoch 701: train D loss: 0.6615, train F loss: -0.5671, acc: 0.9872\n",
      "epoch 702: train D loss: 0.6619, train F loss: -0.5664, acc: 0.9862\n",
      "epoch 703: train D loss: 0.6654, train F loss: -0.5793, acc: 0.9866\n",
      "epoch 704: train D loss: 0.6678, train F loss: -0.5740, acc: 0.9872\n",
      "epoch 705: train D loss: 0.6649, train F loss: -0.5835, acc: 0.9882\n",
      "epoch 706: train D loss: 0.6723, train F loss: -0.5939, acc: 0.9882\n",
      "epoch 707: train D loss: 0.6650, train F loss: -0.5879, acc: 0.9908\n",
      "epoch 708: train D loss: 0.6646, train F loss: -0.5789, acc: 0.9882\n",
      "epoch 709: train D loss: 0.6673, train F loss: -0.5908, acc: 0.9882\n",
      "epoch 710: train D loss: 0.6690, train F loss: -0.5883, acc: 0.9910\n",
      "epoch 711: train D loss: 0.6673, train F loss: -0.5538, acc: 0.9814\n",
      "epoch 712: train D loss: 0.6649, train F loss: -0.5756, acc: 0.9852\n",
      "epoch 713: train D loss: 0.6652, train F loss: -0.5815, acc: 0.9876\n",
      "epoch 714: train D loss: 0.6662, train F loss: -0.5888, acc: 0.9890\n",
      "epoch 715: train D loss: 0.6636, train F loss: -0.5890, acc: 0.9908\n",
      "epoch 716: train D loss: 0.6682, train F loss: -0.5826, acc: 0.9872\n",
      "epoch 717: train D loss: 0.6651, train F loss: -0.5898, acc: 0.9902\n",
      "epoch 718: train D loss: 0.6672, train F loss: -0.5859, acc: 0.9874\n",
      "epoch 719: train D loss: 0.6666, train F loss: -0.5870, acc: 0.9890\n",
      "epoch 720: train D loss: 0.6724, train F loss: -0.5908, acc: 0.9866\n",
      "epoch 721: train D loss: 0.6672, train F loss: -0.5905, acc: 0.9896\n",
      "epoch 722: train D loss: 0.6705, train F loss: -0.5870, acc: 0.9852\n",
      "epoch 723: train D loss: 0.6698, train F loss: -0.5762, acc: 0.9848\n",
      "epoch 724: train D loss: 0.6643, train F loss: -0.5811, acc: 0.9864\n",
      "epoch 725: train D loss: 0.6662, train F loss: -0.5935, acc: 0.9904\n",
      "epoch 726: train D loss: 0.6705, train F loss: -0.5867, acc: 0.9886\n",
      "epoch 727: train D loss: 0.6650, train F loss: -0.5924, acc: 0.9914\n",
      "epoch 728: train D loss: 0.6691, train F loss: -0.5761, acc: 0.9852\n",
      "epoch 729: train D loss: 0.6666, train F loss: -0.5946, acc: 0.9896\n",
      "epoch 730: train D loss: 0.6655, train F loss: -0.5903, acc: 0.9886\n",
      "epoch 731: train D loss: 0.6660, train F loss: -0.5859, acc: 0.9892\n",
      "epoch 732: train D loss: 0.6666, train F loss: -0.5908, acc: 0.9884\n",
      "epoch 733: train D loss: 0.6668, train F loss: -0.5772, acc: 0.9870\n",
      "epoch 734: train D loss: 0.6646, train F loss: -0.5845, acc: 0.9880\n",
      "epoch 735: train D loss: 0.6676, train F loss: -0.5862, acc: 0.9878\n",
      "epoch 736: train D loss: 0.6642, train F loss: -0.5856, acc: 0.9882\n",
      "epoch 737: train D loss: 0.6661, train F loss: -0.5869, acc: 0.9886\n",
      "epoch 738: train D loss: 0.6640, train F loss: -0.5909, acc: 0.9884\n",
      "epoch 739: train D loss: 0.6715, train F loss: -0.5949, acc: 0.9890\n",
      "epoch 740: train D loss: 0.6646, train F loss: -0.5829, acc: 0.9870\n",
      "epoch 741: train D loss: 0.6707, train F loss: -0.5850, acc: 0.9868\n",
      "epoch 742: train D loss: 0.6708, train F loss: -0.5895, acc: 0.9884\n",
      "epoch 743: train D loss: 0.6642, train F loss: -0.5911, acc: 0.9900\n",
      "epoch 744: train D loss: 0.6719, train F loss: -0.5958, acc: 0.9882\n",
      "epoch 745: train D loss: 0.6687, train F loss: -0.5901, acc: 0.9876\n",
      "epoch 746: train D loss: 0.6689, train F loss: -0.5892, acc: 0.9878\n",
      "epoch 747: train D loss: 0.6664, train F loss: -0.5945, acc: 0.9910\n",
      "epoch 748: train D loss: 0.6730, train F loss: -0.6010, acc: 0.9906\n",
      "epoch 749: train D loss: 0.6708, train F loss: -0.5880, acc: 0.9872\n",
      "epoch 750: train D loss: 0.6732, train F loss: -0.6023, acc: 0.9920\n",
      "epoch 751: train D loss: 0.6701, train F loss: -0.5890, acc: 0.9874\n",
      "epoch 752: train D loss: 0.6690, train F loss: -0.6005, acc: 0.9896\n",
      "epoch 753: train D loss: 0.6644, train F loss: -0.5862, acc: 0.9852\n",
      "epoch 754: train D loss: 0.6624, train F loss: -0.5944, acc: 0.9910\n",
      "epoch 755: train D loss: 0.6686, train F loss: -0.5883, acc: 0.9886\n",
      "epoch 756: train D loss: 0.6691, train F loss: -0.5953, acc: 0.9906\n",
      "epoch 757: train D loss: 0.6654, train F loss: -0.5866, acc: 0.9868\n",
      "epoch 758: train D loss: 0.6742, train F loss: -0.6091, acc: 0.9898\n",
      "epoch 759: train D loss: 0.6725, train F loss: -0.5925, acc: 0.9866\n",
      "epoch 760: train D loss: 0.6667, train F loss: -0.5839, acc: 0.9862\n",
      "epoch 761: train D loss: 0.6704, train F loss: -0.5907, acc: 0.9824\n",
      "epoch 762: train D loss: 0.6675, train F loss: -0.6003, acc: 0.9884\n",
      "epoch 763: train D loss: 0.6688, train F loss: -0.5951, acc: 0.9888\n",
      "epoch 764: train D loss: 0.6686, train F loss: -0.5949, acc: 0.9882\n",
      "epoch 765: train D loss: 0.6663, train F loss: -0.5992, acc: 0.9898\n",
      "epoch 766: train D loss: 0.6616, train F loss: -0.5902, acc: 0.9898\n",
      "epoch 767: train D loss: 0.6647, train F loss: -0.5821, acc: 0.9862\n",
      "epoch 768: train D loss: 0.6705, train F loss: -0.5966, acc: 0.9872\n",
      "epoch 769: train D loss: 0.6639, train F loss: -0.6028, acc: 0.9920\n",
      "epoch 770: train D loss: 0.6665, train F loss: -0.5949, acc: 0.9888\n",
      "epoch 771: train D loss: 0.6716, train F loss: -0.6050, acc: 0.9904\n",
      "epoch 772: train D loss: 0.6690, train F loss: -0.5939, acc: 0.9886\n",
      "epoch 773: train D loss: 0.6716, train F loss: -0.6089, acc: 0.9900\n",
      "epoch 774: train D loss: 0.6735, train F loss: -0.5961, acc: 0.9880\n",
      "epoch 775: train D loss: 0.6663, train F loss: -0.5889, acc: 0.9862\n",
      "epoch 776: train D loss: 0.6702, train F loss: -0.5909, acc: 0.9898\n",
      "epoch 777: train D loss: 0.6673, train F loss: -0.5820, acc: 0.9870\n",
      "epoch 778: train D loss: 0.6672, train F loss: -0.5906, acc: 0.9866\n",
      "epoch 779: train D loss: 0.6714, train F loss: -0.6099, acc: 0.9904\n",
      "epoch 780: train D loss: 0.6683, train F loss: -0.5964, acc: 0.9888\n",
      "epoch 781: train D loss: 0.6680, train F loss: -0.5981, acc: 0.9886\n",
      "epoch 782: train D loss: 0.6657, train F loss: -0.5992, acc: 0.9898\n",
      "epoch 783: train D loss: 0.6671, train F loss: -0.5933, acc: 0.9892\n",
      "epoch 784: train D loss: 0.6687, train F loss: -0.6062, acc: 0.9908\n",
      "epoch 785: train D loss: 0.6728, train F loss: -0.6090, acc: 0.9912\n",
      "epoch 786: train D loss: 0.6727, train F loss: -0.6026, acc: 0.9890\n",
      "epoch 787: train D loss: 0.6654, train F loss: -0.5979, acc: 0.9896\n",
      "epoch 788: train D loss: 0.6666, train F loss: -0.5847, acc: 0.9866\n",
      "epoch 789: train D loss: 0.6658, train F loss: -0.5875, acc: 0.9860\n",
      "epoch 790: train D loss: 0.6727, train F loss: -0.6067, acc: 0.9888\n",
      "epoch 791: train D loss: 0.6631, train F loss: -0.5997, acc: 0.9898\n",
      "epoch 792: train D loss: 0.6666, train F loss: -0.6007, acc: 0.9902\n",
      "epoch 793: train D loss: 0.6660, train F loss: -0.6027, acc: 0.9912\n",
      "epoch 794: train D loss: 0.6699, train F loss: -0.5997, acc: 0.9878\n",
      "epoch 795: train D loss: 0.6693, train F loss: -0.6026, acc: 0.9900\n",
      "epoch 796: train D loss: 0.6685, train F loss: -0.6111, acc: 0.9916\n",
      "epoch 797: train D loss: 0.6699, train F loss: -0.6059, acc: 0.9898\n",
      "epoch 798: train D loss: 0.6694, train F loss: -0.5970, acc: 0.9872\n",
      "epoch 799: train D loss: 0.6666, train F loss: -0.6005, acc: 0.9884\n",
      "epoch 800: train D loss: 0.6772, train F loss: -0.6165, acc: 0.9916\n",
      "epoch 801: train D loss: 0.6742, train F loss: -0.6085, acc: 0.9884\n",
      "epoch 802: train D loss: 0.6725, train F loss: -0.6010, acc: 0.9872\n",
      "epoch 803: train D loss: 0.6665, train F loss: -0.6012, acc: 0.9894\n",
      "epoch 804: train D loss: 0.6737, train F loss: -0.6036, acc: 0.9868\n",
      "epoch 805: train D loss: 0.6613, train F loss: -0.6030, acc: 0.9912\n",
      "epoch 806: train D loss: 0.6670, train F loss: -0.6060, acc: 0.9900\n",
      "epoch 807: train D loss: 0.6722, train F loss: -0.5994, acc: 0.9878\n",
      "epoch 808: train D loss: 0.6626, train F loss: -0.6009, acc: 0.9890\n",
      "epoch 809: train D loss: 0.6694, train F loss: -0.5984, acc: 0.9870\n",
      "epoch 810: train D loss: 0.6689, train F loss: -0.6024, acc: 0.9886\n",
      "epoch 811: train D loss: 0.6714, train F loss: -0.5980, acc: 0.9874\n",
      "epoch 812: train D loss: 0.6735, train F loss: -0.6085, acc: 0.9878\n",
      "epoch 813: train D loss: 0.6710, train F loss: -0.6117, acc: 0.9888\n",
      "epoch 814: train D loss: 0.6709, train F loss: -0.5957, acc: 0.9880\n",
      "epoch 815: train D loss: 0.6702, train F loss: -0.6061, acc: 0.9884\n",
      "epoch 816: train D loss: 0.6681, train F loss: -0.6036, acc: 0.9884\n",
      "epoch 817: train D loss: 0.6660, train F loss: -0.6058, acc: 0.9916\n",
      "epoch 818: train D loss: 0.6698, train F loss: -0.6054, acc: 0.9894\n",
      "epoch 819: train D loss: 0.6681, train F loss: -0.6062, acc: 0.9896\n",
      "epoch 820: train D loss: 0.6721, train F loss: -0.6037, acc: 0.9880\n",
      "epoch 821: train D loss: 0.6692, train F loss: -0.5889, acc: 0.9858\n",
      "epoch 822: train D loss: 0.6720, train F loss: -0.6101, acc: 0.9872\n",
      "epoch 823: train D loss: 0.6717, train F loss: -0.6127, acc: 0.9912\n",
      "epoch 824: train D loss: 0.6642, train F loss: -0.5919, acc: 0.9880\n",
      "epoch 825: train D loss: 0.6737, train F loss: -0.6104, acc: 0.9884\n",
      "epoch 826: train D loss: 0.6653, train F loss: -0.6055, acc: 0.9900\n",
      "epoch 827: train D loss: 0.6681, train F loss: -0.6066, acc: 0.9892\n",
      "epoch 828: train D loss: 0.6688, train F loss: -0.6057, acc: 0.9896\n",
      "epoch 829: train D loss: 0.6701, train F loss: -0.6147, acc: 0.9906\n",
      "epoch 830: train D loss: 0.6666, train F loss: -0.6088, acc: 0.9906\n",
      "epoch 831: train D loss: 0.6717, train F loss: -0.6085, acc: 0.9910\n",
      "epoch 832: train D loss: 0.6745, train F loss: -0.6174, acc: 0.9916\n",
      "epoch 833: train D loss: 0.6716, train F loss: -0.6086, acc: 0.9884\n",
      "epoch 834: train D loss: 0.6731, train F loss: -0.6044, acc: 0.9892\n",
      "epoch 835: train D loss: 0.6681, train F loss: -0.6029, acc: 0.9850\n",
      "epoch 836: train D loss: 0.6672, train F loss: -0.6061, acc: 0.9900\n",
      "epoch 837: train D loss: 0.6722, train F loss: -0.6186, acc: 0.9908\n",
      "epoch 838: train D loss: 0.6727, train F loss: -0.6096, acc: 0.9878\n",
      "epoch 839: train D loss: 0.6744, train F loss: -0.6115, acc: 0.9888\n",
      "epoch 840: train D loss: 0.6708, train F loss: -0.6169, acc: 0.9920\n",
      "epoch 841: train D loss: 0.6707, train F loss: -0.6054, acc: 0.9890\n",
      "epoch 842: train D loss: 0.6699, train F loss: -0.6078, acc: 0.9880\n",
      "epoch 843: train D loss: 0.6704, train F loss: -0.6087, acc: 0.9904\n",
      "epoch 844: train D loss: 0.6712, train F loss: -0.6130, acc: 0.9892\n",
      "epoch 845: train D loss: 0.6712, train F loss: -0.6081, acc: 0.9894\n",
      "epoch 846: train D loss: 0.6688, train F loss: -0.6129, acc: 0.9910\n",
      "epoch 847: train D loss: 0.6681, train F loss: -0.6116, acc: 0.9914\n",
      "epoch 848: train D loss: 0.6751, train F loss: -0.6115, acc: 0.9874\n",
      "epoch 849: train D loss: 0.6732, train F loss: -0.6136, acc: 0.9898\n",
      "epoch 850: train D loss: 0.6702, train F loss: -0.6055, acc: 0.9890\n",
      "epoch 851: train D loss: 0.6737, train F loss: -0.6194, acc: 0.9894\n",
      "epoch 852: train D loss: 0.6731, train F loss: -0.6130, acc: 0.9894\n",
      "epoch 853: train D loss: 0.6657, train F loss: -0.6067, acc: 0.9886\n",
      "epoch 854: train D loss: 0.6713, train F loss: -0.6162, acc: 0.9908\n",
      "epoch 855: train D loss: 0.6746, train F loss: -0.6133, acc: 0.9886\n",
      "epoch 856: train D loss: 0.6705, train F loss: -0.6230, acc: 0.9930\n",
      "epoch 857: train D loss: 0.6731, train F loss: -0.6214, acc: 0.9920\n",
      "epoch 858: train D loss: 0.6717, train F loss: -0.6084, acc: 0.9902\n",
      "epoch 859: train D loss: 0.6786, train F loss: -0.6191, acc: 0.9880\n",
      "epoch 860: train D loss: 0.6728, train F loss: -0.6082, acc: 0.9878\n",
      "epoch 861: train D loss: 0.6652, train F loss: -0.6017, acc: 0.9890\n",
      "epoch 862: train D loss: 0.6660, train F loss: -0.6102, acc: 0.9894\n",
      "epoch 863: train D loss: 0.6681, train F loss: -0.6112, acc: 0.9904\n",
      "epoch 864: train D loss: 0.6703, train F loss: -0.6176, acc: 0.9924\n",
      "epoch 865: train D loss: 0.6730, train F loss: -0.6084, acc: 0.9890\n",
      "epoch 866: train D loss: 0.6713, train F loss: -0.6182, acc: 0.9908\n",
      "epoch 867: train D loss: 0.6697, train F loss: -0.6070, acc: 0.9898\n",
      "epoch 868: train D loss: 0.6706, train F loss: -0.6165, acc: 0.9894\n",
      "epoch 869: train D loss: 0.6653, train F loss: -0.5960, acc: 0.9858\n",
      "epoch 870: train D loss: 0.6719, train F loss: -0.5859, acc: 0.9860\n",
      "epoch 871: train D loss: 0.6680, train F loss: -0.6027, acc: 0.9862\n",
      "epoch 872: train D loss: 0.6685, train F loss: -0.6097, acc: 0.9884\n",
      "epoch 873: train D loss: 0.6660, train F loss: -0.5949, acc: 0.9860\n",
      "epoch 874: train D loss: 0.6702, train F loss: -0.6074, acc: 0.9872\n",
      "epoch 875: train D loss: 0.6664, train F loss: -0.6152, acc: 0.9918\n",
      "epoch 876: train D loss: 0.6704, train F loss: -0.6130, acc: 0.9898\n",
      "epoch 877: train D loss: 0.6699, train F loss: -0.6217, acc: 0.9912\n",
      "epoch 878: train D loss: 0.6664, train F loss: -0.5848, acc: 0.9880\n",
      "epoch 879: train D loss: 0.6720, train F loss: -0.6036, acc: 0.9866\n",
      "epoch 880: train D loss: 0.6654, train F loss: -0.6121, acc: 0.9912\n",
      "epoch 881: train D loss: 0.6695, train F loss: -0.6093, acc: 0.9894\n",
      "epoch 882: train D loss: 0.6707, train F loss: -0.6016, acc: 0.9894\n",
      "epoch 883: train D loss: 0.6718, train F loss: -0.6131, acc: 0.9882\n",
      "epoch 884: train D loss: 0.6702, train F loss: -0.6197, acc: 0.9926\n",
      "epoch 885: train D loss: 0.6725, train F loss: -0.6186, acc: 0.9902\n",
      "epoch 886: train D loss: 0.6763, train F loss: -0.6211, acc: 0.9906\n",
      "epoch 887: train D loss: 0.6692, train F loss: -0.6091, acc: 0.9866\n",
      "epoch 888: train D loss: 0.6723, train F loss: -0.6149, acc: 0.9886\n",
      "epoch 889: train D loss: 0.6702, train F loss: -0.6190, acc: 0.9908\n",
      "epoch 890: train D loss: 0.6722, train F loss: -0.6258, acc: 0.9926\n",
      "epoch 891: train D loss: 0.6747, train F loss: -0.6261, acc: 0.9904\n",
      "epoch 892: train D loss: 0.6753, train F loss: -0.6229, acc: 0.9902\n",
      "epoch 893: train D loss: 0.6726, train F loss: -0.6193, acc: 0.9890\n",
      "epoch 894: train D loss: 0.6753, train F loss: -0.6120, acc: 0.9862\n",
      "epoch 895: train D loss: 0.6725, train F loss: -0.6207, acc: 0.9900\n",
      "epoch 896: train D loss: 0.6714, train F loss: -0.6133, acc: 0.9890\n",
      "epoch 897: train D loss: 0.6689, train F loss: -0.6167, acc: 0.9908\n",
      "epoch 898: train D loss: 0.6725, train F loss: -0.6028, acc: 0.9882\n",
      "epoch 899: train D loss: 0.6728, train F loss: -0.6066, acc: 0.9870\n",
      "epoch 900: train D loss: 0.6700, train F loss: -0.6193, acc: 0.9902\n",
      "epoch 901: train D loss: 0.6697, train F loss: -0.6235, acc: 0.9928\n",
      "epoch 902: train D loss: 0.6709, train F loss: -0.6165, acc: 0.9908\n",
      "epoch 903: train D loss: 0.6718, train F loss: -0.6080, acc: 0.9878\n",
      "epoch 904: train D loss: 0.6639, train F loss: -0.6164, acc: 0.9916\n",
      "epoch 905: train D loss: 0.6691, train F loss: -0.6170, acc: 0.9904\n",
      "epoch 906: train D loss: 0.6736, train F loss: -0.6120, acc: 0.9880\n",
      "epoch 907: train D loss: 0.6648, train F loss: -0.6117, acc: 0.9894\n",
      "epoch 908: train D loss: 0.6776, train F loss: -0.6254, acc: 0.9902\n",
      "epoch 909: train D loss: 0.6684, train F loss: -0.6077, acc: 0.9876\n",
      "epoch 910: train D loss: 0.6716, train F loss: -0.6214, acc: 0.9904\n",
      "epoch 911: train D loss: 0.6733, train F loss: -0.6199, acc: 0.9888\n",
      "epoch 912: train D loss: 0.6703, train F loss: -0.6175, acc: 0.9896\n",
      "epoch 913: train D loss: 0.6727, train F loss: -0.6238, acc: 0.9906\n",
      "epoch 914: train D loss: 0.6734, train F loss: -0.6181, acc: 0.9900\n",
      "epoch 915: train D loss: 0.6742, train F loss: -0.6155, acc: 0.9890\n",
      "epoch 916: train D loss: 0.6761, train F loss: -0.6175, acc: 0.9880\n",
      "epoch 917: train D loss: 0.6702, train F loss: -0.6170, acc: 0.9892\n",
      "epoch 918: train D loss: 0.6681, train F loss: -0.6173, acc: 0.9898\n",
      "epoch 919: train D loss: 0.6694, train F loss: -0.6152, acc: 0.9898\n",
      "epoch 920: train D loss: 0.6689, train F loss: -0.6139, acc: 0.9884\n",
      "epoch 921: train D loss: 0.6745, train F loss: -0.6130, acc: 0.9898\n",
      "epoch 922: train D loss: 0.6750, train F loss: -0.6270, acc: 0.9914\n",
      "epoch 923: train D loss: 0.6692, train F loss: -0.6122, acc: 0.9870\n",
      "epoch 924: train D loss: 0.6708, train F loss: -0.6185, acc: 0.9896\n",
      "epoch 925: train D loss: 0.6750, train F loss: -0.6243, acc: 0.9910\n",
      "epoch 926: train D loss: 0.6747, train F loss: -0.6301, acc: 0.9920\n",
      "epoch 927: train D loss: 0.6740, train F loss: -0.6208, acc: 0.9892\n",
      "epoch 928: train D loss: 0.6752, train F loss: -0.6214, acc: 0.9890\n",
      "epoch 929: train D loss: 0.6706, train F loss: -0.6191, acc: 0.9906\n",
      "epoch 930: train D loss: 0.6684, train F loss: -0.6062, acc: 0.9866\n",
      "epoch 931: train D loss: 0.6714, train F loss: -0.6168, acc: 0.9916\n",
      "epoch 932: train D loss: 0.6633, train F loss: -0.6074, acc: 0.9890\n",
      "epoch 933: train D loss: 0.6701, train F loss: -0.6214, acc: 0.9896\n",
      "epoch 934: train D loss: 0.6704, train F loss: -0.6127, acc: 0.9898\n",
      "epoch 935: train D loss: 0.6688, train F loss: -0.6133, acc: 0.9892\n",
      "epoch 936: train D loss: 0.6709, train F loss: -0.6182, acc: 0.9890\n",
      "epoch 937: train D loss: 0.6696, train F loss: -0.6189, acc: 0.9896\n",
      "epoch 938: train D loss: 0.6728, train F loss: -0.6193, acc: 0.9894\n",
      "epoch 939: train D loss: 0.6692, train F loss: -0.6170, acc: 0.9902\n",
      "epoch 940: train D loss: 0.6763, train F loss: -0.6108, acc: 0.9880\n",
      "epoch 941: train D loss: 0.6696, train F loss: -0.5959, acc: 0.9864\n",
      "epoch 942: train D loss: 0.6724, train F loss: -0.6165, acc: 0.9880\n",
      "epoch 943: train D loss: 0.6687, train F loss: -0.6170, acc: 0.9908\n",
      "epoch 944: train D loss: 0.6651, train F loss: -0.6081, acc: 0.9896\n",
      "epoch 945: train D loss: 0.6694, train F loss: -0.6204, acc: 0.9912\n",
      "epoch 946: train D loss: 0.6677, train F loss: -0.6229, acc: 0.9914\n",
      "epoch 947: train D loss: 0.6737, train F loss: -0.6293, acc: 0.9906\n",
      "epoch 948: train D loss: 0.6703, train F loss: -0.6182, acc: 0.9906\n",
      "epoch 949: train D loss: 0.6725, train F loss: -0.6218, acc: 0.9896\n",
      "epoch 950: train D loss: 0.6727, train F loss: -0.6145, acc: 0.9858\n",
      "epoch 951: train D loss: 0.6694, train F loss: -0.6204, acc: 0.9892\n",
      "epoch 952: train D loss: 0.6716, train F loss: -0.6194, acc: 0.9888\n",
      "epoch 953: train D loss: 0.6723, train F loss: -0.6302, acc: 0.9914\n",
      "epoch 954: train D loss: 0.6664, train F loss: -0.6168, acc: 0.9916\n",
      "epoch 955: train D loss: 0.6692, train F loss: -0.6144, acc: 0.9890\n",
      "epoch 956: train D loss: 0.6705, train F loss: -0.6263, acc: 0.9912\n",
      "epoch 957: train D loss: 0.6703, train F loss: -0.6239, acc: 0.9906\n",
      "epoch 958: train D loss: 0.6703, train F loss: -0.6166, acc: 0.9908\n",
      "epoch 959: train D loss: 0.6720, train F loss: -0.6119, acc: 0.9856\n",
      "epoch 960: train D loss: 0.6707, train F loss: -0.6189, acc: 0.9896\n",
      "epoch 961: train D loss: 0.6669, train F loss: -0.6221, acc: 0.9926\n",
      "epoch 962: train D loss: 0.6708, train F loss: -0.6180, acc: 0.9906\n",
      "epoch 963: train D loss: 0.6698, train F loss: -0.6177, acc: 0.9900\n",
      "epoch 964: train D loss: 0.6749, train F loss: -0.6315, acc: 0.9898\n",
      "epoch 965: train D loss: 0.6741, train F loss: -0.6213, acc: 0.9898\n",
      "epoch 966: train D loss: 0.6737, train F loss: -0.6344, acc: 0.9924\n",
      "epoch 967: train D loss: 0.6729, train F loss: -0.6247, acc: 0.9918\n",
      "epoch 968: train D loss: 0.6703, train F loss: -0.6164, acc: 0.9898\n",
      "epoch 969: train D loss: 0.6653, train F loss: -0.6114, acc: 0.9888\n",
      "epoch 970: train D loss: 0.6727, train F loss: -0.6212, acc: 0.9896\n",
      "epoch 971: train D loss: 0.6737, train F loss: -0.6113, acc: 0.9870\n",
      "epoch 972: train D loss: 0.6730, train F loss: -0.6251, acc: 0.9912\n",
      "epoch 973: train D loss: 0.6667, train F loss: -0.6124, acc: 0.9892\n",
      "epoch 974: train D loss: 0.6647, train F loss: -0.6085, acc: 0.9898\n",
      "epoch 975: train D loss: 0.6672, train F loss: -0.6122, acc: 0.9896\n",
      "epoch 976: train D loss: 0.6672, train F loss: -0.6046, acc: 0.9886\n",
      "epoch 977: train D loss: 0.6666, train F loss: -0.6129, acc: 0.9888\n",
      "epoch 978: train D loss: 0.6707, train F loss: -0.6185, acc: 0.9892\n",
      "epoch 979: train D loss: 0.6737, train F loss: -0.6162, acc: 0.9892\n",
      "epoch 980: train D loss: 0.6695, train F loss: -0.6174, acc: 0.9882\n",
      "epoch 981: train D loss: 0.6654, train F loss: -0.6185, acc: 0.9904\n",
      "epoch 982: train D loss: 0.6671, train F loss: -0.6150, acc: 0.9882\n",
      "epoch 983: train D loss: 0.6648, train F loss: -0.6132, acc: 0.9908\n",
      "epoch 984: train D loss: 0.6689, train F loss: -0.6264, acc: 0.9918\n",
      "epoch 985: train D loss: 0.6687, train F loss: -0.6126, acc: 0.9884\n",
      "epoch 986: train D loss: 0.6708, train F loss: -0.6193, acc: 0.9892\n",
      "epoch 987: train D loss: 0.6717, train F loss: -0.6256, acc: 0.9898\n",
      "epoch 988: train D loss: 0.6740, train F loss: -0.6203, acc: 0.9884\n",
      "epoch 989: train D loss: 0.6739, train F loss: -0.6312, acc: 0.9908\n",
      "epoch 990: train D loss: 0.6710, train F loss: -0.6341, acc: 0.9938\n",
      "epoch 991: train D loss: 0.6752, train F loss: -0.6258, acc: 0.9890\n",
      "epoch 992: train D loss: 0.6703, train F loss: -0.6258, acc: 0.9894\n",
      "epoch 993: train D loss: 0.6702, train F loss: -0.6216, acc: 0.9910\n",
      "epoch 994: train D loss: 0.6714, train F loss: -0.6293, acc: 0.9926\n",
      "epoch 995: train D loss: 0.6737, train F loss: -0.6178, acc: 0.9884\n",
      "epoch 996: train D loss: 0.6665, train F loss: -0.6236, acc: 0.9916\n",
      "epoch 997: train D loss: 0.6756, train F loss: -0.6158, acc: 0.9872\n",
      "epoch 998: train D loss: 0.6679, train F loss: -0.6173, acc: 0.9904\n",
      "epoch 999: train D loss: 0.6642, train F loss: -0.6168, acc: 0.9890\n",
      "epoch 1000: train D loss: 0.6715, train F loss: -0.6130, acc: 0.9882\n",
      "epoch 1001: train D loss: 0.6732, train F loss: -0.6228, acc: 0.9894\n",
      "epoch 1002: train D loss: 0.6735, train F loss: -0.6207, acc: 0.9892\n",
      "epoch 1003: train D loss: 0.6738, train F loss: -0.6266, acc: 0.9902\n",
      "epoch 1004: train D loss: 0.6732, train F loss: -0.6285, acc: 0.9908\n",
      "epoch 1005: train D loss: 0.6727, train F loss: -0.6245, acc: 0.9900\n",
      "epoch 1006: train D loss: 0.6719, train F loss: -0.6317, acc: 0.9912\n",
      "epoch 1007: train D loss: 0.6724, train F loss: -0.6268, acc: 0.9912\n",
      "epoch 1008: train D loss: 0.6716, train F loss: -0.6334, acc: 0.9920\n",
      "epoch 1009: train D loss: 0.6751, train F loss: -0.6179, acc: 0.9882\n",
      "epoch 1010: train D loss: 0.6720, train F loss: -0.6282, acc: 0.9900\n",
      "epoch 1011: train D loss: 0.6730, train F loss: -0.6278, acc: 0.9900\n",
      "epoch 1012: train D loss: 0.6728, train F loss: -0.6270, acc: 0.9914\n",
      "epoch 1013: train D loss: 0.6722, train F loss: -0.6172, acc: 0.9870\n",
      "epoch 1014: train D loss: 0.6733, train F loss: -0.6341, acc: 0.9916\n",
      "epoch 1015: train D loss: 0.6736, train F loss: -0.6160, acc: 0.9880\n",
      "epoch 1016: train D loss: 0.6736, train F loss: -0.6325, acc: 0.9930\n",
      "epoch 1017: train D loss: 0.6690, train F loss: -0.6159, acc: 0.9880\n",
      "epoch 1018: train D loss: 0.6680, train F loss: -0.6254, acc: 0.9918\n",
      "epoch 1019: train D loss: 0.6671, train F loss: -0.6216, acc: 0.9906\n",
      "epoch 1020: train D loss: 0.6737, train F loss: -0.6227, acc: 0.9908\n",
      "epoch 1021: train D loss: 0.6736, train F loss: -0.6291, acc: 0.9908\n",
      "epoch 1022: train D loss: 0.6745, train F loss: -0.6151, acc: 0.9856\n",
      "epoch 1023: train D loss: 0.6750, train F loss: -0.6208, acc: 0.9878\n",
      "epoch 1024: train D loss: 0.6672, train F loss: -0.6162, acc: 0.9892\n",
      "epoch 1025: train D loss: 0.6728, train F loss: -0.6271, acc: 0.9900\n",
      "epoch 1026: train D loss: 0.6678, train F loss: -0.6121, acc: 0.9872\n",
      "epoch 1027: train D loss: 0.6707, train F loss: -0.6265, acc: 0.9920\n",
      "epoch 1028: train D loss: 0.6673, train F loss: -0.6261, acc: 0.9920\n",
      "epoch 1029: train D loss: 0.6751, train F loss: -0.6344, acc: 0.9908\n",
      "epoch 1030: train D loss: 0.6726, train F loss: -0.6245, acc: 0.9908\n",
      "epoch 1031: train D loss: 0.6694, train F loss: -0.6215, acc: 0.9898\n",
      "epoch 1032: train D loss: 0.6666, train F loss: -0.6081, acc: 0.9878\n",
      "epoch 1033: train D loss: 0.6653, train F loss: -0.6190, acc: 0.9904\n",
      "epoch 1034: train D loss: 0.6721, train F loss: -0.6229, acc: 0.9906\n",
      "epoch 1035: train D loss: 0.6664, train F loss: -0.6252, acc: 0.9918\n",
      "epoch 1036: train D loss: 0.6739, train F loss: -0.6334, acc: 0.9922\n",
      "epoch 1037: train D loss: 0.6693, train F loss: -0.6251, acc: 0.9908\n",
      "epoch 1038: train D loss: 0.6719, train F loss: -0.6320, acc: 0.9914\n",
      "epoch 1039: train D loss: 0.6714, train F loss: -0.6284, acc: 0.9910\n",
      "epoch 1040: train D loss: 0.6758, train F loss: -0.6378, acc: 0.9922\n",
      "epoch 1041: train D loss: 0.6759, train F loss: -0.6147, acc: 0.9868\n",
      "epoch 1042: train D loss: 0.6707, train F loss: -0.6275, acc: 0.9892\n",
      "epoch 1043: train D loss: 0.6707, train F loss: -0.6240, acc: 0.9902\n",
      "epoch 1044: train D loss: 0.6713, train F loss: -0.6193, acc: 0.9892\n",
      "epoch 1045: train D loss: 0.6674, train F loss: -0.6246, acc: 0.9912\n",
      "epoch 1046: train D loss: 0.6646, train F loss: -0.6232, acc: 0.9922\n",
      "epoch 1047: train D loss: 0.6760, train F loss: -0.6217, acc: 0.9874\n",
      "epoch 1048: train D loss: 0.6761, train F loss: -0.6219, acc: 0.9866\n",
      "epoch 1049: train D loss: 0.6720, train F loss: -0.6298, acc: 0.9902\n",
      "epoch 1050: train D loss: 0.6707, train F loss: -0.6296, acc: 0.9914\n",
      "epoch 1051: train D loss: 0.6742, train F loss: -0.6346, acc: 0.9920\n",
      "epoch 1052: train D loss: 0.6753, train F loss: -0.6337, acc: 0.9898\n",
      "epoch 1053: train D loss: 0.6725, train F loss: -0.6263, acc: 0.9902\n",
      "epoch 1054: train D loss: 0.6711, train F loss: -0.6273, acc: 0.9920\n",
      "epoch 1055: train D loss: 0.6696, train F loss: -0.6217, acc: 0.9894\n",
      "epoch 1056: train D loss: 0.6739, train F loss: -0.6308, acc: 0.9904\n",
      "epoch 1057: train D loss: 0.6713, train F loss: -0.6237, acc: 0.9890\n",
      "epoch 1058: train D loss: 0.6702, train F loss: -0.6285, acc: 0.9912\n",
      "epoch 1059: train D loss: 0.6745, train F loss: -0.6309, acc: 0.9912\n",
      "epoch 1060: train D loss: 0.6732, train F loss: -0.6176, acc: 0.9892\n",
      "epoch 1061: train D loss: 0.6755, train F loss: -0.6380, acc: 0.9926\n",
      "epoch 1062: train D loss: 0.6730, train F loss: -0.6321, acc: 0.9908\n",
      "epoch 1063: train D loss: 0.6700, train F loss: -0.6298, acc: 0.9910\n",
      "epoch 1064: train D loss: 0.6771, train F loss: -0.6317, acc: 0.9902\n",
      "epoch 1065: train D loss: 0.6757, train F loss: -0.6349, acc: 0.9904\n",
      "epoch 1066: train D loss: 0.6736, train F loss: -0.6284, acc: 0.9900\n",
      "epoch 1067: train D loss: 0.6715, train F loss: -0.6217, acc: 0.9888\n",
      "epoch 1068: train D loss: 0.6709, train F loss: -0.6263, acc: 0.9908\n",
      "epoch 1069: train D loss: 0.6743, train F loss: -0.6300, acc: 0.9896\n",
      "epoch 1070: train D loss: 0.6709, train F loss: -0.6315, acc: 0.9904\n",
      "epoch 1071: train D loss: 0.6749, train F loss: -0.6261, acc: 0.9890\n",
      "epoch 1072: train D loss: 0.6719, train F loss: -0.6284, acc: 0.9908\n",
      "epoch 1073: train D loss: 0.6696, train F loss: -0.5818, acc: 0.9792\n",
      "epoch 1074: train D loss: 0.6626, train F loss: -0.6160, acc: 0.9892\n",
      "epoch 1075: train D loss: 0.6712, train F loss: -0.6150, acc: 0.9874\n",
      "epoch 1076: train D loss: 0.6716, train F loss: -0.6258, acc: 0.9894\n",
      "epoch 1077: train D loss: 0.6724, train F loss: -0.6297, acc: 0.9914\n",
      "epoch 1078: train D loss: 0.6665, train F loss: -0.6328, acc: 0.9928\n",
      "epoch 1079: train D loss: 0.6715, train F loss: -0.6251, acc: 0.9904\n",
      "epoch 1080: train D loss: 0.6748, train F loss: -0.6383, acc: 0.9936\n",
      "epoch 1081: train D loss: 0.6633, train F loss: -0.5049, acc: 0.9802\n",
      "epoch 1082: train D loss: 0.6560, train F loss: -0.5798, acc: 0.9848\n",
      "epoch 1083: train D loss: 0.6615, train F loss: -0.6018, acc: 0.9878\n",
      "epoch 1084: train D loss: 0.6612, train F loss: -0.5963, acc: 0.9888\n",
      "epoch 1085: train D loss: 0.6614, train F loss: -0.6050, acc: 0.9898\n",
      "epoch 1086: train D loss: 0.6591, train F loss: -0.6156, acc: 0.9904\n",
      "epoch 1087: train D loss: 0.6599, train F loss: -0.6111, acc: 0.9918\n",
      "epoch 1088: train D loss: 0.6669, train F loss: -0.6273, acc: 0.9934\n",
      "epoch 1089: train D loss: 0.6623, train F loss: -0.6091, acc: 0.9900\n",
      "epoch 1090: train D loss: 0.6680, train F loss: -0.6326, acc: 0.9922\n",
      "epoch 1091: train D loss: 0.6676, train F loss: -0.6278, acc: 0.9922\n",
      "epoch 1092: train D loss: 0.6652, train F loss: -0.6194, acc: 0.9892\n",
      "epoch 1093: train D loss: 0.6676, train F loss: -0.6236, acc: 0.9904\n",
      "epoch 1094: train D loss: 0.6726, train F loss: -0.6330, acc: 0.9926\n",
      "epoch 1095: train D loss: 0.6648, train F loss: -0.6077, acc: 0.9886\n",
      "epoch 1096: train D loss: 0.6694, train F loss: -0.6318, acc: 0.9922\n",
      "epoch 1097: train D loss: 0.6686, train F loss: -0.6280, acc: 0.9904\n",
      "epoch 1098: train D loss: 0.6720, train F loss: -0.6362, acc: 0.9924\n",
      "epoch 1099: train D loss: 0.6699, train F loss: -0.6305, acc: 0.9906\n",
      "epoch 1100: train D loss: 0.6679, train F loss: -0.6238, acc: 0.9902\n",
      "epoch 1101: train D loss: 0.6676, train F loss: -0.6265, acc: 0.9908\n",
      "epoch 1102: train D loss: 0.6724, train F loss: -0.6339, acc: 0.9930\n",
      "epoch 1103: train D loss: 0.6738, train F loss: -0.6398, acc: 0.9924\n",
      "epoch 1104: train D loss: 0.6708, train F loss: -0.6207, acc: 0.9896\n",
      "epoch 1105: train D loss: 0.6738, train F loss: -0.6278, acc: 0.9902\n",
      "epoch 1106: train D loss: 0.6732, train F loss: -0.6351, acc: 0.9914\n",
      "epoch 1107: train D loss: 0.6753, train F loss: -0.6234, acc: 0.9874\n",
      "epoch 1108: train D loss: 0.6682, train F loss: -0.6192, acc: 0.9886\n",
      "epoch 1109: train D loss: 0.6686, train F loss: -0.6245, acc: 0.9904\n",
      "epoch 1110: train D loss: 0.6687, train F loss: -0.6283, acc: 0.9896\n",
      "epoch 1111: train D loss: 0.6685, train F loss: -0.6292, acc: 0.9912\n",
      "epoch 1112: train D loss: 0.6690, train F loss: -0.6213, acc: 0.9892\n",
      "epoch 1113: train D loss: 0.6739, train F loss: -0.6314, acc: 0.9906\n",
      "epoch 1114: train D loss: 0.6730, train F loss: -0.6307, acc: 0.9898\n",
      "epoch 1115: train D loss: 0.6696, train F loss: -0.6228, acc: 0.9914\n",
      "epoch 1116: train D loss: 0.6683, train F loss: -0.6283, acc: 0.9900\n",
      "epoch 1117: train D loss: 0.6705, train F loss: -0.6163, acc: 0.9898\n",
      "epoch 1118: train D loss: 0.6720, train F loss: -0.6299, acc: 0.9914\n",
      "epoch 1119: train D loss: 0.6666, train F loss: -0.6241, acc: 0.9912\n",
      "epoch 1120: train D loss: 0.6722, train F loss: -0.6351, acc: 0.9918\n",
      "epoch 1121: train D loss: 0.6712, train F loss: -0.6278, acc: 0.9908\n",
      "epoch 1122: train D loss: 0.6748, train F loss: -0.6385, acc: 0.9922\n",
      "epoch 1123: train D loss: 0.6704, train F loss: -0.6250, acc: 0.9892\n",
      "epoch 1124: train D loss: 0.6739, train F loss: -0.6347, acc: 0.9904\n",
      "epoch 1125: train D loss: 0.6749, train F loss: -0.6348, acc: 0.9900\n",
      "epoch 1126: train D loss: 0.6757, train F loss: -0.6309, acc: 0.9900\n",
      "epoch 1127: train D loss: 0.6749, train F loss: -0.6359, acc: 0.9896\n",
      "epoch 1128: train D loss: 0.6703, train F loss: -0.6359, acc: 0.9920\n",
      "epoch 1129: train D loss: 0.6676, train F loss: -0.6149, acc: 0.9898\n",
      "epoch 1130: train D loss: 0.6732, train F loss: -0.6231, acc: 0.9890\n",
      "epoch 1131: train D loss: 0.6699, train F loss: -0.6253, acc: 0.9912\n",
      "epoch 1132: train D loss: 0.6716, train F loss: -0.6360, acc: 0.9914\n",
      "epoch 1133: train D loss: 0.6725, train F loss: -0.6301, acc: 0.9904\n",
      "epoch 1134: train D loss: 0.6705, train F loss: -0.6326, acc: 0.9908\n",
      "epoch 1135: train D loss: 0.6735, train F loss: -0.6295, acc: 0.9894\n",
      "epoch 1136: train D loss: 0.6706, train F loss: -0.6316, acc: 0.9906\n",
      "epoch 1137: train D loss: 0.6756, train F loss: -0.6406, acc: 0.9906\n",
      "epoch 1138: train D loss: 0.6749, train F loss: -0.6313, acc: 0.9900\n",
      "epoch 1139: train D loss: 0.6729, train F loss: -0.6343, acc: 0.9920\n",
      "epoch 1140: train D loss: 0.6729, train F loss: -0.6292, acc: 0.9900\n",
      "epoch 1141: train D loss: 0.6746, train F loss: -0.6299, acc: 0.9876\n",
      "epoch 1142: train D loss: 0.6726, train F loss: -0.6410, acc: 0.9938\n",
      "epoch 1143: train D loss: 0.6756, train F loss: -0.6400, acc: 0.9916\n",
      "epoch 1144: train D loss: 0.6747, train F loss: -0.6297, acc: 0.9892\n",
      "epoch 1145: train D loss: 0.6760, train F loss: -0.6360, acc: 0.9898\n",
      "epoch 1146: train D loss: 0.6727, train F loss: -0.6290, acc: 0.9908\n",
      "epoch 1147: train D loss: 0.6710, train F loss: -0.6298, acc: 0.9904\n",
      "epoch 1148: train D loss: 0.6750, train F loss: -0.6394, acc: 0.9918\n",
      "epoch 1149: train D loss: 0.6715, train F loss: -0.6267, acc: 0.9906\n",
      "epoch 1150: train D loss: 0.6733, train F loss: -0.6356, acc: 0.9918\n",
      "epoch 1151: train D loss: 0.6779, train F loss: -0.6385, acc: 0.9926\n",
      "epoch 1152: train D loss: 0.6745, train F loss: -0.6378, acc: 0.9918\n",
      "epoch 1153: train D loss: 0.6707, train F loss: -0.6265, acc: 0.9914\n",
      "epoch 1154: train D loss: 0.6705, train F loss: -0.6302, acc: 0.9912\n",
      "epoch 1155: train D loss: 0.6697, train F loss: -0.6285, acc: 0.9900\n",
      "epoch 1156: train D loss: 0.6735, train F loss: -0.6278, acc: 0.9904\n",
      "epoch 1157: train D loss: 0.6758, train F loss: -0.6372, acc: 0.9904\n",
      "epoch 1158: train D loss: 0.6726, train F loss: -0.6287, acc: 0.9894\n",
      "epoch 1159: train D loss: 0.6752, train F loss: -0.6330, acc: 0.9902\n",
      "epoch 1160: train D loss: 0.6758, train F loss: -0.6324, acc: 0.9902\n",
      "epoch 1161: train D loss: 0.6729, train F loss: -0.6351, acc: 0.9922\n",
      "epoch 1162: train D loss: 0.6645, train F loss: -0.6240, acc: 0.9918\n",
      "epoch 1163: train D loss: 0.6707, train F loss: -0.6324, acc: 0.9922\n",
      "epoch 1164: train D loss: 0.6733, train F loss: -0.6360, acc: 0.9912\n",
      "epoch 1165: train D loss: 0.6765, train F loss: -0.6382, acc: 0.9906\n",
      "epoch 1166: train D loss: 0.6738, train F loss: -0.6330, acc: 0.9904\n",
      "epoch 1167: train D loss: 0.6705, train F loss: -0.6128, acc: 0.9898\n",
      "epoch 1168: train D loss: 0.6607, train F loss: -0.5905, acc: 0.9836\n",
      "epoch 1169: train D loss: 0.6627, train F loss: -0.6174, acc: 0.9900\n",
      "epoch 1170: train D loss: 0.6696, train F loss: -0.6312, acc: 0.9922\n",
      "epoch 1171: train D loss: 0.6761, train F loss: -0.6252, acc: 0.9878\n",
      "epoch 1172: train D loss: 0.6682, train F loss: -0.6306, acc: 0.9898\n",
      "epoch 1173: train D loss: 0.6674, train F loss: -0.6302, acc: 0.9916\n",
      "epoch 1174: train D loss: 0.6709, train F loss: -0.6356, acc: 0.9926\n",
      "epoch 1175: train D loss: 0.6747, train F loss: -0.6353, acc: 0.9932\n",
      "epoch 1176: train D loss: 0.6697, train F loss: -0.6270, acc: 0.9908\n",
      "epoch 1177: train D loss: 0.6685, train F loss: -0.6322, acc: 0.9928\n",
      "epoch 1178: train D loss: 0.6744, train F loss: -0.6355, acc: 0.9922\n",
      "epoch 1179: train D loss: 0.6773, train F loss: -0.6387, acc: 0.9906\n",
      "epoch 1180: train D loss: 0.6747, train F loss: -0.6280, acc: 0.9888\n",
      "epoch 1181: train D loss: 0.6679, train F loss: -0.6319, acc: 0.9928\n",
      "epoch 1182: train D loss: 0.6699, train F loss: -0.6354, acc: 0.9916\n",
      "epoch 1183: train D loss: 0.6733, train F loss: -0.6396, acc: 0.9916\n",
      "epoch 1184: train D loss: 0.6735, train F loss: -0.6356, acc: 0.9912\n",
      "epoch 1185: train D loss: 0.6727, train F loss: -0.6334, acc: 0.9896\n",
      "epoch 1186: train D loss: 0.6741, train F loss: -0.6340, acc: 0.9904\n",
      "epoch 1187: train D loss: 0.6784, train F loss: -0.6355, acc: 0.9906\n",
      "epoch 1188: train D loss: 0.6742, train F loss: -0.6397, acc: 0.9908\n",
      "epoch 1189: train D loss: 0.6757, train F loss: -0.6466, acc: 0.9940\n",
      "epoch 1190: train D loss: 0.6772, train F loss: -0.6454, acc: 0.9916\n",
      "epoch 1191: train D loss: 0.6722, train F loss: -0.6334, acc: 0.9900\n",
      "epoch 1192: train D loss: 0.6765, train F loss: -0.6374, acc: 0.9914\n",
      "epoch 1193: train D loss: 0.6764, train F loss: -0.6395, acc: 0.9894\n",
      "epoch 1194: train D loss: 0.6743, train F loss: -0.6328, acc: 0.9914\n",
      "epoch 1195: train D loss: 0.6780, train F loss: -0.6387, acc: 0.9892\n",
      "epoch 1196: train D loss: 0.6760, train F loss: -0.6415, acc: 0.9924\n",
      "epoch 1197: train D loss: 0.6732, train F loss: -0.6136, acc: 0.9856\n",
      "epoch 1198: train D loss: 0.6712, train F loss: -0.6330, acc: 0.9904\n",
      "epoch 1199: train D loss: 0.6730, train F loss: -0.6343, acc: 0.9912\n",
      "epoch 1200: train D loss: 0.6739, train F loss: -0.6315, acc: 0.9902\n",
      "epoch 1201: train D loss: 0.6701, train F loss: -0.6219, acc: 0.9910\n",
      "epoch 1202: train D loss: 0.6684, train F loss: -0.6313, acc: 0.9928\n",
      "epoch 1203: train D loss: 0.6764, train F loss: -0.6367, acc: 0.9910\n",
      "epoch 1204: train D loss: 0.6664, train F loss: -0.6228, acc: 0.9914\n",
      "epoch 1205: train D loss: 0.6773, train F loss: -0.6438, acc: 0.9926\n",
      "epoch 1206: train D loss: 0.6731, train F loss: -0.6411, acc: 0.9918\n",
      "epoch 1207: train D loss: 0.6720, train F loss: -0.6411, acc: 0.9932\n",
      "epoch 1208: train D loss: 0.6734, train F loss: -0.6380, acc: 0.9912\n",
      "epoch 1209: train D loss: 0.6723, train F loss: -0.6374, acc: 0.9910\n",
      "epoch 1210: train D loss: 0.6688, train F loss: -0.6303, acc: 0.9900\n",
      "epoch 1211: train D loss: 0.6703, train F loss: -0.6185, acc: 0.9874\n",
      "epoch 1212: train D loss: 0.6657, train F loss: -0.6190, acc: 0.9908\n",
      "epoch 1213: train D loss: 0.6728, train F loss: -0.6357, acc: 0.9920\n",
      "epoch 1214: train D loss: 0.6738, train F loss: -0.6301, acc: 0.9894\n",
      "epoch 1215: train D loss: 0.6726, train F loss: -0.6355, acc: 0.9908\n",
      "epoch 1216: train D loss: 0.6707, train F loss: -0.6405, acc: 0.9928\n",
      "epoch 1217: train D loss: 0.6728, train F loss: -0.6271, acc: 0.9890\n",
      "epoch 1218: train D loss: 0.6718, train F loss: -0.6337, acc: 0.9918\n",
      "epoch 1219: train D loss: 0.6729, train F loss: -0.6361, acc: 0.9908\n",
      "epoch 1220: train D loss: 0.6696, train F loss: -0.6251, acc: 0.9896\n",
      "epoch 1221: train D loss: 0.6703, train F loss: -0.6355, acc: 0.9932\n",
      "epoch 1222: train D loss: 0.6726, train F loss: -0.6375, acc: 0.9922\n",
      "epoch 1223: train D loss: 0.6768, train F loss: -0.6369, acc: 0.9906\n",
      "epoch 1224: train D loss: 0.6724, train F loss: -0.6288, acc: 0.9890\n",
      "epoch 1225: train D loss: 0.6716, train F loss: -0.6286, acc: 0.9900\n",
      "epoch 1226: train D loss: 0.6724, train F loss: -0.6395, acc: 0.9936\n",
      "epoch 1227: train D loss: 0.6738, train F loss: -0.6300, acc: 0.9892\n",
      "epoch 1228: train D loss: 0.6706, train F loss: -0.6315, acc: 0.9910\n",
      "epoch 1229: train D loss: 0.6730, train F loss: -0.6248, acc: 0.9896\n",
      "epoch 1230: train D loss: 0.6662, train F loss: -0.6306, acc: 0.9920\n",
      "epoch 1231: train D loss: 0.6711, train F loss: -0.6301, acc: 0.9902\n",
      "epoch 1232: train D loss: 0.6774, train F loss: -0.6458, acc: 0.9932\n",
      "epoch 1233: train D loss: 0.6723, train F loss: -0.6319, acc: 0.9888\n",
      "epoch 1234: train D loss: 0.6681, train F loss: -0.6233, acc: 0.9900\n",
      "epoch 1235: train D loss: 0.6728, train F loss: -0.6382, acc: 0.9918\n",
      "epoch 1236: train D loss: 0.6751, train F loss: -0.6449, acc: 0.9936\n",
      "epoch 1237: train D loss: 0.6770, train F loss: -0.6400, acc: 0.9900\n",
      "epoch 1238: train D loss: 0.6717, train F loss: -0.6361, acc: 0.9898\n",
      "epoch 1239: train D loss: 0.6750, train F loss: -0.6318, acc: 0.9896\n",
      "epoch 1240: train D loss: 0.6764, train F loss: -0.6400, acc: 0.9914\n",
      "epoch 1241: train D loss: 0.6729, train F loss: -0.6396, acc: 0.9926\n",
      "epoch 1242: train D loss: 0.6726, train F loss: -0.6313, acc: 0.9904\n",
      "epoch 1243: train D loss: 0.6742, train F loss: -0.6304, acc: 0.9900\n",
      "epoch 1244: train D loss: 0.6746, train F loss: -0.6395, acc: 0.9920\n",
      "epoch 1245: train D loss: 0.6708, train F loss: -0.6345, acc: 0.9898\n",
      "epoch 1246: train D loss: 0.6702, train F loss: -0.6239, acc: 0.9898\n",
      "epoch 1247: train D loss: 0.6761, train F loss: -0.6350, acc: 0.9902\n",
      "epoch 1248: train D loss: 0.6703, train F loss: -0.6337, acc: 0.9914\n",
      "epoch 1249: train D loss: 0.6726, train F loss: -0.6340, acc: 0.9914\n",
      "epoch 1250: train D loss: 0.6743, train F loss: -0.6356, acc: 0.9914\n",
      "epoch 1251: train D loss: 0.6736, train F loss: -0.6398, acc: 0.9918\n",
      "epoch 1252: train D loss: 0.6755, train F loss: -0.6460, acc: 0.9944\n",
      "epoch 1253: train D loss: 0.6761, train F loss: -0.6498, acc: 0.9938\n",
      "epoch 1254: train D loss: 0.6734, train F loss: -0.6433, acc: 0.9938\n",
      "epoch 1255: train D loss: 0.6761, train F loss: -0.6350, acc: 0.9894\n",
      "epoch 1256: train D loss: 0.6757, train F loss: -0.6422, acc: 0.9922\n",
      "epoch 1257: train D loss: 0.6771, train F loss: -0.6337, acc: 0.9916\n",
      "epoch 1258: train D loss: 0.6720, train F loss: -0.6338, acc: 0.9890\n",
      "epoch 1259: train D loss: 0.6768, train F loss: -0.6393, acc: 0.9906\n",
      "epoch 1260: train D loss: 0.6739, train F loss: -0.6433, acc: 0.9922\n",
      "epoch 1261: train D loss: 0.6775, train F loss: -0.6402, acc: 0.9922\n",
      "epoch 1262: train D loss: 0.6740, train F loss: -0.6340, acc: 0.9918\n",
      "epoch 1263: train D loss: 0.6735, train F loss: -0.6327, acc: 0.9916\n",
      "epoch 1264: train D loss: 0.6710, train F loss: -0.6371, acc: 0.9926\n",
      "epoch 1265: train D loss: 0.6753, train F loss: -0.6348, acc: 0.9900\n",
      "epoch 1266: train D loss: 0.6750, train F loss: -0.6451, acc: 0.9934\n",
      "epoch 1267: train D loss: 0.6729, train F loss: -0.6453, acc: 0.9936\n",
      "epoch 1268: train D loss: 0.6751, train F loss: -0.6379, acc: 0.9906\n",
      "epoch 1269: train D loss: 0.6725, train F loss: -0.6413, acc: 0.9926\n",
      "epoch 1270: train D loss: 0.6748, train F loss: -0.6396, acc: 0.9918\n",
      "epoch 1271: train D loss: 0.6762, train F loss: -0.6402, acc: 0.9898\n",
      "epoch 1272: train D loss: 0.6754, train F loss: -0.6356, acc: 0.9898\n",
      "epoch 1273: train D loss: 0.6722, train F loss: -0.6332, acc: 0.9902\n",
      "epoch 1274: train D loss: 0.6744, train F loss: -0.6406, acc: 0.9914\n",
      "epoch 1275: train D loss: 0.6743, train F loss: -0.6282, acc: 0.9884\n",
      "epoch 1276: train D loss: 0.6771, train F loss: -0.6387, acc: 0.9898\n",
      "epoch 1277: train D loss: 0.6723, train F loss: -0.6381, acc: 0.9908\n",
      "epoch 1278: train D loss: 0.6698, train F loss: -0.6257, acc: 0.9888\n",
      "epoch 1279: train D loss: 0.6711, train F loss: -0.6371, acc: 0.9902\n",
      "epoch 1280: train D loss: 0.6739, train F loss: -0.6339, acc: 0.9890\n",
      "epoch 1281: train D loss: 0.6717, train F loss: -0.6318, acc: 0.9890\n",
      "epoch 1282: train D loss: 0.6743, train F loss: -0.6407, acc: 0.9918\n",
      "epoch 1283: train D loss: 0.6712, train F loss: -0.6343, acc: 0.9916\n",
      "epoch 1284: train D loss: 0.6698, train F loss: -0.6376, acc: 0.9916\n",
      "epoch 1285: train D loss: 0.6744, train F loss: -0.6338, acc: 0.9902\n",
      "epoch 1286: train D loss: 0.6727, train F loss: -0.6326, acc: 0.9894\n",
      "epoch 1287: train D loss: 0.6688, train F loss: -0.6215, acc: 0.9904\n",
      "epoch 1288: train D loss: 0.6758, train F loss: -0.5653, acc: 0.9856\n",
      "epoch 1289: train D loss: 0.6590, train F loss: -0.6064, acc: 0.9844\n",
      "epoch 1290: train D loss: 0.6665, train F loss: -0.6192, acc: 0.9888\n",
      "epoch 1291: train D loss: 0.6638, train F loss: -0.6207, acc: 0.9910\n",
      "epoch 1292: train D loss: 0.6652, train F loss: -0.6254, acc: 0.9922\n",
      "epoch 1293: train D loss: 0.6655, train F loss: -0.6315, acc: 0.9930\n",
      "epoch 1294: train D loss: 0.6718, train F loss: -0.6358, acc: 0.9904\n",
      "epoch 1295: train D loss: 0.6737, train F loss: -0.6417, acc: 0.9918\n",
      "epoch 1296: train D loss: 0.6740, train F loss: -0.6305, acc: 0.9888\n",
      "epoch 1297: train D loss: 0.6722, train F loss: -0.6407, acc: 0.9924\n",
      "epoch 1298: train D loss: 0.6704, train F loss: -0.6392, acc: 0.9950\n",
      "epoch 1299: train D loss: 0.6697, train F loss: -0.6402, acc: 0.9938\n",
      "epoch 1300: train D loss: 0.6708, train F loss: -0.6362, acc: 0.9922\n",
      "epoch 1301: train D loss: 0.6684, train F loss: -0.6218, acc: 0.9894\n",
      "epoch 1302: train D loss: 0.6747, train F loss: -0.6382, acc: 0.9912\n",
      "epoch 1303: train D loss: 0.6749, train F loss: -0.6372, acc: 0.9902\n",
      "epoch 1304: train D loss: 0.6664, train F loss: -0.6316, acc: 0.9928\n",
      "epoch 1305: train D loss: 0.6712, train F loss: -0.6187, acc: 0.9868\n",
      "epoch 1306: train D loss: 0.6685, train F loss: -0.6319, acc: 0.9896\n",
      "epoch 1307: train D loss: 0.6740, train F loss: -0.6399, acc: 0.9926\n",
      "epoch 1308: train D loss: 0.6731, train F loss: -0.6411, acc: 0.9918\n",
      "epoch 1309: train D loss: 0.6766, train F loss: -0.6461, acc: 0.9918\n",
      "epoch 1310: train D loss: 0.6720, train F loss: -0.6334, acc: 0.9912\n",
      "epoch 1311: train D loss: 0.6754, train F loss: -0.6442, acc: 0.9912\n",
      "epoch 1312: train D loss: 0.6684, train F loss: -0.6387, acc: 0.9934\n",
      "epoch 1313: train D loss: 0.6713, train F loss: -0.6321, acc: 0.9910\n",
      "epoch 1314: train D loss: 0.6757, train F loss: -0.6414, acc: 0.9912\n",
      "epoch 1315: train D loss: 0.6736, train F loss: -0.6422, acc: 0.9926\n",
      "epoch 1316: train D loss: 0.6696, train F loss: -0.6309, acc: 0.9906\n",
      "epoch 1317: train D loss: 0.6703, train F loss: -0.6290, acc: 0.9906\n",
      "epoch 1318: train D loss: 0.6734, train F loss: -0.6291, acc: 0.9896\n",
      "epoch 1319: train D loss: 0.6754, train F loss: -0.6362, acc: 0.9910\n",
      "epoch 1320: train D loss: 0.6756, train F loss: -0.6399, acc: 0.9922\n",
      "epoch 1321: train D loss: 0.6722, train F loss: -0.6394, acc: 0.9920\n",
      "epoch 1322: train D loss: 0.6742, train F loss: -0.6372, acc: 0.9910\n",
      "epoch 1323: train D loss: 0.6779, train F loss: -0.6440, acc: 0.9908\n",
      "epoch 1324: train D loss: 0.6711, train F loss: -0.6363, acc: 0.9902\n",
      "epoch 1325: train D loss: 0.6753, train F loss: -0.6396, acc: 0.9914\n",
      "epoch 1326: train D loss: 0.6693, train F loss: -0.6333, acc: 0.9912\n",
      "epoch 1327: train D loss: 0.6791, train F loss: -0.6409, acc: 0.9918\n",
      "epoch 1328: train D loss: 0.6735, train F loss: -0.6348, acc: 0.9910\n",
      "epoch 1329: train D loss: 0.6792, train F loss: -0.6348, acc: 0.9890\n",
      "epoch 1330: train D loss: 0.6725, train F loss: -0.6328, acc: 0.9900\n",
      "epoch 1331: train D loss: 0.6714, train F loss: -0.6323, acc: 0.9902\n",
      "epoch 1332: train D loss: 0.6737, train F loss: -0.6393, acc: 0.9922\n",
      "epoch 1333: train D loss: 0.6779, train F loss: -0.6393, acc: 0.9900\n",
      "epoch 1334: train D loss: 0.6685, train F loss: -0.6277, acc: 0.9900\n",
      "epoch 1335: train D loss: 0.6756, train F loss: -0.6372, acc: 0.9914\n",
      "epoch 1336: train D loss: 0.6714, train F loss: -0.6321, acc: 0.9898\n",
      "epoch 1337: train D loss: 0.6695, train F loss: -0.6368, acc: 0.9918\n",
      "epoch 1338: train D loss: 0.6698, train F loss: -0.6317, acc: 0.9916\n",
      "epoch 1339: train D loss: 0.6685, train F loss: -0.6386, acc: 0.9930\n",
      "epoch 1340: train D loss: 0.6721, train F loss: -0.6344, acc: 0.9910\n",
      "epoch 1341: train D loss: 0.6741, train F loss: -0.6422, acc: 0.9924\n",
      "epoch 1342: train D loss: 0.6765, train F loss: -0.6429, acc: 0.9912\n",
      "epoch 1343: train D loss: 0.6728, train F loss: -0.6415, acc: 0.9916\n",
      "epoch 1344: train D loss: 0.6742, train F loss: -0.6354, acc: 0.9918\n",
      "epoch 1345: train D loss: 0.6727, train F loss: -0.6408, acc: 0.9918\n",
      "epoch 1346: train D loss: 0.6737, train F loss: -0.6284, acc: 0.9894\n",
      "epoch 1347: train D loss: 0.6781, train F loss: -0.6435, acc: 0.9910\n",
      "epoch 1348: train D loss: 0.6713, train F loss: -0.6313, acc: 0.9908\n",
      "epoch 1349: train D loss: 0.6740, train F loss: -0.6376, acc: 0.9904\n",
      "epoch 1350: train D loss: 0.6742, train F loss: -0.6332, acc: 0.9914\n",
      "epoch 1351: train D loss: 0.6740, train F loss: -0.6316, acc: 0.9910\n",
      "epoch 1352: train D loss: 0.6721, train F loss: -0.6399, acc: 0.9926\n",
      "epoch 1353: train D loss: 0.6728, train F loss: -0.6438, acc: 0.9926\n",
      "epoch 1354: train D loss: 0.6751, train F loss: -0.6443, acc: 0.9924\n",
      "epoch 1355: train D loss: 0.6747, train F loss: -0.6429, acc: 0.9928\n",
      "epoch 1356: train D loss: 0.6767, train F loss: -0.6516, acc: 0.9938\n",
      "epoch 1357: train D loss: 0.6777, train F loss: -0.6398, acc: 0.9900\n",
      "epoch 1358: train D loss: 0.6732, train F loss: -0.6350, acc: 0.9908\n",
      "epoch 1359: train D loss: 0.6765, train F loss: -0.6448, acc: 0.9918\n",
      "epoch 1360: train D loss: 0.6726, train F loss: -0.6379, acc: 0.9912\n",
      "epoch 1361: train D loss: 0.6753, train F loss: -0.6403, acc: 0.9912\n",
      "epoch 1362: train D loss: 0.6752, train F loss: -0.6394, acc: 0.9914\n",
      "epoch 1363: train D loss: 0.6736, train F loss: -0.6377, acc: 0.9916\n",
      "epoch 1364: train D loss: 0.6789, train F loss: -0.6412, acc: 0.9910\n",
      "epoch 1365: train D loss: 0.6742, train F loss: -0.6372, acc: 0.9896\n",
      "epoch 1366: train D loss: 0.6741, train F loss: -0.6396, acc: 0.9922\n",
      "epoch 1367: train D loss: 0.6736, train F loss: -0.6389, acc: 0.9916\n",
      "epoch 1368: train D loss: 0.6749, train F loss: -0.6412, acc: 0.9904\n",
      "epoch 1369: train D loss: 0.6713, train F loss: -0.6381, acc: 0.9914\n",
      "epoch 1370: train D loss: 0.6770, train F loss: -0.6441, acc: 0.9928\n",
      "epoch 1371: train D loss: 0.6750, train F loss: -0.6435, acc: 0.9932\n",
      "epoch 1372: train D loss: 0.6741, train F loss: -0.6392, acc: 0.9906\n",
      "epoch 1373: train D loss: 0.6757, train F loss: -0.6402, acc: 0.9926\n",
      "epoch 1374: train D loss: 0.6770, train F loss: -0.6352, acc: 0.9888\n",
      "epoch 1375: train D loss: 0.6765, train F loss: -0.6438, acc: 0.9912\n",
      "epoch 1376: train D loss: 0.6811, train F loss: -0.6540, acc: 0.9940\n",
      "epoch 1377: train D loss: 0.6744, train F loss: -0.6448, acc: 0.9922\n",
      "epoch 1378: train D loss: 0.6728, train F loss: -0.6352, acc: 0.9902\n",
      "epoch 1379: train D loss: 0.6719, train F loss: -0.6356, acc: 0.9902\n",
      "epoch 1380: train D loss: 0.6721, train F loss: -0.6430, acc: 0.9930\n",
      "epoch 1381: train D loss: 0.6712, train F loss: -0.6380, acc: 0.9926\n",
      "epoch 1382: train D loss: 0.6710, train F loss: -0.6340, acc: 0.9906\n",
      "epoch 1383: train D loss: 0.6722, train F loss: -0.6286, acc: 0.9910\n",
      "epoch 1384: train D loss: 0.6737, train F loss: -0.6303, acc: 0.9900\n",
      "epoch 1385: train D loss: 0.6770, train F loss: -0.6461, acc: 0.9918\n",
      "epoch 1386: train D loss: 0.6736, train F loss: -0.6386, acc: 0.9918\n",
      "epoch 1387: train D loss: 0.6755, train F loss: -0.6421, acc: 0.9938\n",
      "epoch 1388: train D loss: 0.6711, train F loss: -0.6334, acc: 0.9918\n",
      "epoch 1389: train D loss: 0.6738, train F loss: -0.6360, acc: 0.9906\n",
      "epoch 1390: train D loss: 0.6735, train F loss: -0.6371, acc: 0.9908\n",
      "epoch 1391: train D loss: 0.6747, train F loss: -0.6448, acc: 0.9934\n",
      "epoch 1392: train D loss: 0.6772, train F loss: -0.6419, acc: 0.9916\n",
      "epoch 1393: train D loss: 0.6783, train F loss: -0.6359, acc: 0.9886\n",
      "epoch 1394: train D loss: 0.6737, train F loss: -0.6409, acc: 0.9924\n",
      "epoch 1395: train D loss: 0.6779, train F loss: -0.6405, acc: 0.9912\n",
      "epoch 1396: train D loss: 0.6719, train F loss: -0.6359, acc: 0.9914\n",
      "epoch 1397: train D loss: 0.6686, train F loss: -0.6389, acc: 0.9932\n",
      "epoch 1398: train D loss: 0.6764, train F loss: -0.6381, acc: 0.9920\n",
      "epoch 1399: train D loss: 0.6749, train F loss: -0.6425, acc: 0.9924\n",
      "epoch 1400: train D loss: 0.6775, train F loss: -0.6410, acc: 0.9910\n",
      "epoch 1401: train D loss: 0.6727, train F loss: -0.6384, acc: 0.9918\n",
      "epoch 1402: train D loss: 0.6746, train F loss: -0.6368, acc: 0.9898\n",
      "epoch 1403: train D loss: 0.6740, train F loss: -0.6414, acc: 0.9924\n",
      "epoch 1404: train D loss: 0.6787, train F loss: -0.6491, acc: 0.9920\n",
      "epoch 1405: train D loss: 0.6786, train F loss: -0.6396, acc: 0.9906\n",
      "epoch 1406: train D loss: 0.6761, train F loss: -0.6407, acc: 0.9908\n",
      "epoch 1407: train D loss: 0.6734, train F loss: -0.6389, acc: 0.9912\n",
      "epoch 1408: train D loss: 0.6756, train F loss: -0.6484, acc: 0.9930\n",
      "epoch 1409: train D loss: 0.6748, train F loss: -0.6325, acc: 0.9904\n",
      "epoch 1410: train D loss: 0.6747, train F loss: -0.6445, acc: 0.9936\n",
      "epoch 1411: train D loss: 0.6770, train F loss: -0.6370, acc: 0.9894\n",
      "epoch 1412: train D loss: 0.6772, train F loss: -0.6342, acc: 0.9906\n",
      "epoch 1413: train D loss: 0.6770, train F loss: -0.6472, acc: 0.9936\n",
      "epoch 1414: train D loss: 0.6771, train F loss: -0.6392, acc: 0.9914\n",
      "epoch 1415: train D loss: 0.6748, train F loss: -0.6315, acc: 0.9898\n",
      "epoch 1416: train D loss: 0.6808, train F loss: -0.6487, acc: 0.9902\n",
      "epoch 1417: train D loss: 0.6732, train F loss: -0.6395, acc: 0.9920\n",
      "epoch 1418: train D loss: 0.6733, train F loss: -0.6228, acc: 0.9916\n",
      "epoch 1419: train D loss: 0.6738, train F loss: -0.6350, acc: 0.9904\n",
      "epoch 1420: train D loss: 0.6725, train F loss: -0.6396, acc: 0.9912\n",
      "epoch 1421: train D loss: 0.6770, train F loss: -0.6425, acc: 0.9912\n",
      "epoch 1422: train D loss: 0.6746, train F loss: -0.6379, acc: 0.9906\n",
      "epoch 1423: train D loss: 0.6703, train F loss: -0.5491, acc: 0.9870\n",
      "epoch 1424: train D loss: 0.6718, train F loss: -0.6387, acc: 0.9924\n",
      "epoch 1425: train D loss: 0.6663, train F loss: -0.6271, acc: 0.9918\n",
      "epoch 1426: train D loss: 0.6720, train F loss: -0.6417, acc: 0.9936\n",
      "epoch 1427: train D loss: 0.6767, train F loss: -0.6393, acc: 0.9908\n",
      "epoch 1428: train D loss: 0.6679, train F loss: -0.6225, acc: 0.9906\n",
      "epoch 1429: train D loss: 0.6754, train F loss: -0.6444, acc: 0.9912\n",
      "epoch 1430: train D loss: 0.6735, train F loss: -0.6434, acc: 0.9924\n",
      "epoch 1431: train D loss: 0.6744, train F loss: -0.6431, acc: 0.9916\n",
      "epoch 1432: train D loss: 0.6768, train F loss: -0.6439, acc: 0.9924\n",
      "epoch 1433: train D loss: 0.6740, train F loss: -0.6455, acc: 0.9938\n",
      "epoch 1434: train D loss: 0.6796, train F loss: -0.6450, acc: 0.9918\n",
      "epoch 1435: train D loss: 0.6769, train F loss: -0.6521, acc: 0.9938\n",
      "epoch 1436: train D loss: 0.6796, train F loss: -0.6486, acc: 0.9928\n",
      "epoch 1437: train D loss: 0.6764, train F loss: -0.6469, acc: 0.9936\n",
      "epoch 1438: train D loss: 0.6741, train F loss: -0.6373, acc: 0.9924\n",
      "epoch 1439: train D loss: 0.6755, train F loss: -0.6396, acc: 0.9912\n",
      "epoch 1440: train D loss: 0.6732, train F loss: -0.6380, acc: 0.9906\n",
      "epoch 1441: train D loss: 0.6768, train F loss: -0.6350, acc: 0.9908\n",
      "epoch 1442: train D loss: 0.6689, train F loss: -0.6251, acc: 0.9908\n",
      "epoch 1443: train D loss: 0.6731, train F loss: -0.6432, acc: 0.9920\n",
      "epoch 1444: train D loss: 0.6771, train F loss: -0.6427, acc: 0.9928\n",
      "epoch 1445: train D loss: 0.6745, train F loss: -0.6425, acc: 0.9918\n",
      "epoch 1446: train D loss: 0.6756, train F loss: -0.6395, acc: 0.9920\n",
      "epoch 1447: train D loss: 0.6756, train F loss: -0.6385, acc: 0.9904\n",
      "epoch 1448: train D loss: 0.6779, train F loss: -0.6436, acc: 0.9908\n",
      "epoch 1449: train D loss: 0.6746, train F loss: -0.6429, acc: 0.9922\n",
      "epoch 1450: train D loss: 0.6724, train F loss: -0.6274, acc: 0.9912\n",
      "epoch 1451: train D loss: 0.6763, train F loss: -0.6367, acc: 0.9890\n",
      "epoch 1452: train D loss: 0.6734, train F loss: -0.6345, acc: 0.9906\n",
      "epoch 1453: train D loss: 0.6733, train F loss: -0.6429, acc: 0.9930\n",
      "epoch 1454: train D loss: 0.6740, train F loss: -0.6430, acc: 0.9928\n",
      "epoch 1455: train D loss: 0.6741, train F loss: -0.6468, acc: 0.9926\n",
      "epoch 1456: train D loss: 0.6752, train F loss: -0.6424, acc: 0.9914\n",
      "epoch 1457: train D loss: 0.6758, train F loss: -0.6438, acc: 0.9926\n",
      "epoch 1458: train D loss: 0.6736, train F loss: -0.6428, acc: 0.9934\n",
      "epoch 1459: train D loss: 0.6766, train F loss: -0.6474, acc: 0.9930\n",
      "epoch 1460: train D loss: 0.6794, train F loss: -0.6444, acc: 0.9924\n",
      "epoch 1461: train D loss: 0.6762, train F loss: -0.6462, acc: 0.9920\n",
      "epoch 1462: train D loss: 0.6801, train F loss: -0.6507, acc: 0.9928\n",
      "epoch 1463: train D loss: 0.6742, train F loss: -0.6363, acc: 0.9892\n",
      "epoch 1464: train D loss: 0.6722, train F loss: -0.6379, acc: 0.9908\n",
      "epoch 1465: train D loss: 0.6734, train F loss: -0.6354, acc: 0.9912\n",
      "epoch 1466: train D loss: 0.6711, train F loss: -0.6316, acc: 0.9898\n",
      "epoch 1467: train D loss: 0.6728, train F loss: -0.6373, acc: 0.9912\n",
      "epoch 1468: train D loss: 0.6733, train F loss: -0.6304, acc: 0.9900\n",
      "epoch 1469: train D loss: 0.6758, train F loss: -0.6373, acc: 0.9900\n",
      "epoch 1470: train D loss: 0.6594, train F loss: -0.3435, acc: 0.9764\n",
      "epoch 1471: train D loss: 0.6112, train F loss: -0.5095, acc: 0.9750\n",
      "epoch 1472: train D loss: 0.6495, train F loss: -0.5874, acc: 0.9852\n",
      "epoch 1473: train D loss: 0.6570, train F loss: -0.6137, acc: 0.9896\n",
      "epoch 1474: train D loss: 0.6622, train F loss: -0.6162, acc: 0.9898\n",
      "epoch 1475: train D loss: 0.6585, train F loss: -0.6270, acc: 0.9932\n",
      "epoch 1476: train D loss: 0.6599, train F loss: -0.6098, acc: 0.9910\n",
      "epoch 1477: train D loss: 0.6630, train F loss: -0.6289, acc: 0.9942\n",
      "epoch 1478: train D loss: 0.6664, train F loss: -0.6347, acc: 0.9920\n",
      "epoch 1479: train D loss: 0.6645, train F loss: -0.6357, acc: 0.9944\n",
      "epoch 1480: train D loss: 0.6658, train F loss: -0.6341, acc: 0.9926\n",
      "epoch 1481: train D loss: 0.6632, train F loss: -0.6263, acc: 0.9924\n",
      "epoch 1482: train D loss: 0.6679, train F loss: -0.6409, acc: 0.9930\n",
      "epoch 1483: train D loss: 0.6635, train F loss: -0.6304, acc: 0.9926\n",
      "epoch 1484: train D loss: 0.6681, train F loss: -0.6405, acc: 0.9942\n",
      "epoch 1485: train D loss: 0.6765, train F loss: -0.6460, acc: 0.9934\n",
      "epoch 1486: train D loss: 0.6734, train F loss: -0.6464, acc: 0.9934\n",
      "epoch 1487: train D loss: 0.6653, train F loss: -0.6334, acc: 0.9924\n",
      "epoch 1488: train D loss: 0.6711, train F loss: -0.6372, acc: 0.9920\n",
      "epoch 1489: train D loss: 0.6735, train F loss: -0.6474, acc: 0.9938\n",
      "epoch 1490: train D loss: 0.6724, train F loss: -0.6383, acc: 0.9916\n",
      "epoch 1491: train D loss: 0.6724, train F loss: -0.6357, acc: 0.9914\n",
      "epoch 1492: train D loss: 0.6696, train F loss: -0.6364, acc: 0.9924\n",
      "epoch 1493: train D loss: 0.6690, train F loss: -0.6378, acc: 0.9918\n",
      "epoch 1494: train D loss: 0.6770, train F loss: -0.6271, acc: 0.9868\n",
      "epoch 1495: train D loss: 0.6383, train F loss: -0.4857, acc: 0.9782\n",
      "epoch 1496: train D loss: 0.6590, train F loss: -0.6151, acc: 0.9894\n",
      "epoch 1497: train D loss: 0.6627, train F loss: -0.6178, acc: 0.9900\n",
      "epoch 1498: train D loss: 0.6725, train F loss: -0.6373, acc: 0.9914\n",
      "epoch 1499: train D loss: 0.6591, train F loss: -0.5824, acc: 0.9882\n",
      "epoch 1500: train D loss: 0.6649, train F loss: -0.6254, acc: 0.9914\n",
      "epoch 1501: train D loss: 0.6653, train F loss: -0.6367, acc: 0.9928\n",
      "epoch 1502: train D loss: 0.6682, train F loss: -0.6297, acc: 0.9908\n",
      "epoch 1503: train D loss: 0.6713, train F loss: -0.6388, acc: 0.9922\n",
      "epoch 1504: train D loss: 0.6653, train F loss: -0.6350, acc: 0.9926\n",
      "epoch 1505: train D loss: 0.6712, train F loss: -0.6350, acc: 0.9914\n",
      "epoch 1506: train D loss: 0.6743, train F loss: -0.6429, acc: 0.9934\n",
      "epoch 1507: train D loss: 0.6721, train F loss: -0.6442, acc: 0.9930\n",
      "epoch 1508: train D loss: 0.6705, train F loss: -0.6413, acc: 0.9926\n",
      "epoch 1509: train D loss: 0.6691, train F loss: -0.6400, acc: 0.9918\n",
      "epoch 1510: train D loss: 0.6702, train F loss: -0.6386, acc: 0.9924\n",
      "epoch 1511: train D loss: 0.6718, train F loss: -0.6269, acc: 0.9896\n",
      "epoch 1512: train D loss: 0.6762, train F loss: -0.6392, acc: 0.9902\n",
      "epoch 1513: train D loss: 0.6717, train F loss: -0.6438, acc: 0.9936\n",
      "epoch 1514: train D loss: 0.6740, train F loss: -0.6428, acc: 0.9914\n",
      "epoch 1515: train D loss: 0.6717, train F loss: -0.6415, acc: 0.9924\n",
      "epoch 1516: train D loss: 0.6704, train F loss: -0.6398, acc: 0.9934\n",
      "epoch 1517: train D loss: 0.6745, train F loss: -0.6478, acc: 0.9942\n",
      "epoch 1518: train D loss: 0.6731, train F loss: -0.6443, acc: 0.9930\n",
      "epoch 1519: train D loss: 0.6791, train F loss: -0.6527, acc: 0.9938\n",
      "epoch 1520: train D loss: 0.6778, train F loss: -0.6500, acc: 0.9926\n",
      "epoch 1521: train D loss: 0.6723, train F loss: -0.6371, acc: 0.9906\n",
      "epoch 1522: train D loss: 0.6745, train F loss: -0.6329, acc: 0.9906\n",
      "epoch 1523: train D loss: 0.6767, train F loss: -0.6262, acc: 0.9858\n",
      "epoch 1524: train D loss: 0.6779, train F loss: -0.6518, acc: 0.9930\n",
      "epoch 1525: train D loss: 0.6763, train F loss: -0.6444, acc: 0.9908\n",
      "epoch 1526: train D loss: 0.6699, train F loss: -0.6357, acc: 0.9932\n",
      "epoch 1527: train D loss: 0.6725, train F loss: -0.6408, acc: 0.9916\n",
      "epoch 1528: train D loss: 0.6709, train F loss: -0.6433, acc: 0.9928\n",
      "epoch 1529: train D loss: 0.6732, train F loss: -0.6388, acc: 0.9920\n",
      "epoch 1530: train D loss: 0.6746, train F loss: -0.6352, acc: 0.9906\n",
      "epoch 1531: train D loss: 0.6750, train F loss: -0.6493, acc: 0.9942\n",
      "epoch 1532: train D loss: 0.6740, train F loss: -0.6489, acc: 0.9932\n",
      "epoch 1533: train D loss: 0.6695, train F loss: -0.6320, acc: 0.9912\n",
      "epoch 1534: train D loss: 0.6733, train F loss: -0.6413, acc: 0.9916\n",
      "epoch 1535: train D loss: 0.6717, train F loss: -0.6372, acc: 0.9916\n",
      "epoch 1536: train D loss: 0.6744, train F loss: -0.6353, acc: 0.9918\n",
      "epoch 1537: train D loss: 0.6794, train F loss: -0.6495, acc: 0.9928\n",
      "epoch 1538: train D loss: 0.6780, train F loss: -0.6435, acc: 0.9904\n",
      "epoch 1539: train D loss: 0.6780, train F loss: -0.6305, acc: 0.9878\n",
      "epoch 1540: train D loss: 0.6726, train F loss: -0.6480, acc: 0.9938\n",
      "epoch 1541: train D loss: 0.6726, train F loss: -0.6454, acc: 0.9942\n",
      "epoch 1542: train D loss: 0.6775, train F loss: -0.6458, acc: 0.9916\n",
      "epoch 1543: train D loss: 0.6743, train F loss: -0.6401, acc: 0.9914\n",
      "epoch 1544: train D loss: 0.6789, train F loss: -0.6497, acc: 0.9924\n",
      "epoch 1545: train D loss: 0.6744, train F loss: -0.6489, acc: 0.9926\n",
      "epoch 1546: train D loss: 0.6756, train F loss: -0.6461, acc: 0.9932\n",
      "epoch 1547: train D loss: 0.6742, train F loss: -0.6363, acc: 0.9922\n",
      "epoch 1548: train D loss: 0.6774, train F loss: -0.6375, acc: 0.9894\n",
      "epoch 1549: train D loss: 0.6764, train F loss: -0.6470, acc: 0.9928\n",
      "epoch 1550: train D loss: 0.6733, train F loss: -0.6415, acc: 0.9928\n",
      "epoch 1551: train D loss: 0.6752, train F loss: -0.6409, acc: 0.9908\n",
      "epoch 1552: train D loss: 0.6792, train F loss: -0.6417, acc: 0.9924\n",
      "epoch 1553: train D loss: 0.6729, train F loss: -0.6471, acc: 0.9938\n",
      "epoch 1554: train D loss: 0.6779, train F loss: -0.6534, acc: 0.9940\n",
      "epoch 1555: train D loss: 0.6799, train F loss: -0.6548, acc: 0.9936\n",
      "epoch 1556: train D loss: 0.6786, train F loss: -0.6476, acc: 0.9912\n",
      "epoch 1557: train D loss: 0.6759, train F loss: -0.6369, acc: 0.9892\n",
      "epoch 1558: train D loss: 0.6766, train F loss: -0.6423, acc: 0.9920\n",
      "epoch 1559: train D loss: 0.6760, train F loss: -0.6484, acc: 0.9928\n",
      "epoch 1560: train D loss: 0.6800, train F loss: -0.6506, acc: 0.9924\n",
      "epoch 1561: train D loss: 0.6756, train F loss: -0.6441, acc: 0.9918\n",
      "epoch 1562: train D loss: 0.6777, train F loss: -0.6536, acc: 0.9934\n",
      "epoch 1563: train D loss: 0.6793, train F loss: -0.6528, acc: 0.9928\n",
      "epoch 1564: train D loss: 0.6752, train F loss: -0.6451, acc: 0.9916\n",
      "epoch 1565: train D loss: 0.6811, train F loss: -0.6233, acc: 0.9884\n",
      "epoch 1566: train D loss: 0.6732, train F loss: -0.6279, acc: 0.9884\n",
      "epoch 1567: train D loss: 0.6755, train F loss: -0.6505, acc: 0.9942\n",
      "epoch 1568: train D loss: 0.6767, train F loss: -0.6508, acc: 0.9926\n",
      "epoch 1569: train D loss: 0.6789, train F loss: -0.6401, acc: 0.9896\n",
      "epoch 1570: train D loss: 0.6776, train F loss: -0.6507, acc: 0.9926\n",
      "epoch 1571: train D loss: 0.6735, train F loss: -0.6495, acc: 0.9938\n",
      "epoch 1572: train D loss: 0.6721, train F loss: -0.6376, acc: 0.9914\n",
      "epoch 1573: train D loss: 0.6730, train F loss: -0.6426, acc: 0.9928\n",
      "epoch 1574: train D loss: 0.6769, train F loss: -0.6441, acc: 0.9914\n",
      "epoch 1575: train D loss: 0.6750, train F loss: -0.6325, acc: 0.9914\n",
      "epoch 1576: train D loss: 0.6707, train F loss: -0.6390, acc: 0.9922\n",
      "epoch 1577: train D loss: 0.6743, train F loss: -0.6426, acc: 0.9904\n",
      "epoch 1578: train D loss: 0.6748, train F loss: -0.6334, acc: 0.9904\n",
      "epoch 1579: train D loss: 0.6782, train F loss: -0.6505, acc: 0.9932\n",
      "epoch 1580: train D loss: 0.6714, train F loss: -0.6410, acc: 0.9924\n",
      "epoch 1581: train D loss: 0.6733, train F loss: -0.6475, acc: 0.9946\n",
      "epoch 1582: train D loss: 0.6776, train F loss: -0.6501, acc: 0.9934\n",
      "epoch 1583: train D loss: 0.6766, train F loss: -0.6474, acc: 0.9936\n",
      "epoch 1584: train D loss: 0.6779, train F loss: -0.6424, acc: 0.9906\n",
      "epoch 1585: train D loss: 0.6752, train F loss: -0.6430, acc: 0.9916\n",
      "epoch 1586: train D loss: 0.6781, train F loss: -0.6438, acc: 0.9920\n",
      "epoch 1587: train D loss: 0.6777, train F loss: -0.6466, acc: 0.9920\n",
      "epoch 1588: train D loss: 0.6759, train F loss: -0.6417, acc: 0.9926\n",
      "epoch 1589: train D loss: 0.6736, train F loss: -0.6233, acc: 0.9886\n",
      "epoch 1590: train D loss: 0.6762, train F loss: -0.6446, acc: 0.9910\n",
      "epoch 1591: train D loss: 0.6798, train F loss: -0.6501, acc: 0.9932\n",
      "epoch 1592: train D loss: 0.6760, train F loss: -0.6421, acc: 0.9918\n",
      "epoch 1593: train D loss: 0.6762, train F loss: -0.6491, acc: 0.9928\n",
      "epoch 1594: train D loss: 0.6741, train F loss: -0.6412, acc: 0.9910\n",
      "epoch 1595: train D loss: 0.6801, train F loss: -0.6467, acc: 0.9930\n",
      "epoch 1596: train D loss: 0.6742, train F loss: -0.6480, acc: 0.9926\n",
      "epoch 1597: train D loss: 0.6774, train F loss: -0.6474, acc: 0.9916\n",
      "epoch 1598: train D loss: 0.6742, train F loss: -0.6460, acc: 0.9928\n",
      "epoch 1599: train D loss: 0.6783, train F loss: -0.6479, acc: 0.9916\n",
      "epoch 1600: train D loss: 0.6775, train F loss: -0.6508, acc: 0.9932\n",
      "epoch 1601: train D loss: 0.6756, train F loss: -0.6352, acc: 0.9890\n",
      "epoch 1602: train D loss: 0.6767, train F loss: -0.6441, acc: 0.9910\n",
      "epoch 1603: train D loss: 0.6759, train F loss: -0.6498, acc: 0.9940\n",
      "epoch 1604: train D loss: 0.6799, train F loss: -0.6528, acc: 0.9928\n",
      "epoch 1605: train D loss: 0.6799, train F loss: -0.6500, acc: 0.9922\n",
      "epoch 1606: train D loss: 0.6733, train F loss: -0.6448, acc: 0.9930\n",
      "epoch 1607: train D loss: 0.6753, train F loss: -0.6367, acc: 0.9906\n",
      "epoch 1608: train D loss: 0.6813, train F loss: -0.6464, acc: 0.9910\n",
      "epoch 1609: train D loss: 0.6799, train F loss: -0.6505, acc: 0.9906\n",
      "epoch 1610: train D loss: 0.6769, train F loss: -0.6482, acc: 0.9918\n",
      "epoch 1611: train D loss: 0.6786, train F loss: -0.6494, acc: 0.9922\n",
      "epoch 1612: train D loss: 0.6783, train F loss: -0.6541, acc: 0.9932\n",
      "epoch 1613: train D loss: 0.6772, train F loss: -0.6494, acc: 0.9914\n",
      "epoch 1614: train D loss: 0.6787, train F loss: -0.6471, acc: 0.9926\n",
      "epoch 1615: train D loss: 0.6742, train F loss: -0.6333, acc: 0.9902\n",
      "epoch 1616: train D loss: 0.6808, train F loss: -0.6370, acc: 0.9904\n",
      "epoch 1617: train D loss: 0.6722, train F loss: -0.6335, acc: 0.9910\n",
      "epoch 1618: train D loss: 0.6759, train F loss: -0.6377, acc: 0.9892\n",
      "epoch 1619: train D loss: 0.6786, train F loss: -0.6479, acc: 0.9922\n",
      "epoch 1620: train D loss: 0.6772, train F loss: -0.6386, acc: 0.9910\n",
      "epoch 1621: train D loss: 0.6724, train F loss: -0.6408, acc: 0.9928\n",
      "epoch 1622: train D loss: 0.6723, train F loss: -0.6420, acc: 0.9926\n",
      "epoch 1623: train D loss: 0.6779, train F loss: -0.6449, acc: 0.9926\n",
      "epoch 1624: train D loss: 0.6738, train F loss: -0.6475, acc: 0.9932\n",
      "epoch 1625: train D loss: 0.6758, train F loss: -0.6398, acc: 0.9914\n",
      "epoch 1626: train D loss: 0.6768, train F loss: -0.6483, acc: 0.9926\n",
      "epoch 1627: train D loss: 0.6757, train F loss: -0.6360, acc: 0.9916\n",
      "epoch 1628: train D loss: 0.6724, train F loss: -0.6412, acc: 0.9918\n",
      "epoch 1629: train D loss: 0.6789, train F loss: -0.6455, acc: 0.9902\n",
      "epoch 1630: train D loss: 0.6789, train F loss: -0.6438, acc: 0.9920\n",
      "epoch 1631: train D loss: 0.6751, train F loss: -0.6288, acc: 0.9890\n",
      "epoch 1632: train D loss: 0.6736, train F loss: -0.6409, acc: 0.9940\n",
      "epoch 1633: train D loss: 0.6716, train F loss: -0.6399, acc: 0.9918\n",
      "epoch 1634: train D loss: 0.6735, train F loss: -0.6315, acc: 0.9890\n",
      "epoch 1635: train D loss: 0.6732, train F loss: -0.6381, acc: 0.9926\n",
      "epoch 1636: train D loss: 0.6718, train F loss: -0.6412, acc: 0.9924\n",
      "epoch 1637: train D loss: 0.6735, train F loss: -0.6442, acc: 0.9924\n",
      "epoch 1638: train D loss: 0.6755, train F loss: -0.6401, acc: 0.9908\n",
      "epoch 1639: train D loss: 0.6784, train F loss: -0.6391, acc: 0.9910\n",
      "epoch 1640: train D loss: 0.6775, train F loss: -0.6442, acc: 0.9906\n",
      "epoch 1641: train D loss: 0.6748, train F loss: -0.6469, acc: 0.9920\n",
      "epoch 1642: train D loss: 0.6764, train F loss: -0.6496, acc: 0.9928\n",
      "epoch 1643: train D loss: 0.6792, train F loss: -0.6454, acc: 0.9920\n",
      "epoch 1644: train D loss: 0.6715, train F loss: -0.6270, acc: 0.9910\n",
      "epoch 1645: train D loss: 0.6744, train F loss: -0.6287, acc: 0.9918\n",
      "epoch 1646: train D loss: 0.6753, train F loss: -0.6473, acc: 0.9936\n",
      "epoch 1647: train D loss: 0.6799, train F loss: -0.6530, acc: 0.9930\n",
      "epoch 1648: train D loss: 0.6756, train F loss: -0.6461, acc: 0.9922\n",
      "epoch 1649: train D loss: 0.6745, train F loss: -0.6417, acc: 0.9916\n",
      "epoch 1650: train D loss: 0.6773, train F loss: -0.6511, acc: 0.9924\n",
      "epoch 1651: train D loss: 0.6765, train F loss: -0.6373, acc: 0.9890\n",
      "epoch 1652: train D loss: 0.6753, train F loss: -0.6458, acc: 0.9922\n",
      "epoch 1653: train D loss: 0.6738, train F loss: -0.6445, acc: 0.9932\n",
      "epoch 1654: train D loss: 0.6792, train F loss: -0.6413, acc: 0.9910\n",
      "epoch 1655: train D loss: 0.6758, train F loss: -0.6549, acc: 0.9950\n",
      "epoch 1656: train D loss: 0.6724, train F loss: -0.6467, acc: 0.9934\n",
      "epoch 1657: train D loss: 0.6741, train F loss: -0.6432, acc: 0.9912\n",
      "epoch 1658: train D loss: 0.6787, train F loss: -0.6560, acc: 0.9942\n",
      "epoch 1659: train D loss: 0.6799, train F loss: -0.6519, acc: 0.9938\n",
      "epoch 1660: train D loss: 0.6780, train F loss: -0.6496, acc: 0.9922\n",
      "epoch 1661: train D loss: 0.6807, train F loss: -0.6548, acc: 0.9934\n",
      "epoch 1662: train D loss: 0.6783, train F loss: -0.6398, acc: 0.9890\n",
      "epoch 1663: train D loss: 0.6784, train F loss: -0.6425, acc: 0.9916\n",
      "epoch 1664: train D loss: 0.6742, train F loss: -0.6460, acc: 0.9916\n",
      "epoch 1665: train D loss: 0.6773, train F loss: -0.6494, acc: 0.9926\n",
      "epoch 1666: train D loss: 0.6769, train F loss: -0.6527, acc: 0.9936\n",
      "epoch 1667: train D loss: 0.6755, train F loss: -0.6495, acc: 0.9934\n",
      "epoch 1668: train D loss: 0.6786, train F loss: -0.6450, acc: 0.9928\n",
      "epoch 1669: train D loss: 0.6804, train F loss: -0.6403, acc: 0.9900\n",
      "epoch 1670: train D loss: 0.6767, train F loss: -0.6488, acc: 0.9914\n",
      "epoch 1671: train D loss: 0.6741, train F loss: -0.6479, acc: 0.9932\n",
      "epoch 1672: train D loss: 0.6722, train F loss: -0.6415, acc: 0.9916\n",
      "epoch 1673: train D loss: 0.6772, train F loss: -0.6486, acc: 0.9932\n",
      "epoch 1674: train D loss: 0.6744, train F loss: -0.6472, acc: 0.9936\n",
      "epoch 1675: train D loss: 0.6805, train F loss: -0.6424, acc: 0.9892\n",
      "epoch 1676: train D loss: 0.6770, train F loss: -0.6502, acc: 0.9934\n",
      "epoch 1677: train D loss: 0.6766, train F loss: -0.6489, acc: 0.9932\n",
      "epoch 1678: train D loss: 0.6768, train F loss: -0.6477, acc: 0.9926\n",
      "epoch 1679: train D loss: 0.6760, train F loss: -0.6467, acc: 0.9918\n",
      "epoch 1680: train D loss: 0.6774, train F loss: -0.6542, acc: 0.9940\n",
      "epoch 1681: train D loss: 0.6761, train F loss: -0.6490, acc: 0.9932\n",
      "epoch 1682: train D loss: 0.6748, train F loss: -0.6457, acc: 0.9918\n",
      "epoch 1683: train D loss: 0.6774, train F loss: -0.6487, acc: 0.9922\n",
      "epoch 1684: train D loss: 0.6786, train F loss: -0.6504, acc: 0.9932\n",
      "epoch 1685: train D loss: 0.6776, train F loss: -0.6513, acc: 0.9934\n",
      "epoch 1686: train D loss: 0.6758, train F loss: -0.6337, acc: 0.9894\n",
      "epoch 1687: train D loss: 0.6752, train F loss: -0.6442, acc: 0.9920\n",
      "epoch 1688: train D loss: 0.6768, train F loss: -0.6483, acc: 0.9922\n",
      "epoch 1689: train D loss: 0.6767, train F loss: -0.6453, acc: 0.9928\n",
      "epoch 1690: train D loss: 0.6758, train F loss: -0.6391, acc: 0.9914\n",
      "epoch 1691: train D loss: 0.6775, train F loss: -0.6404, acc: 0.9920\n",
      "epoch 1692: train D loss: 0.6775, train F loss: -0.6493, acc: 0.9922\n",
      "epoch 1693: train D loss: 0.6726, train F loss: -0.6470, acc: 0.9942\n",
      "epoch 1694: train D loss: 0.6766, train F loss: -0.6407, acc: 0.9916\n",
      "epoch 1695: train D loss: 0.6756, train F loss: -0.6386, acc: 0.9914\n",
      "epoch 1696: train D loss: 0.6789, train F loss: -0.6410, acc: 0.9904\n",
      "epoch 1697: train D loss: 0.6700, train F loss: -0.6437, acc: 0.9930\n",
      "epoch 1698: train D loss: 0.6747, train F loss: -0.6440, acc: 0.9916\n",
      "epoch 1699: train D loss: 0.6730, train F loss: -0.6464, acc: 0.9944\n",
      "epoch 1700: train D loss: 0.6798, train F loss: -0.6548, acc: 0.9954\n",
      "epoch 1701: train D loss: 0.6786, train F loss: -0.6420, acc: 0.9906\n",
      "epoch 1702: train D loss: 0.6797, train F loss: -0.6451, acc: 0.9906\n",
      "epoch 1703: train D loss: 0.6789, train F loss: -0.6489, acc: 0.9914\n",
      "epoch 1704: train D loss: 0.6755, train F loss: -0.6483, acc: 0.9932\n",
      "epoch 1705: train D loss: 0.6752, train F loss: -0.6471, acc: 0.9928\n",
      "epoch 1706: train D loss: 0.6758, train F loss: -0.6412, acc: 0.9912\n",
      "epoch 1707: train D loss: 0.6784, train F loss: -0.6404, acc: 0.9886\n",
      "epoch 1708: train D loss: 0.6759, train F loss: -0.6461, acc: 0.9926\n",
      "epoch 1709: train D loss: 0.6777, train F loss: -0.6476, acc: 0.9918\n",
      "epoch 1710: train D loss: 0.6812, train F loss: -0.6494, acc: 0.9920\n",
      "epoch 1711: train D loss: 0.6756, train F loss: -0.6516, acc: 0.9940\n",
      "epoch 1712: train D loss: 0.6764, train F loss: -0.6417, acc: 0.9904\n",
      "epoch 1713: train D loss: 0.6733, train F loss: -0.6327, acc: 0.9918\n",
      "epoch 1714: train D loss: 0.6789, train F loss: -0.6469, acc: 0.9916\n",
      "epoch 1715: train D loss: 0.6766, train F loss: -0.6451, acc: 0.9924\n",
      "epoch 1716: train D loss: 0.6762, train F loss: -0.6302, acc: 0.9892\n",
      "epoch 1717: train D loss: 0.6780, train F loss: -0.6373, acc: 0.9906\n",
      "epoch 1718: train D loss: 0.6740, train F loss: -0.6372, acc: 0.9930\n",
      "epoch 1719: train D loss: 0.6743, train F loss: -0.6353, acc: 0.9894\n",
      "epoch 1720: train D loss: 0.6707, train F loss: -0.6437, acc: 0.9940\n",
      "epoch 1721: train D loss: 0.6772, train F loss: -0.6416, acc: 0.9930\n",
      "epoch 1722: train D loss: 0.6737, train F loss: -0.6397, acc: 0.9932\n",
      "epoch 1723: train D loss: 0.6732, train F loss: -0.6432, acc: 0.9918\n",
      "epoch 1724: train D loss: 0.6749, train F loss: -0.6490, acc: 0.9938\n",
      "epoch 1725: train D loss: 0.6777, train F loss: -0.6542, acc: 0.9936\n",
      "epoch 1726: train D loss: 0.6769, train F loss: -0.6529, acc: 0.9934\n",
      "epoch 1727: train D loss: 0.6780, train F loss: -0.6464, acc: 0.9922\n",
      "epoch 1728: train D loss: 0.6778, train F loss: -0.6370, acc: 0.9894\n",
      "epoch 1729: train D loss: 0.6737, train F loss: -0.5713, acc: 0.9862\n",
      "epoch 1730: train D loss: 0.6785, train F loss: -0.6507, acc: 0.9926\n",
      "epoch 1731: train D loss: 0.6723, train F loss: -0.6397, acc: 0.9916\n",
      "epoch 1732: train D loss: 0.6714, train F loss: -0.6432, acc: 0.9928\n",
      "epoch 1733: train D loss: 0.6741, train F loss: -0.6449, acc: 0.9922\n",
      "epoch 1734: train D loss: 0.6780, train F loss: -0.6573, acc: 0.9954\n",
      "epoch 1735: train D loss: 0.6780, train F loss: -0.6492, acc: 0.9938\n",
      "epoch 1736: train D loss: 0.6780, train F loss: -0.6501, acc: 0.9928\n",
      "epoch 1737: train D loss: 0.6779, train F loss: -0.6378, acc: 0.9908\n",
      "epoch 1738: train D loss: 0.6752, train F loss: -0.6487, acc: 0.9928\n",
      "epoch 1739: train D loss: 0.6768, train F loss: -0.6506, acc: 0.9934\n",
      "epoch 1740: train D loss: 0.6790, train F loss: -0.6520, acc: 0.9922\n",
      "epoch 1741: train D loss: 0.6764, train F loss: -0.6461, acc: 0.9912\n",
      "epoch 1742: train D loss: 0.6790, train F loss: -0.6584, acc: 0.9950\n",
      "epoch 1743: train D loss: 0.6770, train F loss: -0.6418, acc: 0.9922\n",
      "epoch 1744: train D loss: 0.6779, train F loss: -0.6491, acc: 0.9936\n",
      "epoch 1745: train D loss: 0.6753, train F loss: -0.6481, acc: 0.9934\n",
      "epoch 1746: train D loss: 0.6798, train F loss: -0.6530, acc: 0.9926\n",
      "epoch 1747: train D loss: 0.6802, train F loss: -0.6441, acc: 0.9908\n",
      "epoch 1748: train D loss: 0.6778, train F loss: -0.6476, acc: 0.9928\n",
      "epoch 1749: train D loss: 0.6812, train F loss: -0.6594, acc: 0.9950\n",
      "epoch 1750: train D loss: 0.6778, train F loss: -0.6468, acc: 0.9938\n",
      "epoch 1751: train D loss: 0.6763, train F loss: -0.6320, acc: 0.9888\n",
      "epoch 1752: train D loss: 0.6777, train F loss: -0.6453, acc: 0.9924\n",
      "epoch 1753: train D loss: 0.6787, train F loss: -0.6456, acc: 0.9914\n",
      "epoch 1754: train D loss: 0.6741, train F loss: -0.6447, acc: 0.9926\n",
      "epoch 1755: train D loss: 0.6758, train F loss: -0.6466, acc: 0.9918\n",
      "epoch 1756: train D loss: 0.6744, train F loss: -0.6258, acc: 0.9896\n",
      "epoch 1757: train D loss: 0.6709, train F loss: -0.6337, acc: 0.9898\n",
      "epoch 1758: train D loss: 0.6698, train F loss: -0.6206, acc: 0.9892\n",
      "epoch 1759: train D loss: 0.6685, train F loss: -0.6352, acc: 0.9924\n",
      "epoch 1760: train D loss: 0.6714, train F loss: -0.6453, acc: 0.9942\n",
      "epoch 1761: train D loss: 0.6748, train F loss: -0.6424, acc: 0.9924\n",
      "epoch 1762: train D loss: 0.6766, train F loss: -0.6471, acc: 0.9922\n",
      "epoch 1763: train D loss: 0.6725, train F loss: -0.6470, acc: 0.9942\n",
      "epoch 1764: train D loss: 0.6746, train F loss: -0.6557, acc: 0.9962\n",
      "epoch 1765: train D loss: 0.6727, train F loss: -0.6466, acc: 0.9936\n",
      "epoch 1766: train D loss: 0.6777, train F loss: -0.6552, acc: 0.9948\n",
      "epoch 1767: train D loss: 0.6769, train F loss: -0.6527, acc: 0.9948\n",
      "epoch 1768: train D loss: 0.6804, train F loss: -0.6390, acc: 0.9886\n",
      "epoch 1769: train D loss: 0.6725, train F loss: -0.6395, acc: 0.9900\n",
      "epoch 1770: train D loss: 0.6794, train F loss: -0.6488, acc: 0.9906\n",
      "epoch 1771: train D loss: 0.6770, train F loss: -0.6493, acc: 0.9930\n",
      "epoch 1772: train D loss: 0.6748, train F loss: -0.6512, acc: 0.9934\n",
      "epoch 1773: train D loss: 0.6801, train F loss: -0.6562, acc: 0.9936\n",
      "epoch 1774: train D loss: 0.6720, train F loss: -0.6376, acc: 0.9926\n",
      "epoch 1775: train D loss: 0.6773, train F loss: -0.6448, acc: 0.9908\n",
      "epoch 1776: train D loss: 0.6731, train F loss: -0.6442, acc: 0.9924\n",
      "epoch 1777: train D loss: 0.6756, train F loss: -0.6441, acc: 0.9922\n",
      "epoch 1778: train D loss: 0.6759, train F loss: -0.6503, acc: 0.9942\n",
      "epoch 1779: train D loss: 0.6774, train F loss: -0.6416, acc: 0.9916\n",
      "epoch 1780: train D loss: 0.6778, train F loss: -0.6490, acc: 0.9928\n",
      "epoch 1781: train D loss: 0.6711, train F loss: -0.6307, acc: 0.9922\n",
      "epoch 1782: train D loss: 0.6775, train F loss: -0.6488, acc: 0.9928\n",
      "epoch 1783: train D loss: 0.6777, train F loss: -0.6515, acc: 0.9922\n",
      "epoch 1784: train D loss: 0.6780, train F loss: -0.6448, acc: 0.9928\n",
      "epoch 1785: train D loss: 0.6714, train F loss: -0.6349, acc: 0.9912\n",
      "epoch 1786: train D loss: 0.6722, train F loss: -0.6351, acc: 0.9902\n",
      "epoch 1787: train D loss: 0.6750, train F loss: -0.6437, acc: 0.9932\n",
      "epoch 1788: train D loss: 0.6776, train F loss: -0.6508, acc: 0.9926\n",
      "epoch 1789: train D loss: 0.6783, train F loss: -0.6564, acc: 0.9946\n",
      "epoch 1790: train D loss: 0.6809, train F loss: -0.6453, acc: 0.9916\n",
      "epoch 1791: train D loss: 0.6756, train F loss: -0.6483, acc: 0.9942\n",
      "epoch 1792: train D loss: 0.6767, train F loss: -0.6501, acc: 0.9930\n",
      "epoch 1793: train D loss: 0.6758, train F loss: -0.6497, acc: 0.9938\n",
      "epoch 1794: train D loss: 0.6771, train F loss: -0.6480, acc: 0.9920\n",
      "epoch 1795: train D loss: 0.6755, train F loss: -0.6440, acc: 0.9934\n",
      "epoch 1796: train D loss: 0.6788, train F loss: -0.6502, acc: 0.9920\n",
      "epoch 1797: train D loss: 0.6774, train F loss: -0.6437, acc: 0.9930\n",
      "epoch 1798: train D loss: 0.6778, train F loss: -0.6524, acc: 0.9944\n",
      "epoch 1799: train D loss: 0.6792, train F loss: -0.6479, acc: 0.9926\n",
      "epoch 1800: train D loss: 0.6790, train F loss: -0.6496, acc: 0.9922\n",
      "epoch 1801: train D loss: 0.6789, train F loss: -0.6426, acc: 0.9898\n",
      "epoch 1802: train D loss: 0.6725, train F loss: -0.6388, acc: 0.9908\n",
      "epoch 1803: train D loss: 0.6824, train F loss: -0.6553, acc: 0.9926\n",
      "epoch 1804: train D loss: 0.6748, train F loss: -0.6441, acc: 0.9924\n",
      "epoch 1805: train D loss: 0.6780, train F loss: -0.6518, acc: 0.9938\n",
      "epoch 1806: train D loss: 0.6749, train F loss: -0.6520, acc: 0.9938\n",
      "epoch 1807: train D loss: 0.6767, train F loss: -0.6452, acc: 0.9910\n",
      "epoch 1808: train D loss: 0.6790, train F loss: -0.6448, acc: 0.9906\n",
      "epoch 1809: train D loss: 0.6802, train F loss: -0.6569, acc: 0.9934\n",
      "epoch 1810: train D loss: 0.6763, train F loss: -0.6493, acc: 0.9932\n",
      "epoch 1811: train D loss: 0.6797, train F loss: -0.6439, acc: 0.9912\n",
      "epoch 1812: train D loss: 0.6787, train F loss: -0.6449, acc: 0.9906\n",
      "epoch 1813: train D loss: 0.6804, train F loss: -0.6477, acc: 0.9918\n",
      "epoch 1814: train D loss: 0.6779, train F loss: -0.6494, acc: 0.9922\n",
      "epoch 1815: train D loss: 0.6738, train F loss: -0.6371, acc: 0.9914\n",
      "epoch 1816: train D loss: 0.6767, train F loss: -0.6437, acc: 0.9922\n",
      "epoch 1817: train D loss: 0.6801, train F loss: -0.6521, acc: 0.9920\n",
      "epoch 1818: train D loss: 0.6788, train F loss: -0.6442, acc: 0.9912\n",
      "epoch 1819: train D loss: 0.6824, train F loss: -0.6551, acc: 0.9930\n",
      "epoch 1820: train D loss: 0.6785, train F loss: -0.6482, acc: 0.9922\n",
      "epoch 1821: train D loss: 0.6709, train F loss: -0.6424, acc: 0.9916\n",
      "epoch 1822: train D loss: 0.6766, train F loss: -0.6444, acc: 0.9930\n",
      "epoch 1823: train D loss: 0.6736, train F loss: -0.6435, acc: 0.9926\n",
      "epoch 1824: train D loss: 0.6794, train F loss: -0.6507, acc: 0.9932\n",
      "epoch 1825: train D loss: 0.6804, train F loss: -0.6471, acc: 0.9906\n",
      "epoch 1826: train D loss: 0.6772, train F loss: -0.6531, acc: 0.9934\n",
      "epoch 1827: train D loss: 0.6774, train F loss: -0.6460, acc: 0.9914\n",
      "epoch 1828: train D loss: 0.6774, train F loss: -0.6461, acc: 0.9916\n",
      "epoch 1829: train D loss: 0.6761, train F loss: -0.6328, acc: 0.9896\n",
      "epoch 1830: train D loss: 0.6785, train F loss: -0.6528, acc: 0.9938\n",
      "epoch 1831: train D loss: 0.6720, train F loss: -0.6263, acc: 0.9868\n",
      "epoch 1832: train D loss: 0.6703, train F loss: -0.6189, acc: 0.9884\n",
      "epoch 1833: train D loss: 0.6728, train F loss: -0.6389, acc: 0.9912\n",
      "epoch 1834: train D loss: 0.6752, train F loss: -0.6520, acc: 0.9944\n",
      "epoch 1835: train D loss: 0.6703, train F loss: -0.6319, acc: 0.9920\n",
      "epoch 1836: train D loss: 0.6734, train F loss: -0.6460, acc: 0.9944\n",
      "epoch 1837: train D loss: 0.6755, train F loss: -0.6465, acc: 0.9924\n",
      "epoch 1838: train D loss: 0.6735, train F loss: -0.6474, acc: 0.9942\n",
      "epoch 1839: train D loss: 0.6746, train F loss: -0.6478, acc: 0.9934\n",
      "epoch 1840: train D loss: 0.6743, train F loss: -0.6487, acc: 0.9932\n",
      "epoch 1841: train D loss: 0.6741, train F loss: -0.6480, acc: 0.9948\n",
      "epoch 1842: train D loss: 0.6757, train F loss: -0.6492, acc: 0.9930\n",
      "epoch 1843: train D loss: 0.6722, train F loss: -0.6400, acc: 0.9920\n",
      "epoch 1844: train D loss: 0.6784, train F loss: -0.6495, acc: 0.9926\n",
      "epoch 1845: train D loss: 0.6772, train F loss: -0.6457, acc: 0.9930\n",
      "epoch 1846: train D loss: 0.6747, train F loss: -0.6474, acc: 0.9930\n",
      "epoch 1847: train D loss: 0.6786, train F loss: -0.6465, acc: 0.9930\n",
      "epoch 1848: train D loss: 0.6785, train F loss: -0.6511, acc: 0.9920\n",
      "epoch 1849: train D loss: 0.6791, train F loss: -0.6501, acc: 0.9926\n",
      "epoch 1850: train D loss: 0.6758, train F loss: -0.6462, acc: 0.9930\n",
      "epoch 1851: train D loss: 0.6787, train F loss: -0.6436, acc: 0.9922\n",
      "epoch 1852: train D loss: 0.6773, train F loss: -0.6470, acc: 0.9924\n",
      "epoch 1853: train D loss: 0.6749, train F loss: -0.6449, acc: 0.9908\n",
      "epoch 1854: train D loss: 0.6792, train F loss: -0.6509, acc: 0.9930\n",
      "epoch 1855: train D loss: 0.6778, train F loss: -0.6501, acc: 0.9920\n",
      "epoch 1856: train D loss: 0.6786, train F loss: -0.6531, acc: 0.9930\n",
      "epoch 1857: train D loss: 0.6764, train F loss: -0.6489, acc: 0.9930\n",
      "epoch 1858: train D loss: 0.6759, train F loss: -0.6468, acc: 0.9924\n",
      "epoch 1859: train D loss: 0.6772, train F loss: -0.6443, acc: 0.9912\n",
      "epoch 1860: train D loss: 0.6756, train F loss: -0.6349, acc: 0.9904\n",
      "epoch 1861: train D loss: 0.6756, train F loss: -0.6509, acc: 0.9932\n",
      "epoch 1862: train D loss: 0.6738, train F loss: -0.6480, acc: 0.9934\n",
      "epoch 1863: train D loss: 0.6772, train F loss: -0.6433, acc: 0.9930\n",
      "epoch 1864: train D loss: 0.6734, train F loss: -0.6485, acc: 0.9938\n",
      "epoch 1865: train D loss: 0.6746, train F loss: -0.6471, acc: 0.9918\n",
      "epoch 1866: train D loss: 0.6763, train F loss: -0.6496, acc: 0.9932\n",
      "epoch 1867: train D loss: 0.6769, train F loss: -0.6495, acc: 0.9932\n",
      "epoch 1868: train D loss: 0.6798, train F loss: -0.6417, acc: 0.9916\n",
      "epoch 1869: train D loss: 0.6782, train F loss: -0.6552, acc: 0.9940\n",
      "epoch 1870: train D loss: 0.6729, train F loss: -0.6467, acc: 0.9922\n",
      "epoch 1871: train D loss: 0.6755, train F loss: -0.6465, acc: 0.9934\n",
      "epoch 1872: train D loss: 0.6768, train F loss: -0.6431, acc: 0.9916\n",
      "epoch 1873: train D loss: 0.6776, train F loss: -0.6497, acc: 0.9924\n",
      "epoch 1874: train D loss: 0.6778, train F loss: -0.5711, acc: 0.9920\n",
      "epoch 1875: train D loss: 0.6287, train F loss: -0.5505, acc: 0.9786\n",
      "epoch 1876: train D loss: 0.6647, train F loss: -0.6319, acc: 0.9916\n",
      "epoch 1877: train D loss: 0.6725, train F loss: -0.6441, acc: 0.9924\n",
      "epoch 1878: train D loss: 0.6741, train F loss: -0.6473, acc: 0.9942\n",
      "epoch 1879: train D loss: 0.6725, train F loss: -0.6346, acc: 0.9918\n",
      "epoch 1880: train D loss: 0.6691, train F loss: -0.6366, acc: 0.9928\n",
      "epoch 1881: train D loss: 0.6668, train F loss: -0.6448, acc: 0.9950\n",
      "epoch 1882: train D loss: 0.6722, train F loss: -0.6416, acc: 0.9924\n",
      "epoch 1883: train D loss: 0.6713, train F loss: -0.6443, acc: 0.9938\n",
      "epoch 1884: train D loss: 0.6735, train F loss: -0.6472, acc: 0.9948\n",
      "epoch 1885: train D loss: 0.6727, train F loss: -0.6356, acc: 0.9918\n",
      "epoch 1886: train D loss: 0.6751, train F loss: -0.6521, acc: 0.9936\n",
      "epoch 1887: train D loss: 0.6729, train F loss: -0.6415, acc: 0.9924\n",
      "epoch 1888: train D loss: 0.6751, train F loss: -0.6485, acc: 0.9940\n",
      "epoch 1889: train D loss: 0.6795, train F loss: -0.6543, acc: 0.9938\n",
      "epoch 1890: train D loss: 0.6707, train F loss: -0.6396, acc: 0.9912\n",
      "epoch 1891: train D loss: 0.6718, train F loss: -0.6481, acc: 0.9942\n",
      "epoch 1892: train D loss: 0.6788, train F loss: -0.6492, acc: 0.9928\n",
      "epoch 1893: train D loss: 0.6765, train F loss: -0.6507, acc: 0.9928\n",
      "epoch 1894: train D loss: 0.6750, train F loss: -0.6501, acc: 0.9932\n",
      "epoch 1895: train D loss: 0.6744, train F loss: -0.6540, acc: 0.9954\n",
      "epoch 1896: train D loss: 0.6744, train F loss: -0.6473, acc: 0.9940\n",
      "epoch 1897: train D loss: 0.6783, train F loss: -0.6542, acc: 0.9940\n",
      "epoch 1898: train D loss: 0.6788, train F loss: -0.6541, acc: 0.9940\n",
      "epoch 1899: train D loss: 0.6789, train F loss: -0.6513, acc: 0.9936\n",
      "epoch 1900: train D loss: 0.6786, train F loss: -0.6208, acc: 0.9900\n",
      "epoch 1901: train D loss: 0.6771, train F loss: -0.6444, acc: 0.9916\n",
      "epoch 1902: train D loss: 0.6774, train F loss: -0.6499, acc: 0.9940\n",
      "epoch 1903: train D loss: 0.6785, train F loss: -0.6461, acc: 0.9904\n",
      "epoch 1904: train D loss: 0.6713, train F loss: -0.6411, acc: 0.9916\n",
      "epoch 1905: train D loss: 0.6763, train F loss: -0.6548, acc: 0.9950\n",
      "epoch 1906: train D loss: 0.6798, train F loss: -0.6487, acc: 0.9922\n",
      "epoch 1907: train D loss: 0.6793, train F loss: -0.6583, acc: 0.9942\n",
      "epoch 1908: train D loss: 0.6768, train F loss: -0.6508, acc: 0.9932\n",
      "epoch 1909: train D loss: 0.6775, train F loss: -0.6522, acc: 0.9932\n",
      "epoch 1910: train D loss: 0.6739, train F loss: -0.6461, acc: 0.9932\n",
      "epoch 1911: train D loss: 0.6753, train F loss: -0.6441, acc: 0.9924\n",
      "epoch 1912: train D loss: 0.6783, train F loss: -0.6416, acc: 0.9892\n",
      "epoch 1913: train D loss: 0.6795, train F loss: -0.6540, acc: 0.9932\n",
      "epoch 1914: train D loss: 0.6747, train F loss: -0.6407, acc: 0.9910\n",
      "epoch 1915: train D loss: 0.6759, train F loss: -0.6426, acc: 0.9908\n",
      "epoch 1916: train D loss: 0.6756, train F loss: -0.6478, acc: 0.9932\n",
      "epoch 1917: train D loss: 0.6775, train F loss: -0.6399, acc: 0.9908\n",
      "epoch 1918: train D loss: 0.6748, train F loss: -0.6519, acc: 0.9948\n",
      "epoch 1919: train D loss: 0.6744, train F loss: -0.6448, acc: 0.9914\n",
      "epoch 1920: train D loss: 0.6724, train F loss: -0.6428, acc: 0.9918\n",
      "epoch 1921: train D loss: 0.6749, train F loss: -0.6362, acc: 0.9906\n",
      "epoch 1922: train D loss: 0.6773, train F loss: -0.6495, acc: 0.9916\n",
      "epoch 1923: train D loss: 0.6794, train F loss: -0.6496, acc: 0.9922\n",
      "epoch 1924: train D loss: 0.6707, train F loss: -0.6375, acc: 0.9914\n",
      "epoch 1925: train D loss: 0.6745, train F loss: -0.6464, acc: 0.9936\n",
      "epoch 1926: train D loss: 0.6728, train F loss: -0.6435, acc: 0.9926\n",
      "epoch 1927: train D loss: 0.6750, train F loss: -0.6404, acc: 0.9926\n",
      "epoch 1928: train D loss: 0.6771, train F loss: -0.6495, acc: 0.9938\n",
      "epoch 1929: train D loss: 0.6758, train F loss: -0.6501, acc: 0.9938\n",
      "epoch 1930: train D loss: 0.6792, train F loss: -0.6437, acc: 0.9912\n",
      "epoch 1931: train D loss: 0.6735, train F loss: -0.6389, acc: 0.9924\n",
      "epoch 1932: train D loss: 0.6722, train F loss: -0.6444, acc: 0.9932\n",
      "epoch 1933: train D loss: 0.6793, train F loss: -0.6401, acc: 0.9910\n",
      "epoch 1934: train D loss: 0.6765, train F loss: -0.6447, acc: 0.9926\n",
      "epoch 1935: train D loss: 0.6753, train F loss: -0.6480, acc: 0.9934\n",
      "epoch 1936: train D loss: 0.6750, train F loss: -0.6475, acc: 0.9924\n",
      "epoch 1937: train D loss: 0.6758, train F loss: -0.6479, acc: 0.9934\n",
      "epoch 1938: train D loss: 0.6717, train F loss: -0.6462, acc: 0.9934\n",
      "epoch 1939: train D loss: 0.6820, train F loss: -0.6476, acc: 0.9892\n",
      "epoch 1940: train D loss: 0.6771, train F loss: -0.6524, acc: 0.9936\n",
      "epoch 1941: train D loss: 0.6804, train F loss: -0.6545, acc: 0.9944\n",
      "epoch 1942: train D loss: 0.6759, train F loss: -0.6535, acc: 0.9942\n",
      "epoch 1943: train D loss: 0.6759, train F loss: -0.6465, acc: 0.9920\n",
      "epoch 1944: train D loss: 0.6811, train F loss: -0.6445, acc: 0.9920\n",
      "epoch 1945: train D loss: 0.6770, train F loss: -0.6465, acc: 0.9934\n",
      "epoch 1946: train D loss: 0.6791, train F loss: -0.6445, acc: 0.9902\n",
      "epoch 1947: train D loss: 0.6771, train F loss: -0.6413, acc: 0.9920\n",
      "epoch 1948: train D loss: 0.6756, train F loss: -0.6500, acc: 0.9942\n",
      "epoch 1949: train D loss: 0.6740, train F loss: -0.6457, acc: 0.9930\n",
      "epoch 1950: train D loss: 0.6755, train F loss: -0.6515, acc: 0.9926\n",
      "epoch 1951: train D loss: 0.6775, train F loss: -0.6507, acc: 0.9922\n",
      "epoch 1952: train D loss: 0.6787, train F loss: -0.6520, acc: 0.9920\n",
      "epoch 1953: train D loss: 0.6748, train F loss: -0.6490, acc: 0.9926\n",
      "epoch 1954: train D loss: 0.6765, train F loss: -0.6393, acc: 0.9906\n",
      "epoch 1955: train D loss: 0.6745, train F loss: -0.6388, acc: 0.9910\n",
      "epoch 1956: train D loss: 0.6765, train F loss: -0.6391, acc: 0.9902\n",
      "epoch 1957: train D loss: 0.6738, train F loss: -0.6448, acc: 0.9918\n",
      "epoch 1958: train D loss: 0.6736, train F loss: -0.6517, acc: 0.9946\n",
      "epoch 1959: train D loss: 0.6775, train F loss: -0.6530, acc: 0.9938\n",
      "epoch 1960: train D loss: 0.6751, train F loss: -0.6445, acc: 0.9932\n",
      "epoch 1961: train D loss: 0.6784, train F loss: -0.6443, acc: 0.9908\n",
      "epoch 1962: train D loss: 0.6720, train F loss: -0.6464, acc: 0.9936\n",
      "epoch 1963: train D loss: 0.6754, train F loss: -0.5958, acc: 0.9884\n",
      "epoch 1964: train D loss: 0.6744, train F loss: -0.6463, acc: 0.9930\n",
      "epoch 1965: train D loss: 0.6766, train F loss: -0.6478, acc: 0.9932\n",
      "epoch 1966: train D loss: 0.6727, train F loss: -0.6450, acc: 0.9912\n",
      "epoch 1967: train D loss: 0.6755, train F loss: -0.6472, acc: 0.9926\n",
      "epoch 1968: train D loss: 0.6783, train F loss: -0.6557, acc: 0.9952\n",
      "epoch 1969: train D loss: 0.6763, train F loss: -0.6521, acc: 0.9946\n",
      "epoch 1970: train D loss: 0.6746, train F loss: -0.6520, acc: 0.9950\n",
      "epoch 1971: train D loss: 0.6778, train F loss: -0.6511, acc: 0.9930\n",
      "epoch 1972: train D loss: 0.6750, train F loss: -0.6517, acc: 0.9944\n",
      "epoch 1973: train D loss: 0.6769, train F loss: -0.6554, acc: 0.9942\n",
      "epoch 1974: train D loss: 0.6775, train F loss: -0.6537, acc: 0.9930\n",
      "epoch 1975: train D loss: 0.6765, train F loss: -0.6326, acc: 0.9904\n",
      "epoch 1976: train D loss: 0.6775, train F loss: -0.6409, acc: 0.9904\n",
      "epoch 1977: train D loss: 0.6777, train F loss: -0.6469, acc: 0.9910\n",
      "epoch 1978: train D loss: 0.6773, train F loss: -0.6446, acc: 0.9920\n",
      "epoch 1979: train D loss: 0.6786, train F loss: -0.6459, acc: 0.9908\n",
      "epoch 1980: train D loss: 0.6779, train F loss: -0.6360, acc: 0.9906\n",
      "epoch 1981: train D loss: 0.6772, train F loss: -0.6522, acc: 0.9928\n",
      "epoch 1982: train D loss: 0.6745, train F loss: -0.6386, acc: 0.9904\n",
      "epoch 1983: train D loss: 0.6742, train F loss: -0.6465, acc: 0.9916\n",
      "epoch 1984: train D loss: 0.6745, train F loss: -0.6540, acc: 0.9950\n",
      "epoch 1985: train D loss: 0.6803, train F loss: -0.6526, acc: 0.9934\n",
      "epoch 1986: train D loss: 0.6747, train F loss: -0.6494, acc: 0.9922\n",
      "epoch 1987: train D loss: 0.6779, train F loss: -0.6546, acc: 0.9942\n",
      "epoch 1988: train D loss: 0.6781, train F loss: -0.6539, acc: 0.9940\n",
      "epoch 1989: train D loss: 0.6791, train F loss: -0.6556, acc: 0.9940\n",
      "epoch 1990: train D loss: 0.6756, train F loss: -0.6531, acc: 0.9956\n",
      "epoch 1991: train D loss: 0.6808, train F loss: -0.6523, acc: 0.9926\n",
      "epoch 1992: train D loss: 0.6774, train F loss: -0.6522, acc: 0.9930\n",
      "epoch 1993: train D loss: 0.6782, train F loss: -0.6504, acc: 0.9932\n",
      "epoch 1994: train D loss: 0.6769, train F loss: -0.6361, acc: 0.9878\n",
      "epoch 1995: train D loss: 0.6778, train F loss: -0.6529, acc: 0.9924\n",
      "epoch 1996: train D loss: 0.6774, train F loss: -0.6473, acc: 0.9924\n",
      "epoch 1997: train D loss: 0.6751, train F loss: -0.6415, acc: 0.9912\n",
      "epoch 1998: train D loss: 0.6780, train F loss: -0.6281, acc: 0.9872\n",
      "epoch 1999: train D loss: 0.6750, train F loss: -0.6440, acc: 0.9918\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(source_loader, target_loader, lamb):\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0, 0\n",
    "    \n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
    "        source_data = source_data.cuda()\n",
    "        source_label = source_label.cuda()\n",
    "        target_data = target_data.cuda()\n",
    "        \n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([mixed_data.shape[0], 1]).cuda()\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "        \n",
    "        features = feature_extractor(mixed_data)\n",
    "        domain_logits = domain_classifier(features.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label)\n",
    "        running_D_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        class_logits = label_predictor(features[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(features)\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
    "        running_F_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_L.step()\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_L.zero_grad()\n",
    "        \n",
    "        total_hit += (class_logits.argmax(dim=1) == source_label).sum().item()\n",
    "        total_num += source_data.shape[0]\n",
    "        print(i, end='\\r')\n",
    "    \n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
    "\n",
    "num_epoch = 2000\n",
    "for epoch in range(num_epoch):\n",
    "    lamb = adaptive_lambda(epoch, num_epoch)\n",
    "    train_D_loss, train_F_loss, train_acc =  train_epoch(source_loader, target_loader, lamb)\n",
    "    torch.save(feature_extractor.state_dict(), 'extractor_model.bin')\n",
    "    torch.save(label_predictor.state_dict(), 'predictor_model.bin')\n",
    "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc: {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "feature_extractor.eval()\n",
    "label_predictor.eval()\n",
    "\n",
    "for i, (test_data, _) in enumerate(test_loader):\n",
    "    test_data = test_data.cuda()\n",
    "    logits = label_predictor(feature_extractor(test_data))\n",
    "    pred = logits.argmax(dim=1).cpu().detach().numpy()\n",
    "    result.append(pred)\n",
    "\n",
    "import pandas as pd\n",
    "result = np.concatenate(result)\n",
    "\n",
    "df = pd.DataFrame({'id': np.arange(0, len(result)), 'label': result})\n",
    "df.to_csv('DaNN_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
